{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoonkim313/dataCampusProject-Team10/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "obGBWFLnIypX",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/mnt')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/mnt/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0, nb_path)\n",
        "from konlpy.tag import Hannanum, Okt\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import heapq\n",
        "import pandas as pd\n",
        "from operator import itemgetter\n",
        "from collections import deque\n",
        "from ast import literal_eval\n",
        "from collections import defaultdict\n",
        "from soykeyword.lasso import LassoKeywordExtractor\n",
        "from pprint import pprint\n",
        "from krwordrank.word import KRWordRank\n",
        "from copy import deepcopy\n",
        "import kss\n",
        "import itertools\n",
        "import unicodedata\n",
        "import requests\n",
        "from functools import reduce\n",
        "from transformers import *\n",
        "import torch\n",
        "from summarizer import Summarizer\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "from textrankr import TextRank\n",
        "from lexrankr import LexRank\n",
        "\n",
        "%cd /content/mnt/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT\n",
        "import frame_parser\n",
        "path=\"/content/mnt/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\"\n",
        "parser = frame_parser.FrameParser(model_path=path, language='ko')\n",
        "h = Hannanum()\n",
        "okt = Okt()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJMQe0p-PO01",
        "colab_type": "text"
      },
      "source": [
        "### Flask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8JWigx7M9UY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f0df9a95-5518-400a-a315-bd20d8cfee1c"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"<h1>Running Flask on Google Colab!</h1>\"\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://aefb9f12ce4e.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar-s1XyrwKmj",
        "colab_type": "text"
      },
      "source": [
        "#### Linked List 구현해서 Frame Net의 의미역 parsing 후보군에 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gn8ClK-v4uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Node:\n",
        "    def __init__(self, data):\n",
        "        self.words = data[0]\n",
        "        self.tags = data[3]\n",
        "        self.next = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str((self.words, self.tags))\n",
        "\n",
        "class LinkedList:\n",
        "  def __init__(self):\n",
        "      self.head = None\n",
        "\n",
        "  def __repr__(self):\n",
        "      node = self.head\n",
        "      nodes = []\n",
        "      while node is not None:\n",
        "          nodes.append(str(node.tags))\n",
        "          node = node.next\n",
        "      nodes.append(\"None\")\n",
        "      return ' -> '.join(nodes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl_ThLJhwOS4",
        "colab_type": "text"
      },
      "source": [
        "### Multiple Inheritance by Super()\n",
        "\n",
        "  **Text --> Highlight --> Relation**\n",
        "\n",
        "  **Text --> Highlight --> Summarize**\n",
        "\n",
        "\n",
        "\n",
        "Text 클래스\n",
        "\n",
        "    주어진 텍스트를 문단별로, 문장별로 나누어줌\n",
        "\n",
        "Highlight 클래스\n",
        "     \n",
        "     CSS 문법을 사용하여 중심문장에는 underline, 중심 단어에는 highlight, 관계성을 나타내는 단어들에는 box를 삽입해준다\n",
        "\n",
        "Relation 클래스\n",
        "\n",
        "    frameNET의 807개의 의미역을 사용하여 단어들의 다의성을 고려한 용례, beginning/ inside/ outside tagging을 이용한다. 구절 사이의 언어적 관계성을 파악할 수 있으며 이를 통해 요약, 압축을 구현함.\n",
        "     \n",
        "Summarize 클래스\n",
        "\n",
        "    Bert, Textrank, Lexrank 세가지 알고리즘을 사용하여 요약 결과를 multiple voting한다\n",
        "    중심 문장은 가장 높은 득표수를 차지한 문장으로 리턴함\n",
        "  Relation.__mro__\n",
        "  \n",
        "  result : (__main__.Relation, __main__.Highlight, __main__.Text, object)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGCJPBUtvXj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Text():\n",
        "    def __init__(self, text):\n",
        "        text = re.sub(\"'\", ' ', text)\n",
        "        paragraphs = text.split('\\n')\n",
        "        self.text = text\n",
        "        self.paragraphs = [i for i in paragraphs if i]\n",
        "        self.docs = [kss.split_sentences(paragraph) for paragraph in paragraphs if kss.split_sentences(paragraph)]\n",
        "        self.newtext = deepcopy(self.text)\n",
        "        print(\"TEXT\")\n",
        "\n",
        "    def findall(self, p, s):\n",
        "        i = s.find(p)\n",
        "        while i != -1:\n",
        "            yield i\n",
        "            i = s.find(p, i + 1)\n",
        "    \n",
        "    def _matchSentenceIndex(self, sentence):\n",
        "        indices = [idx for idx, sent  in enumerate(self.docs[1]) if sent.startswith(str(sentence[:5]))]\n",
        "        return indices\n",
        "    \n",
        "    def matchSentenceIndex(self, summarized, paragraph):\n",
        "        vote = [0]*len(paragraph)\n",
        "        for i in summarized:\n",
        "          for idx, sent in enumerate(self.docs[1]):\n",
        "            if sent.startswith(str(i[:5])):\n",
        "              vote[idx]+=1  \n",
        "        return vote\n",
        "\n",
        "\n",
        "class Highlight(Text):\n",
        "    def __init__(self, text, candidates=None):\n",
        "        super().__init__(text)\n",
        "        self.candidates = candidates\n",
        "        self.cand = candidates\n",
        "        print(\"Highlight\")\n",
        "\n",
        "    def add_tags(self, underline = False, highlight = True):\n",
        "        if self.cand == None:\n",
        "            conj = '그리고, 그런데, 그러나, 그래도, 그래서, 또는, 및, 즉, 게다가, 따라서, 때문에, 아니면, 왜냐하면, 단, 오히려, 비록, 예를 들어, 반면에, 하지만, 그렇다면, 바로, 이에 대해'\n",
        "            conj = conj.replace(\"'\", \"\")\n",
        "            self.candidates = conj.split(\",\")\n",
        "            self.idx = [(i, i + len(candidate)) for candidate in self.candidates for i in\n",
        "                        self.findall(candidate, self.text)]\n",
        "        else:\n",
        "            self.idx = [(i, i + len(candidate)) for candidate in self.candidates for i in\n",
        "                        self.findall(candidate, self.newtext)]\n",
        "\n",
        "        for i in range(len(self.idx)):\n",
        "            try:\n",
        "                if highligh:\n",
        "                    self.idx = [(start, start + len(candidate)) for candidate in self.candidates for start in\n",
        "                                self.findall(candidate, self.newtext)]\n",
        "                    word = self.newtext[self.idx[i][0]:self.idx[i][1]]\n",
        "                    if highlight and self.cand == None:\n",
        "                        tagged = \" <mark style='background-color:#F9D877'>%s</mark>\" % (word)\n",
        "                    elif highlight:\n",
        "                        tagged = \" <mark style='background-color:#FFD0F2'>%s</mark>\" % (word)\n",
        "                if underline:\n",
        "                    self.idx = [(start, start + len(candidate)) for candidate in self.candidates for start in\n",
        "                                self.findall(candidate, self.newtext)]\n",
        "                    word = self.newtext[self.idx[i][0]:self.idx[i][1]]\n",
        "                    tagged = \"<u style='text-decoration:underline; text-decoration-color:#906fa8; font-weight: bold; text-decoration-style: wavy'>%s</u>\" % (word)\n",
        "                \n",
        "                self.newtext = tagged.join([self.newtext[:self.idx[i][0]], self.newtext[self.idx[i][1]:]])\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return '\\n'.join(self.newtext.split(\"\\n\"))\n",
        "\n",
        "\n",
        "class Relation(Highlight):\n",
        "\n",
        "    def __init__(self, text, candidates=None, num=20):\n",
        "        super().__init__(text, candidates)\n",
        "        wordrank_extractor = KRWordRank(min_count=4, max_length=10)\n",
        "        self.keywords, rank, graph = wordrank_extractor.extract(self.paragraphs, num_keywords=num)\n",
        "        self.path = \"/content/mnt/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\"\n",
        "        p = []\n",
        "        kw = []\n",
        "        for k, v in self.keywords.items():\n",
        "            p.append(okt.pos(k))\n",
        "            kw.append(k)\n",
        "        words = self.text.split(' ')\n",
        "        s = set()\n",
        "        keylist = [word for i in kw for word in words if i in word]\n",
        "        for i in keylist:\n",
        "            s.add(i)\n",
        "        p = [okt.pos(word) for word in s]\n",
        "        s = set()\n",
        "        for idx in range(len(p)):\n",
        "            ls = p[idx]\n",
        "            for j in range(len(ls)):\n",
        "                tag = ls[j][1]\n",
        "                word = ls[j][0]\n",
        "                if tag == \"Noun\":\n",
        "                    s.add(word)\n",
        "        self.keys = []\n",
        "        for temp in s:\n",
        "            self.keys.append(\" \" + temp)\n",
        "        print(\"KEYS: \", self.keys)\n",
        "\n",
        "    def frameParse(self, id):\n",
        "        parser = frame_parser.FrameParser(model_path=self.path, language='ko')\n",
        "        ps = parser.parser(self.docs[id], sent_id='1', result_format='conll')\n",
        "        return ps\n",
        "\n",
        "    def extractFrame(self):\n",
        "        self.final = {}\n",
        "        for paragraph in self.docs:\n",
        "            print(\"PARAGRAPH: \", self.docs)\n",
        "            for idx in range(len(paragraph)):\n",
        "                parsed = frameParse(paragraph[idx])  # candidates 생성\n",
        "                self.final.setdefault(idx, str)\n",
        "                parsedList = LinkedList()\n",
        "                for j in range(len(parsed)):\n",
        "                    parsed_candidate = parsed[j]\n",
        "                    new_node = Node(parsed_candidate)\n",
        "                    if j == 0:\n",
        "                        old_node = new_node\n",
        "                        parsedList.head = old_node\n",
        "                    elif j == len(parsed) - 1:\n",
        "                        old_node.next = new_node\n",
        "                        new_node.next = None\n",
        "                        print(idx, '  ', parsedList)\n",
        "                        self.final[idx] = parsedList\n",
        "                    else:\n",
        "                        old_node.next = new_node\n",
        "                        old_node = new_node\n",
        "\n",
        "    def findConsecutiveBIO(self, words, tag):\n",
        "        began = False\n",
        "        count = 1\n",
        "        self.que = deque(words)\n",
        "        for i in range(len(words)):\n",
        "            if tag[i] == 'O' and not began:\n",
        "                self.que.popleft()\n",
        "                began = False\n",
        "            if tag[i] == 'O' and began:\n",
        "                self.que.pop()\n",
        "                began = False\n",
        "            if tag[i].startswith('B'):\n",
        "                began = True\n",
        "            if began & tag[i].startswith('I'):\n",
        "                count += 1\n",
        "        return self.que\n",
        "\n",
        "    def extractRelation(self):\n",
        "        s = \"\"\n",
        "        self.res = [] * 20\n",
        "        for k, v in self.final.items():\n",
        "            a = v.head\n",
        "            try:\n",
        "                while a is not None:\n",
        "                    i = 0\n",
        "                    temp = \" \"\n",
        "                    if len(a.words) == len(a.tags):\n",
        "                        q = findConsecutiveBIO(a.words, a.tags)\n",
        "                        print(f\"The que {q}\")\n",
        "                    else:\n",
        "                        print(\"ERROR OCCURED\")\n",
        "\n",
        "                    count = len(a.words)\n",
        "                    for tag in a.tags:\n",
        "                        if tag.startswith(\"O\"):\n",
        "                            count -= 1\n",
        "                    if count > len(a.words) * 0.45:\n",
        "                        print(f\"The threshold is reached !!😏 The count is {count}\")\n",
        "                        temp = \" \".join(q)\n",
        "                        words = a.words\n",
        "                        tags = a.tags\n",
        "                    else:\n",
        "                        temp = None\n",
        "                        words = None\n",
        "                        tags = None\n",
        "                    self.res[i] = (words, tag, temp)\n",
        "                    i+=1\n",
        "                    a = a.next\n",
        "            except:\n",
        "                pass\n",
        "        self.res = [j for j in self.res if j]\n",
        "\n",
        "    def roles(self):\n",
        "        for k, tup in self.res.items():\n",
        "            a, b, c = tup\n",
        "            roles = [0] * len(b)\n",
        "            words = [0] * len(b)\n",
        "            for i in range(len(b)):\n",
        "                try:\n",
        "                    roles[i] = b[i].split(\"-\")[1]\n",
        "                except:\n",
        "                    roles[i] = b[i].split(\"-\")[0]\n",
        "            print(roles)\n",
        "            for _ in roles:\n",
        "                pass\n",
        "\n",
        "class Summarize(Highlight):\n",
        "\n",
        "  def __init__(self, text, idx):\n",
        "      print(\"SUMMARIZE\")\n",
        "      super().__init__(text)\n",
        "      self.idx = idx\n",
        "      self.paragraph = self.paragraphs[idx]\n",
        "      url = \"https://api.smrzr.io/summarize?ratio=0.15\"\n",
        "      headers = {\n",
        "          'content-type': 'raw/text',\n",
        "          'origin': 'https://smrzr.io',\n",
        "          'referer': 'https://smrzr.io/',\n",
        "          'sec-fetch-dest': 'empty',\n",
        "          'sec-fetch-mode': 'cors',\n",
        "          'sec-fetch-site': 'same-site',\n",
        "          \"user-agent\": \"Mozilla/5.0\"\n",
        "      }\n",
        "      resp = requests.post(url, headers=headers, data=self.paragraph.encode('utf-8'))\n",
        "      assert resp.status_code == 200\n",
        "\n",
        "      summary = resp.json()['summary']\n",
        "      self.brSum = summary.split('\\n') # 리스트의 형태로 extracted sentence 반환함\n",
        "      tr = TextRank(self.paragraph)\n",
        "      t = tr.summarize(2)\n",
        "      self.trSum = kss.split_sentences(t)\n",
        "      lr = LexRank() \n",
        "      lr.summarize(self.paragraph)\n",
        "      self.lrSum = lr.probe()\n",
        "      \n",
        "\n",
        "  def summarizeSlow(self, ratio, model_name='Distil'):\n",
        "      global model\n",
        "      if model_name == \"Distil\":\n",
        "          model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=True)\n",
        "          tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "      elif model_name == \"Bert\":\n",
        "          bmodel = AutoModel.from_pretrained('bert-base-multilingual-uncased', output_hidden_states=True)\n",
        "          configuration = bmodel.config\n",
        "          configuration.vocab_size = 32000\n",
        "          configuration.max_position_embeddings = 384\n",
        "          configuration.hidden_size = 1024\n",
        "          configuration.num_attention_heads = 16\n",
        "          configuration.output_hidden_states = True\n",
        "          model = BertModel.from_pretrained(\"/Users/yoonk/Downloads/large_v2.bin\", config=configuration)\n",
        "          tokenizer = BertTokenizer.from_pretrained(\"/Users/yoonk/Downloads/large_v2_32k_vocab.txt\")\n",
        "\n",
        "      summarizer = Summarizer(custom_model=model, custom_tokenizer=tokenizer)\n",
        "      url = 'https://kapi.kakao.com/v1/translation/translate'\n",
        "      before_lang = 'kr'\n",
        "      after_lang = 'en'\n",
        "      KEY = '18fea080a0db2ee0346353e8db4466e8'\n",
        "      header = {\n",
        "          \"Authorization\": 'KakaoAK {}'.format(KEY)\n",
        "      }\n",
        "\n",
        "      data = {\n",
        "          \"src_lang\": before_lang,\n",
        "          \"target_lang\": after_lang,\n",
        "          \"query\": text\n",
        "      }\n",
        "\n",
        "      response = requests.get(url, headers=header, params=data)\n",
        "      result = response.json()\n",
        "      translated = result['translated_text'][0]\n",
        "      result_eng = summarizer(translated, ratio=ratio)\n",
        "\n",
        "      before_lang = 'en'\n",
        "      after_lang = 'kr'\n",
        "      data = {\n",
        "          \"src_lang\": before_lang,\n",
        "          \"target_lang\": after_lang,\n",
        "          \"query\": ''.join(result_eng)\n",
        "      }\n",
        "      before_lang = 'en'\n",
        "      after_lang = 'kr'\n",
        "\n",
        "      response = requests.get(url, headers=header, params=data)\n",
        "      result_kor = response.json()\n",
        "      summarized = result_kor['translated_text'][0]\n",
        "      return summarized\n",
        "\n",
        "      \n",
        "  def ensembleSummarize(self):\n",
        "      lenSentences=len(self.docs[self.idx])\n",
        "      s = [0]*lenSentences\n",
        "      for idx in range(lenSentences):\n",
        "          \n",
        "          if self.paragraph[idx] in self.brSum:\n",
        "              s[idx]+=1\n",
        "              print(\"등장! \", self.paragraph[idx])\n",
        "          \n",
        "          if self.paragraph[idx] in self.lrSum:\n",
        "              s[idx]+=1\n",
        "              print(\"등장! \", self.paragraph[idx])\n",
        "          \n",
        "          if self.paragraph[idx] in self.trSum:\n",
        "              s[idx]+=1\n",
        "              print(\"등장! \", self.paragraph[idx])\n",
        "\n",
        "      print(\"Bert Summarization\",self.brSum)\n",
        "      print()\n",
        "      print(\"LexRank Summarization\",self.lrSum)\n",
        "      print()\n",
        "      print(\"TextRank Summarization\",self.trSum)\n",
        "      i = s.index(max(s))\n",
        "      extracted = self.docs[self.idx][i]\n",
        "      return extracted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_v5o0el7I9ur",
        "colab": {}
      },
      "source": [
        "text = '''\n",
        "평등은 자유와 더불어 근대 사회의 핵심 이념으로 자리 잡고있다. 인간은 가령 인종이나 성별과 상관없이 누구나 평등하다고 생각한다. 모든 인간은 평등하다고 말하는데, 이 말은 무슨 뜻일까? 그리고 그 근거는 무엇인가? \n",
        "일단 이 말을 모든 인간을 모든 측면에서 똑같이 대우하는 절대칙 평등으로 생각하는 이는 없다. 인간은 저마다 다르게 가지고 대어난 능력과 소질을 똑같게 만들 수 없기 때문이다. 절대적 평등은 개인의 개성이냐 자율성 등의 가치와 충돌하기도 한다. 평등에 대한 요구는 모든 불평등을 악으로 보는 것이 아니라 충분한 이유가 제시되지 않은 불평등을 제거하는 데 목표를 두고 있다. ‘이유 없는 차별 금지’라는 조건적 평등 원칙은 차별 대우를 할 때는 이유를 제시할 것을 요구하고 있다. 이것은 어떤 이유가 제시된다면 특정한 부류에 속하는 사람들에게는 평등한 대우를, 그 부류에 속하지 않는 사람들에게는 차별저 대우를 하는 것을 허용한다. 그렇다면 사람들을 특정한 부류로 구분하는 기준은 무엇인가? 이것은 바로 평등의 근거에 대한 물음이다.\n",
        "근대의 여러 인권 선언에 나타난 평등 개념은 개인들 사이의 평등성을 타고난 자연적 권리로 간주하였다. 하지만 이러한 자연권 이론은 무엇이 자연적 권리이고 권리의 존재가 자명한 이유가 무엇인지 등의 문제에 부딪히게 된다. 그래서 롤스는 기존의 자연권 사상에 의존하지 않는 방시으로 인간 평등의 근거를 마런하려고 한다. 그는 어떤 규칙이 공평하고 일관되게 운영되며, 그 규칙에 따라 유사한 경우는 유사하게 취급된다면 형식적 정의는 실현된다고 본다. 하지만 롤스는 형식저 정의에 따라 규칙을 준수하는 것만으로는 정의를 담보할 수 없다고 생각한다. 그 규칙이 더 높은 도덕적 권위를 지닌 다른 이넘과 충돌할 수 었기에, 실질적 정의가 보장되기 위해서는 규칙의 내용이 중요한 것이다.\n",
        "롤스는 인간 평등의 근거를 설명하면서 영역 성질 (range property) 개념을 도입한다. 예를 들어 어떤 원의 내부에 있는 점들은 그 위치가 서로 다르지만 원의 내부에 있다는 점에서 동일한 영역 성질을 갖는다. 반면에 원의 내부에 있는 집과 원의 외부에 였는 점은원의 경계선을 기준으로 서로 다른 영역 성질을 갖는다. 그는 평등한 대우를 받기 위한 영역 성질로서 ‘도덕적 인격'을 제시한다. 도덕적 인격이란 도덕적 호소가 가능하고 그런 호소에 관심을 기이는 능력이 있다는 것인데, 이 능력을 최소치만 갖고 있다면 평등한 대우에 대한 권한을 갖게 된다. 도덕적 인격이라고 해서 도덕적으로 훌륭하다는 뜻이 아니라 도덕과 무관하다는 말과 대비되는 뜻으로 쓰고 있다. 그런데 어린 아이는 인격체로서의 최소한의 기준을 충족하고 있는지가 논란이 될 수 있다. 이에 대해 롤스는 도덕적 인격을 규정하는 최소한의 요구 조건은 잠재적 능력이지 그것의 실현 여부가 아니기에 어린 아이도 평등한 존재라고 말한다. 싱어는 위와 같은 롤스의 시도를 비판한다. 도덕에 대한 민감성의 수준은 사람에 따라 다르다. 그래서 도덕적 인격의 능력이 그렇게 중요하다면 그것을 갖춘 정도에 따라 도덕적 위계를 다르게 하지 말아야 할 이유가 분명하지 않다고 말한다. \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrwJKcTB-S8j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "ad275e66-de67-4b38-a02f-4e14dc065552"
      },
      "source": [
        "sm = Summarize(text,1)\n",
        "sm.ensembleSummarize()\n",
        "sm.matchSentenceIndex()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SUMMARIZE\n",
            "TEXT\n",
            "Highlight\n",
            "Bert Summarization ['일단 이 말을 모든 인간을 모든 측면에서 똑같이 대우하는 절대칙 평등으로 생각하는 이는 없다 . 인간은 저마다 다르게 가지고 대어난 능력과 소질을 똑같게 만들 수 없기 때문이다 .']\n",
            "\n",
            "LexRank Summarization ['‘이유 없는 차별 금지’라는 조건적 평등 원칙은 차별 대우를 할 때는 이유를 제시할 것을 요구하고 있다', '그렇다면 사람들을 특정한 부류로 구분하는 기준은 무엇인가? 이것은 바로 평등의 근거에 대한 물음이다']\n",
            "\n",
            "TextRank Summarization ['평등에 대한 요구는 모든 불평등을 악으로 보는 것이 아니라 충분한 이유가 제시되지 않은 불평등을 제거하는 데 목표를 두고 있다.', '‘이유 없는 차별 금지’라는 조건적 평등 원칙은 차별 대우를 할 때는 이유를 제시할 것을 요구하고 있다.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/notebooks/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'일단 이 말을 모든 인간을 모든 측면에서 똑같이 대우하는 절대칙 평등으로 생각하는 이는 없다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud7wmTvCILR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = '‘이유 없는 차별 금지’라는 조건적 평등 원칙은 차별 대우를 할 때는 이유를 제시할 것을 요구하고 있다'\n",
        "if temp in sm.docs[1]:\n",
        "  print(\"등장\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyEogB3pIWBd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ecce6c09-9a5b-47e0-b603-84f8cb64c794"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S7zGTQO4BUO7",
        "colab": {}
      },
      "source": [
        "# 1st sentence : GOALS & MEANS \n",
        "\n",
        "parsed = parser.parser(ls[0], sent_id='1', result_format='conll')\n",
        "words = parsed[0][0]\n",
        "print(\"😀 Words vector from sentence \",words)\n",
        "q = [-1]*len(parsed)\n",
        "for i in range(len(parsed)):\n",
        "  count=0\n",
        "  for element in parsed[i][3]:\n",
        "    if element.startswith(\"O\"):\n",
        "      count+=1\n",
        "  q[i] = len(words) - count\n",
        "hq = heapq.nlargest(2, q)\n",
        "\n",
        "\n",
        "print(\"Number of parsed candidates \",len(parsed))\n",
        "print(q, heapq.nlargest(2, q)) # 첫번째 경우 사용\n",
        "\n",
        "\n",
        "words = parsed[0][0]\n",
        "roles = parsed[0][2]\n",
        "tagged = parsed[0][3]\n",
        "for idx in range(len(tagged)):\n",
        "  try:\n",
        "    roles[i] = tagged[i].split(\"-\")[1]\n",
        "  except:\n",
        "    roles[i] = tagged[i].split(\"-\")[0]\n",
        "\n",
        "print(\"roles \",roles)\n",
        "\n",
        "goals = []\n",
        "means = []\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Goal\":\n",
        "    goals.append(words[_])\n",
        "  elif roles[_]=='Means':\n",
        "    means.append(words[_])\n",
        "  elif roles[_]==\"Purpose\":\n",
        "    purpose.append(words[_])\n",
        "  elif roles[_]==\"Instrument\":\n",
        "    instrument.append(words[_])\n",
        "  elif roles[_]!=\"_\": # using 포함\n",
        "    instrument.append(words[_])\n",
        "\n",
        "# MEANS\n",
        "MEANS = ' '.join(means)\n",
        "pos = h.pos(MEANS)\n",
        "imp = []\n",
        "for tup in h.pos(MEANS):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "MEANS_TRUNCATED = ' '.join(imp)\n",
        "print(MEANS_TRUNCATED)\n",
        "\n",
        "# GOALS\n",
        "GOAL = ' '.join(goals)\n",
        "pos = h.pos(GOAL)\n",
        "imp = []\n",
        "for tup in h.pos(GOAL):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "GOAL_TRUNCATED = ' '.join(imp)\n",
        "print(GOAL_TRUNCATED)\n",
        "\n",
        "final_string = MEANS_TRUNCATED + ' -- >' + GOAL_TRUNCATED\n",
        "print(\"😀 final string to be inserted\",final_string)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}