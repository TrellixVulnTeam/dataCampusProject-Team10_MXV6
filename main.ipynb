{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoonkim313/dataCampusProject-Team10/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_c_ftRNRpuP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> 공유 드라이브와 작업 환경 연결\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "obGBWFLnIypX",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebook'\n",
        "os.symlink('/content/drive/Shared drives/BigDATA TEAM 10', nb_path)\n",
        "sys.path.insert(0,nb_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8pGPjInRL-g",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> 사전에 설치된 라이브러리의 목록\n",
        "\n",
        "\n",
        "    !pip install --target=$nb_path transformers\n",
        "    !apt-get update\n",
        "    !apt-get g++ openjdk-8-jdk \n",
        "    !pip3 install --target=$nb_path konlpy\n",
        "    !pip install --target=$nb_path soykeyword\n",
        "    !pip install --target=$nb_path krwordrank\n",
        "    !pip install --target=$nb_path heapq\n",
        "    !pip install --target=$nb_path kss\n",
        "    !pip install --target=$nb_path bert\n",
        "    !pip install --target=$nb_path textrankr\n",
        "    !pip install --target=$nb_path lexrankr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BW_dvLIjPrrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from konlpy.tag import Hannanum, Okt\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import heapq\n",
        "import pandas as pd\n",
        "from operator import itemgetter\n",
        "from collections import deque\n",
        "from ast import literal_eval\n",
        "from collections import defaultdict\n",
        "from soykeyword.lasso import LassoKeywordExtractor\n",
        "from pprint import pprint\n",
        "from krwordrank.word import KRWordRank\n",
        "from copy import deepcopy\n",
        "import kss\n",
        "import itertools\n",
        "import unicodedata\n",
        "import requests\n",
        "from functools import reduce\n",
        "from transformers import *\n",
        "import torch\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "from textrankr import TextRank\n",
        "from lexrankr import LexRank"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RauwSoNrRYvr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "c025c362-35a7-49d3-d793-27869128c4f9"
      },
      "source": [
        "%cd /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT\n",
        "import frame_parser\n",
        "path=\"/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\"\n",
        "parser = frame_parser.FrameParser(model_path=path, language='ko')\n",
        "h = Hannanum()\n",
        "okt = Okt()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT\n",
            "\n",
            "###DEVICE: cpu\n",
            "srl model: framenet\n",
            "language: ko\n",
            "version: 1.2\n",
            "using viterbi: False\n",
            "using masking: True\n",
            "pretrained BERT: bert-base-multilingual-cased\n",
            "using TGT special token: True\n",
            "used dictionary:\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lu2idx.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lufrmap.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n",
            "...loaded model path: /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "...model is loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar-s1XyrwKmj",
        "colab_type": "text"
      },
      "source": [
        "#### Linked List 구현해서 parsed 후보군에 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gn8ClK-v4uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Node:\n",
        "    def __init__(self, data):\n",
        "        self.words = data[0]\n",
        "        self.tags = data[3]\n",
        "        self.next = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str((self.words, self.tags))\n",
        "\n",
        "class LinkedList:\n",
        "  def __init__(self):\n",
        "      self.head = None\n",
        "\n",
        "  def __repr__(self):\n",
        "      node = self.head\n",
        "      nodes = []\n",
        "      while node is not None:\n",
        "          nodes.append(str(node.tags))\n",
        "          node = node.next\n",
        "      nodes.append(\"None\")\n",
        "      return ' -> '.join(nodes)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl_ThLJhwOS4",
        "colab_type": "text"
      },
      "source": [
        "### Multiple Inheritance by Super()\n",
        "\n",
        "  **Text --> Highlight --> Relation**\n",
        "\n",
        "\n",
        "Text 클래스\n",
        "\n",
        "    주어진 텍스트를 문단별로, 문장별로 나누어줌\n",
        "\n",
        "Highlight 클래스\n",
        "     \n",
        "     CSS 문법을 사용하여 중심문장에는 underline, 중심 단어에는 highlight, 관계성을 나타내는 단어들에는 box를 삽입해준다\n",
        "\n",
        "Relation 클래스\n",
        "\n",
        "    frameNET의 807개의 의미역을 사용하여 단어들의 다의성을 고려한 용례, beginning/ inside/ outside tagging을 이용한다. 구절 사이의 언어적 관계성을 파악할 수 있으며 이를 통해 요약, 압축을 구현함.\n",
        "     \n",
        "\n",
        "  Relation.__mro__\n",
        "  result : (__main__.Relation, __main__.Highlight, __main__.Text, object)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGCJPBUtvXj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Text():\n",
        "    def __init__(self, text):\n",
        "        text = re.sub(\"'\", ' ', text)\n",
        "        paragraphs = text.split('\\n')\n",
        "        self.text = text\n",
        "        self.paragraphs = [i for i in paragraphs if i]\n",
        "        self.docs = [kss.split_sentences(paragraph) for paragraph in paragraphs if kss.split_sentences(paragraph)]\n",
        "        self.newtext = deepcopy(self.text)\n",
        "        print(\"TEXT\")\n",
        "\n",
        "    def findall(self, p, s):\n",
        "        i = s.find(p)\n",
        "        while i != -1:\n",
        "            yield i\n",
        "            i = s.find(p, i + 1)\n",
        "\n",
        "\n",
        "class Highlight(Text):\n",
        "    def __init__(self, text, candidates=None):\n",
        "        super().__init__(text)\n",
        "        self.candidates = candidates\n",
        "        self.cand = candidates\n",
        "        print(\"Highlight\")\n",
        "\n",
        "    def add_tags(self, underline = False, highlight = True):\n",
        "        if self.cand == None:\n",
        "            conj = '그리고, 그런데, 그러나, 그래도, 그래서, 또는, 및, 즉, 게다가, 따라서, 때문에, 아니면, 왜냐하면, 단, 오히려, 비록, 예를 들어, 반면에, 하지만, 그렇다면, 바로, 이에 대해'\n",
        "            conj = conj.replace(\"'\", \"\")\n",
        "            self.candidates = conj.split(\",\")\n",
        "            self.idx = [(i, i + len(candidate)) for candidate in self.candidates for i in\n",
        "                        self.findall(candidate, self.text)]\n",
        "        else:\n",
        "            self.idx = [(i, i + len(candidate)) for candidate in self.candidates for i in\n",
        "                        self.findall(candidate, self.newtext)]\n",
        "\n",
        "        for i in range(len(self.idx)):\n",
        "            try:\n",
        "                if highligh:\n",
        "                    self.idx = [(start, start + len(candidate)) for candidate in self.candidates for start in\n",
        "                                self.findall(candidate, self.newtext)]\n",
        "                    word = self.newtext[self.idx[i][0]:self.idx[i][1]]\n",
        "                    if highlight and self.cand == None:\n",
        "                        tagged = \" <mark style='background-color:#F9D877'>%s</mark>\" % (word)\n",
        "                    elif highlight:\n",
        "                        tagged = \" <mark style='background-color:#FFD0F2'>%s</mark>\" % (word)\n",
        "                if underline:\n",
        "                    self.idx = [(start, start + len(candidate)) for candidate in self.candidates for start in\n",
        "                                self.findall(candidate, self.newtext)]\n",
        "                    word = self.newtext[self.idx[i][0]:self.idx[i][1]]\n",
        "                    tagged = \"<u style='text-decoration:underline; text-decoration-color:#906fa8; font-weight: bold; text-decoration-style: wavy'>%s</u>\" % (word)\n",
        "                \n",
        "                self.newtext = tagged.join([self.newtext[:self.idx[i][0]], self.newtext[self.idx[i][1]:]])\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return '\\n'.join(self.newtext.split(\"\\n\"))\n",
        "\n",
        "\n",
        "class Relation(Highlight):\n",
        "    def __init__(self, text, candidates=None, num=20):\n",
        "        super().__init__(text, candidates)\n",
        "        wordrank_extractor = KRWordRank(min_count=4, max_length=10)\n",
        "        self.keywords, rank, graph = wordrank_extractor.extract(self.paragraphs, num_keywords=num)\n",
        "        self.path = \"/content/mnt/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\"\n",
        "        p = []\n",
        "        kw = []\n",
        "        for k, v in self.keywords.items():\n",
        "            p.append(okt.pos(k))\n",
        "            kw.append(k)\n",
        "        words = self.text.split(' ')\n",
        "        s = set()\n",
        "        keylist = [word for i in kw for word in words if i in word]\n",
        "        for i in keylist:\n",
        "            s.add(i)\n",
        "        p = [okt.pos(word) for word in s]\n",
        "        s = set()\n",
        "        for idx in range(len(p)):\n",
        "            ls = p[idx]\n",
        "            for j in range(len(ls)):\n",
        "                tag = ls[j][1]\n",
        "                word = ls[j][0]\n",
        "                if tag == \"Noun\":\n",
        "                    s.add(word)\n",
        "        self.keys = []\n",
        "        for temp in s:\n",
        "            self.keys.append(\" \" + temp)\n",
        "        print(\"KEYS: \", self.keys)\n",
        "\n",
        "    def frameParse(self, id):\n",
        "        parser = frame_parser.FrameParser(model_path=self.path, language='ko')\n",
        "        ps = parser.parser(self.docs[id], sent_id='1', result_format='conll')\n",
        "        return ps\n",
        "\n",
        "    def extractFrame(self):\n",
        "        self.final = {}\n",
        "        for paragraph in self.docs:\n",
        "            print(\"PARAGRAPH: \", self.docs)\n",
        "            for idx in range(len(paragraph)):\n",
        "                parsed = frameParse(paragraph[idx])  # candidates 생성\n",
        "                self.final.setdefault(idx, str)\n",
        "                parsedList = LinkedList()\n",
        "                for j in range(len(parsed)):\n",
        "                    parsed_candidate = parsed[j]\n",
        "                    new_node = Node(parsed_candidate)\n",
        "                    if j == 0:\n",
        "                        old_node = new_node\n",
        "                        parsedList.head = old_node\n",
        "                    elif j == len(parsed) - 1:\n",
        "                        old_node.next = new_node\n",
        "                        new_node.next = None\n",
        "                        print(idx, '  ', parsedList)\n",
        "                        self.final[idx] = parsedList\n",
        "                    else:\n",
        "                        old_node.next = new_node\n",
        "                        old_node = new_node\n",
        "\n",
        "    def findConsecutiveBIO(self, words, tag):\n",
        "        began = False\n",
        "        count = 1\n",
        "        self.que = deque(words)\n",
        "        for i in range(len(words)):\n",
        "            if tag[i] == 'O' and not began:\n",
        "                self.que.popleft()\n",
        "                began = False\n",
        "            if tag[i] == 'O' and began:\n",
        "                self.que.pop()\n",
        "                began = False\n",
        "            if tag[i].startswith('B'):\n",
        "                began = True\n",
        "            if began & tag[i].startswith('I'):\n",
        "                count += 1\n",
        "        return self.que\n",
        "\n",
        "    def extractRelation(self):\n",
        "        s = \"\"\n",
        "        self.res = [] * 20\n",
        "        for k, v in self.final.items():\n",
        "            a = v.head\n",
        "            try:\n",
        "                while a is not None:\n",
        "                    i = 0\n",
        "                    temp = \" \"\n",
        "                    if len(a.words) == len(a.tags):\n",
        "                        q = findConsecutiveBIO(a.words, a.tags)\n",
        "                        print(f\"The que {q}\")\n",
        "                    else:\n",
        "                        print(\"ERROR OCCURED\")\n",
        "\n",
        "                    count = len(a.words)\n",
        "                    for tag in a.tags:\n",
        "                        if tag.startswith(\"O\"):\n",
        "                            count -= 1\n",
        "                    if count > len(a.words) * 0.45:\n",
        "                        print(f\"The threshold is reached !!😏 The count is {count}\")\n",
        "                        temp = \" \".join(q)\n",
        "                        words = a.words\n",
        "                        tags = a.tags\n",
        "                    else:\n",
        "                        temp = None\n",
        "                        words = None\n",
        "                        tags = None\n",
        "                    self.res[i] = (words, tag, temp)\n",
        "                    i+=1\n",
        "                    a = a.next\n",
        "            except:\n",
        "                pass\n",
        "        self.res = [j for j in self.res if j]\n",
        "\n",
        "    def roles(self):\n",
        "        for k, tup in self.res.items():\n",
        "            a, b, c = tup\n",
        "            roles = [0] * len(b)\n",
        "            words = [0] * len(b)\n",
        "            for i in range(len(b)):\n",
        "                try:\n",
        "                    roles[i] = b[i].split(\"-\")[1]\n",
        "                except:\n",
        "                    roles[i] = b[i].split(\"-\")[0]\n",
        "            print(roles)\n",
        "            for _ in roles:\n",
        "                pass\n",
        "\n",
        "class Summarize(Highlight):\n",
        "    def __init__(self, text):\n",
        "      super().__init__(text)\n",
        "\n",
        "    def summarize(text):\n",
        "        url = \"https://api.smrzr.io/summarize?ratio=0.15\"\n",
        "        headers = {\n",
        "            'content-type': 'raw/text',\n",
        "            'origin': 'https://smrzr.io',\n",
        "            'referer': 'https://smrzr.io/',\n",
        "            'sec-fetch-dest': 'empty',\n",
        "            'sec-fetch-mode': 'cors',\n",
        "            'sec-fetch-site': 'same-site',\n",
        "            \"user-agent\": \"Mozilla/5.0\"\n",
        "        }\n",
        "        resp = requests.post(url, headers=headers, data=text.encode('utf-8'))\n",
        "        assert resp.status_code == 200\n",
        "\n",
        "        summary = resp.json()['summary']\n",
        "        temp = summary.split('\\n')\n",
        "        return temp\n",
        "\n",
        "\n",
        "    def summarizeSlow(text, ratio, model_name='Distil'):\n",
        "        \"\"\"keys for kakao translation api service\n",
        "        002558914cc8caacc923bbb4f9d6a772\n",
        "        03e207126164816f438e90f73832984c\n",
        "        18fea080a0db2ee0346353e8db4466e8\n",
        "        \"\"\"\n",
        "        global model\n",
        "        if model_name == \"Distil\":\n",
        "            model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=True)\n",
        "            tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "        elif model_name == \"Bert\":\n",
        "            bmodel = AutoModel.from_pretrained('bert-base-multilingual-uncased', output_hidden_states=True)\n",
        "            configuration = bmodel.config\n",
        "            configuration.vocab_size = 32000\n",
        "            configuration.max_position_embeddings = 384\n",
        "            configuration.hidden_size = 1024\n",
        "            configuration.num_attention_heads = 16\n",
        "            configuration.output_hidden_states = True\n",
        "            model = BertModel.from_pretrained(\"/Users/yoonk/Downloads/large_v2.bin\", config=configuration)\n",
        "            tokenizer = BertTokenizer.from_pretrained(\"/Users/yoonk/Downloads/large_v2_32k_vocab.txt\")\n",
        "\n",
        "        summarizer = Summarizer(custom_model=model, custom_tokenizer=tokenizer)\n",
        "        url = 'https://kapi.kakao.com/v1/translation/translate'\n",
        "        before_lang = 'kr'\n",
        "        after_lang = 'en'\n",
        "        KEY = '18fea080a0db2ee0346353e8db4466e8'\n",
        "        header = {\n",
        "            \"Authorization\": 'KakaoAK {}'.format(KEY)\n",
        "        }\n",
        "\n",
        "        data = {\n",
        "            \"src_lang\": before_lang,\n",
        "            \"target_lang\": after_lang,\n",
        "            \"query\": text\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, headers=header, params=data)\n",
        "        result = response.json()\n",
        "        translated = result['translated_text'][0]\n",
        "        result_eng = summarizer(translated, ratio=ratio)\n",
        "\n",
        "        before_lang = 'en'\n",
        "        after_lang = 'kr'\n",
        "        data = {\n",
        "            \"src_lang\": before_lang,\n",
        "            \"target_lang\": after_lang,\n",
        "            \"query\": ''.join(result_eng)\n",
        "        }\n",
        "        before_lang = 'en'\n",
        "        after_lang = 'kr'\n",
        "\n",
        "        response = requests.get(url, headers=header, params=data)\n",
        "        result_kor = response.json()\n",
        "        summarized = result_kor['translated_text'][0]\n",
        "\n",
        "        return summarized\n",
        "\n",
        "\n",
        "    def summarizeTextRank(text, max=3):\n",
        "        tr = TextRank(text)\n",
        "        return tr.summarize(max)\n",
        "\n",
        "    def summarizeLexRank(text,num=3):\n",
        "        lr = LexRank()\n",
        "        lr.summarize(text)\n",
        "        summaries = lr.probe(num)\n",
        "        return summaries\n",
        "\n",
        "\n",
        "    text = '''\n",
        "    TEXT2PPTX의 구현하기 위해 two track process를 거쳤습니다. 대본을 피피티로 옮기기 전에 요약하고 분석하기 위해 자연어처리의 최신 기술을 다수 사용하였습니다. 또한 파이썬에서 파워포인트의 소스에 접근하기 위해 xml 코드를 심층적으로 분석했습니다. 그 결과 사용자가 자연어로 쓰인 대본을 텍2피에 제공하면 높은 수준의 피피티로 제공할 수 있게 되었습니다.\n",
        "    '''\n",
        "\n",
        "    def ensembleSummarize(text):\n",
        "        sentences=kss.split_sentences(text)\n",
        "        n=len(sentences)\n",
        "        s = [0]*n\n",
        "        for idx in range(len(sentences)):\n",
        "            if sentences[idx] in summarize(text):\n",
        "                s[idx]+=1\n",
        "            if sentences[idx] in summarizeLexRank(text):\n",
        "                s[idx]+=1\n",
        "            if sentences[idx] in summarizeTextRank(text).split(\"\\n\"):\n",
        "                s[idx]+=1\n",
        "        i = s.index(max(s))\n",
        "        return sentences[i]\n",
        "\n",
        "    print(\"BERT\",summarize(text))\n",
        "    print(\"ensemble\",ensembleSummarize(text))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_v5o0el7I9ur",
        "colab": {}
      },
      "source": [
        "text = '''\n",
        "평등은 자유와 더불어 근대 사회의 핵심 이념으로 자리 잡고있다. 인간은 가령 인종이나 성별과 상관없이 누구나 평등하다고 생각한다. 모든 인간은 평등하다고 말하는데, 이 말은 무슨 뜻일까? 그리고 그 근거는 무엇인가? \n",
        "일단 이 말을 모든 인간을 모든 측면에서 똑같이 대우하는 절대칙 평등으로 생각하는 이는 없다. 인간은 저마다 다르게 가지고 대어난 능력과 소질을 똑같게 만들 수 없기 때문이다. 절대적 평등은 개인의 개성이냐 자율성 등의 가치와 충돌하기도 한다. 평등에 대한 요구는 모든 불평등을 악으로 보는 것이 아니라 충분한 이유가 제시되지 않은 불평등을 제거하는 데 목표를 두고 있다. ‘이유 없는 차별 금지’라는 조건적 평등 원칙은 차별 대우를 할 때는 이유를 제시할 것을 요구하고 있다. 이것은 어떤 이유가 제시된다면 특정한 부류에 속하는 사람들에게는 평등한 대우를, 그 부류에 속하지 않는 사람들에게는 차별저 대우를 하는 것을 허용한다. 그렇다면 사람들을 특정한 부류로 구분하는 기준은 무엇인가? 이것은 바로 평등의 근거에 대한 물음이다.\n",
        "근대의 여러 인권 선언에 나타난 평등 개념은 개인들 사이의 평등성을 타고난 자연적 권리로 간주하였다. 하지만 이러한 자연권 이론은 무엇이 자연적 권리이고 권리의 존재가 자명한 이유가 무엇인지 등의 문제에 부딪히게 된다. 그래서 롤스는 기존의 자연권 사상에 의존하지 않는 방시으로 인간 평등의 근거를 마런하려고 한다. 그는 어떤 규칙이 공평하고 일관되게 운영되며, 그 규칙에 따라 유사한 경우는 유사하게 취급된다면 형식적 정의는 실현된다고 본다. 하지만 롤스는 형식저 정의에 따라 규칙을 준수하는 것만으로는 정의를 담보할 수 없다고 생각한다. 그 규칙이 더 높은 도덕적 권위를 지닌 다른 이넘과 충돌할 수 었기에, 실질적 정의가 보장되기 위해서는 규칙의 내용이 중요한 것이다.\n",
        "롤스는 인간 평등의 근거를 설명하면서 영역 성질 (range property) 개념을 도입한다. 예를 들어 어떤 원의 내부에 있는 점들은 그 위치가 서로 다르지만 원의 내부에 있다는 점에서 동일한 영역 성질을 갖는다. 반면에 원의 내부에 있는 집과 원의 외부에 였는 점은원의 경계선을 기준으로 서로 다른 영역 성질을 갖는다. 그는 평등한 대우를 받기 위한 영역 성질로서 ‘도덕적 인격'을 제시한다. 도덕적 인격이란 도덕적 호소가 가능하고 그런 호소에 관심을 기이는 능력이 있다는 것인데, 이 능력을 최소치만 갖고 있다면 평등한 대우에 대한 권한을 갖게 된다. 도덕적 인격이라고 해서 도덕적으로 훌륭하다는 뜻이 아니라 도덕과 무관하다는 말과 대비되는 뜻으로 쓰고 있다. 그런데 어린 아이는 인격체로서의 최소한의 기준을 충족하고 있는지가 논란이 될 수 있다. 이에 대해 롤스는 도덕적 인격을 규정하는 최소한의 요구 조건은 잠재적 능력이지 그것의 실현 여부가 아니기에 어린 아이도 평등한 존재라고 말한다. 싱어는 위와 같은 롤스의 시도를 비판한다. 도덕에 대한 민감성의 수준은 사람에 따라 다르다. 그래서 도덕적 인격의 능력이 그렇게 중요하다면 그것을 갖춘 정도에 따라 도덕적 위계를 다르게 하지 말아야 할 이유가 분명하지 않다고 말한다. \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GReuNdb-TtD",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrwJKcTB-S8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S7zGTQO4BUO7",
        "colab": {}
      },
      "source": [
        "# 1st sentence : GOALS & MEANS \n",
        "\n",
        "parsed = parser.parser(ls[0], sent_id='1', result_format='conll')\n",
        "words = parsed[0][0]\n",
        "print(\"😀 Words vector from sentence \",words)\n",
        "q = [-1]*len(parsed)\n",
        "for i in range(len(parsed)):\n",
        "  count=0\n",
        "  for element in parsed[i][3]:\n",
        "    if element.startswith(\"O\"):\n",
        "      count+=1\n",
        "  q[i] = len(words) - count\n",
        "hq = heapq.nlargest(2, q)\n",
        "\n",
        "\n",
        "print(\"Number of parsed candidates \",len(parsed))\n",
        "print(q, heapq.nlargest(2, q)) # 첫번째 경우 사용\n",
        "\n",
        "\n",
        "words = parsed[0][0]\n",
        "roles = parsed[0][2]\n",
        "tagged = parsed[0][3]\n",
        "for idx in range(len(tagged)):\n",
        "  try:\n",
        "    roles[i] = tagged[i].split(\"-\")[1]\n",
        "  except:\n",
        "    roles[i] = tagged[i].split(\"-\")[0]\n",
        "\n",
        "print(\"roles \",roles)\n",
        "\n",
        "goals = []\n",
        "means = []\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Goal\":\n",
        "    goals.append(words[_])\n",
        "  elif roles[_]=='Means':\n",
        "    means.append(words[_])\n",
        "  elif roles[_]==\"Purpose\":\n",
        "    purpose.append(words[_])\n",
        "  elif roles[_]==\"Instrument\":\n",
        "    instrument.append(words[_])\n",
        "  elif roles[_]!=\"_\": # using 포함\n",
        "    instrument.append(words[_])\n",
        "\n",
        "# MEANS\n",
        "MEANS = ' '.join(means)\n",
        "pos = h.pos(MEANS)\n",
        "imp = []\n",
        "for tup in h.pos(MEANS):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "MEANS_TRUNCATED = ' '.join(imp)\n",
        "print(MEANS_TRUNCATED)\n",
        "\n",
        "# GOALS\n",
        "GOAL = ' '.join(goals)\n",
        "pos = h.pos(GOAL)\n",
        "imp = []\n",
        "for tup in h.pos(GOAL):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "GOAL_TRUNCATED = ' '.join(imp)\n",
        "print(GOAL_TRUNCATED)\n",
        "\n",
        "final_string = MEANS_TRUNCATED + ' -- >' + GOAL_TRUNCATED\n",
        "print(\"😀 final string to be inserted\",final_string)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}