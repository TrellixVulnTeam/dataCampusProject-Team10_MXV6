{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutomatedRelationExtraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoonkim313/dataCampusProject-Team10/blob/master/Relation%20Extraction/5.AutomatedRelationExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "obGBWFLnIypX",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e0077c16-92b2-4ddc-91c6-3d4734326d5c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install transformers\n",
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk \n",
        "!pip3 install konlpy\n",
        "!pip install kss\n",
        "%cd /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
        "!pip install soykeyword\n",
        "from frameBERT import frame_parser\n",
        "path=\"/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\""
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:13 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Fetched 88.7 kB in 1s (66.8 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "g++ is already the newest version (4:7.4.0-1ubuntu2.3).\n",
            "openjdk-8-jdk is already the newest version (8u265-b01-0ubuntu2~18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.3)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.0.2)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: kss in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "Collecting soykeyword\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/e5/df6ca4d5e9e92e423093703db91befee391644631f43ede1ef01d63c07dd/soykeyword-0.0.14-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from soykeyword) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from soykeyword) (1.18.5)\n",
            "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.6/dist-packages (from soykeyword) (5.4.8)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->soykeyword) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->soykeyword) (0.16.0)\n",
            "Installing collected packages: soykeyword\n",
            "Successfully installed soykeyword-0.0.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MbmSXGrwJ585",
        "colab": {}
      },
      "source": [
        "import kss\n",
        "from konlpy.tag import Hannanum\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import heapq\n",
        "import pandas as pd\n",
        "from operator import itemgetter\n",
        "from collections import deque\n",
        "from ast import literal_eval\n",
        "from collections import defaultdict\n",
        "from soykeyword.lasso import LassoKeywordExtractor\n",
        "h = Hannanum()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_v5o0el7I9ur",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "52667588-62ff-4ab8-f9f4-c4a4b6d37b71"
      },
      "source": [
        "text = '''\n",
        "평등은 자유와 더불어 근대 사회의 핵심 이념으로 자리 잡고\n",
        "있다. 인간은 가령 인종이나 성별과 상관없이 누구나 평등하다고\n",
        "생각한다. 모든 인간은 평등하다고 말하는데, 이 말은 무슨 뜻일까?\n",
        "그리고 그 근거는 무엇인가? 일단 이 말을 모든 인간을 모든\n",
        "측면에서 똑같이 대우하는 절대칙 평등으로 생각하는 이는 없다.\n",
        "인간은 저마다 다르게 가지고 대어난 능력과 소질을 똑같게 만들\n",
        "수 없기 때문이다. 절대적 평등은 개인의 개성이냐 자율성 등의\n",
        "가치와 충돌하기도 한다.\n",
        "평등에 대한 요구는 모든 불평등을 악으로 보는 것이 아니라\n",
        "충분한 이유가 제시되지 않은 불평등을 제거하는 데 목표를 두고\n",
        "있다. ‘이유 없는 차별 금지’라는 조건적 평등 원칙은 차별 대우를\n",
        "할 때는 이유를 제시할 것을 요구하고 있다. 이것은 어떤 이유가\n",
        "제시된다면 특정한 부류에 속하는 사람들에게는 평등한 대우를,\n",
        "그 부류에 속하지 않는 사람들에게는 차별저 대우를 하는 것을\n",
        "허용한다. 그렇다면 사람들을 특정한 부류로 구분하는 기준은\n",
        "무엇인가? 이것은 바로 평등의 근거에 대한 물음이다.\n",
        "근대의 여러 인권 선언에 나타난 평등 개념은 개인들 사이의\n",
        "평등성을 타고난 자연적 권리로 간주하였다. 하지만 이러한 자연권\n",
        "이론은 무엇이 자연적 권리이고 권리의 존재가 자명한 이유가\n",
        "무엇인지 등의 문제에 부딪히게 된다. 그래서 롤스는 기존의\n",
        "자연권 사상에 의존하지 않는 방시으로 인간 평등의 근거를 마런\n",
        "하려고 한다. 그는 어떤 규칙이 공평하고 일관되게 운영되며, 그\n",
        "규칙에 따라 유사한 경우는 유사하게 취급된다면 형식적 정의는\n",
        "실현된다고 본다. 하지만 롤스는 형식저 정의에 따라 규칙을 준수하는 것만으로는 정의를 담보할 수 없다고 생각한다. 그 규칙이\n",
        "더 높은 도덕적 권위를 지닌 다른 이넘과 충돌할 수 었기에,\n",
        "실질적 정의가 보장되기 위해서는 규칙의 내용이 중요한 것이다.\n",
        "롤스는 인간 평등의 근거를 설명하면서 영역 성질 (range property)\n",
        "개념을 도입한다. 예를 들어 어떤 원의 내부에 있는 점들은 그 위\n",
        "치가 서로 다르지만 원의 내부에 있다는 점에서 동일한 영역 성질\n",
        "을 갖는다. 반면에 원의 내부에 있는 집과 원의 외부에 였는 점은\n",
        "원의 경계선을 기준으로 서로 다른 영역 성질을 갖는다. 그는 평등\n",
        "한 대우를 받기 위한 영역 성질로서 ‘도덕적 인격'을 제시한다.\n",
        "도덕적 인격이란 도덕적 호소가 가능하고 그런 호소에 관심을 기울\n",
        "이는 능력이 있다는 것인데, 이 능력을 최소치만 갖고 있다면 평등한\n",
        "대우에 대한 권한을 갖게 된다. 도덕적 인격이라고 해서 도덕적으로\n",
        "훌륭하다는 뜻이 아니라 도덕과 무관하다는 말과 대비되는 뜻으로\n",
        "쓰고 있다. 그런데 어린 아이는 인격체로서의 최소한의 기준을 충족하고 있는지가 논란이 될 수 있다. 이에 대해 롤스는 도덕적\n",
        "인격을 규정하는 최소한의 요구 조건은 잠재적 능력이지 그것의\n",
        "실현 여부가 아니기에 어린 아이도 평등한 존재라고 말한다.\n",
        "성어는 위와 같은 롤스의 시도를 비판한다. 도덕에 대한 민감성의\n",
        "수준은 사람에 따라 다르다. 그래서 도덕적 인격의 능력이 그렇게\n",
        "중요하다면 그것을 갖춘 정도에 따라 도덕적 위계를 다르게 하지\n",
        "말아야 할 이유가 분명하지 않다고 말한다. 그리고 평등한 권리를\n",
        "갖는 존재가 되기 위한 최소한의 경계선을 어디에 그어야 하는지도\n",
        "문제로 남는다고 본다. 한편 롤스에서는 도덕적인 능력을 태어날\n",
        "때부터 가지고 있지 않거나 영구적으로 상실한 사람은 도더저\n",
        "지위를 가지고 있지 못하게 되는데, 이는 몽상적인 평등 개념과 어긋난다. 그래서 싱어는 평등의 근거로 ‘이익 평등 고려의 원칙'을\n",
        "내세운다. 그에 따르면 어떤 존재가 이익, 즉 이해관계를 갖기\n",
        "위해서는 기본적으로 고통과 쾌락을 느낄 수 있는 능력을 갖고\n",
        "있어야 한다. 그리고 그 능력을 가진 존재는 이해관계를 가진\n",
        "존재이기 때문에 평등한 도더저 고려의 대상이 된다. 이때 이해\n",
        "관계가 강한 존재를 더 대우하는 것이 가능하다. 반면에 그 능력을\n",
        "갖지 못한 존재는 아무런 선호냐 이익도 갖지 않기 때문에 평등한\n",
        "도덕적 고려의 대상이 되지 않는다.\n",
        "'''\n",
        "text = re.sub('\\n',' ',text)\n",
        "text = re.sub(\"'\",' ',text)\n",
        "print(text)\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 평등은 자유와 더불어 근대 사회의 핵심 이념으로 자리 잡고 있다. 인간은 가령 인종이나 성별과 상관없이 누구나 평등하다고 생각한다. 모든 인간은 평등하다고 말하는데, 이 말은 무슨 뜻일까? 그리고 그 근거는 무엇인가? 일단 이 말을 모든 인간을 모든 측면에서 똑같이 대우하는 절대칙 평등으로 생각하는 이는 없다. 인간은 저마다 다르게 가지고 대어난 능력과 소질을 똑같게 만들 수 없기 때문이다. 절대적 평등은 개인의 개성이냐 자율성 등의 가치와 충돌하기도 한다. 평등에 대한 요구는 모든 불평등을 악으로 보는 것이 아니라 충분한 이유가 제시되지 않은 불평등을 제거하는 데 목표를 두고 있다. ‘이유 없는 차별 금지’라는 조건적 평등 원칙은 차별 대우를 할 때는 이유를 제시할 것을 요구하고 있다. 이것은 어떤 이유가 제시된다면 특정한 부류에 속하는 사람들에게는 평등한 대우를, 그 부류에 속하지 않는 사람들에게는 차별저 대우를 하는 것을 허용한다. 그렇다면 사람들을 특정한 부류로 구분하는 기준은 무엇인가? 이것은 바로 평등의 근거에 대한 물음이다. 근대의 여러 인권 선언에 나타난 평등 개념은 개인들 사이의 평등성을 타고난 자연적 권리로 간주하였다. 하지만 이러한 자연권 이론은 무엇이 자연적 권리이고 권리의 존재가 자명한 이유가 무엇인지 등의 문제에 부딪히게 된다. 그래서 롤스는 기존의 자연권 사상에 의존하지 않는 방시으로 인간 평등의 근거를 마런 하려고 한다. 그는 어떤 규칙이 공평하고 일관되게 운영되며, 그 규칙에 따라 유사한 경우는 유사하게 취급된다면 형식적 정의는 실현된다고 본다. 하지만 롤스는 형식저 정의에 따라 규칙을 준수하는 것만으로는 정의를 담보할 수 없다고 생각한다. 그 규칙이 더 높은 도덕적 권위를 지닌 다른 이넘과 충돌할 수 었기에, 실질적 정의가 보장되기 위해서는 규칙의 내용이 중요한 것이다. 롤스는 인간 평등의 근거를 설명하면서 영역 성질 (range property) 개념을 도입한다. 예를 들어 어떤 원의 내부에 있는 점들은 그 위 치가 서로 다르지만 원의 내부에 있다는 점에서 동일한 영역 성질 을 갖는다. 반면에 원의 내부에 있는 집과 원의 외부에 였는 점은 원의 경계선을 기준으로 서로 다른 영역 성질을 갖는다. 그는 평등 한 대우를 받기 위한 영역 성질로서 ‘도덕적 인격 을 제시한다. 도덕적 인격이란 도덕적 호소가 가능하고 그런 호소에 관심을 기울 이는 능력이 있다는 것인데, 이 능력을 최소치만 갖고 있다면 평등한 대우에 대한 권한을 갖게 된다. 도덕적 인격이라고 해서 도덕적으로 훌륭하다는 뜻이 아니라 도덕과 무관하다는 말과 대비되는 뜻으로 쓰고 있다. 그런데 어린 아이는 인격체로서의 최소한의 기준을 충족하고 있는지가 논란이 될 수 있다. 이에 대해 롤스는 도덕적 인격을 규정하는 최소한의 요구 조건은 잠재적 능력이지 그것의 실현 여부가 아니기에 어린 아이도 평등한 존재라고 말한다. 성어는 위와 같은 롤스의 시도를 비판한다. 도덕에 대한 민감성의 수준은 사람에 따라 다르다. 그래서 도덕적 인격의 능력이 그렇게 중요하다면 그것을 갖춘 정도에 따라 도덕적 위계를 다르게 하지 말아야 할 이유가 분명하지 않다고 말한다. 그리고 평등한 권리를 갖는 존재가 되기 위한 최소한의 경계선을 어디에 그어야 하는지도 문제로 남는다고 본다. 한편 롤스에서는 도덕적인 능력을 태어날 때부터 가지고 있지 않거나 영구적으로 상실한 사람은 도더저 지위를 가지고 있지 못하게 되는데, 이는 몽상적인 평등 개념과 어긋난다. 그래서 싱어는 평등의 근거로 ‘이익 평등 고려의 원칙 을 내세운다. 그에 따르면 어떤 존재가 이익, 즉 이해관계를 갖기 위해서는 기본적으로 고통과 쾌락을 느낄 수 있는 능력을 갖고 있어야 한다. 그리고 그 능력을 가진 존재는 이해관계를 가진 존재이기 때문에 평등한 도더저 고려의 대상이 된다. 이때 이해 관계가 강한 존재를 더 대우하는 것이 가능하다. 반면에 그 능력을 갖지 못한 존재는 아무런 선호냐 이익도 갖지 않기 때문에 평등한 도덕적 고려의 대상이 되지 않는다. \n",
            "['평등은 자유와 더불어 근대 사회의 핵심 이념으로 자리 잡고 있다. 인간은 가령 인종이나 성별과 상관없이 누구나 평등하다고 생각한다. 모든 인간은 평등하다고 말하는데, 이 말은 무슨 뜻일까? 그리고 그 근거는 무엇인가? 일단 이 말을 모든 인간을 모든 측면에서 똑같이 대우하는 절대칙 평등으로 생각하는 이는 없다. 인간은 저마다 다르게 가지고 대어난 능력과 소질을 똑같게 만들 수 없기 때문이다. 절대적 평등은 개인의 개성이냐 자율성 등의 가치와 충돌하기도 한다. 평등에 대한 요구는 모든 불평등을 악으로 보는 것이 아니라 충분한 이유가 제시되지 않은 불평등을 제거하는 데 목표를 두고 있다. ‘이유 없는 차별 금지’라는 조건적 평등 원칙은 차별 대우를 할 때는 이유를 제시할 것을 요구하고 있다. 이것은 어떤 이유가 제시된다면 특정한 부류에 속하는 사람들에게는 평등한 대우를, 그 부류에 속하지 않는 사람들에게는 차별저 대우를 하는 것을 허용한다. 그렇다면 사람들을 특정한 부류로 구분하는 기준은 무엇인가? 이것은 바로 평등의 근거에 대한 물음이다. 근대의 여러 인권 선언에 나타난 평등 개념은 개인들 사이의 평등성을 타고난 자연적 권리로 간주하였다.', '하지만 이러한 자연권 이론은 무엇이 자연적 권리이고 권리의 존재가 자명한 이유가 무엇인지 등의 문제에 부딪히게 된다. 그래서 롤스는 기존의 자연권 사상에 의존하지 않는 방시으로 인간 평등의 근거를 마런 하려고 한다. 그는 어떤 규칙이 공평하고 일관되게 운영되며, 그 규칙에 따라 유사한 경우는 유사하게 취급된다면 형식적 정의는 실현된다고 본다. 하지만 롤스는 형식저 정의에 따라 규칙을 준수하는 것만으로는 정의를 담보할 수 없다고 생각한다. 그 규칙이 더 높은 도덕적 권위를 지닌 다른 이넘과 충돌할 수 었기에, 실질적 정의가 보장되기 위해서는 규칙의 내용이 중요한 것이다. 롤스는 인간 평등의 근거를 설명하면서 영역 성질 (range property) 개념을 도입한다. 예를 들어 어떤 원의 내부에 있는 점들은 그 위 치가 서로 다르지만 원의 내부에 있다는 점에서 동일한 영역 성질 을 갖는다. 반면에 원의 내부에 있는 집과 원의 외부에 였는 점은 원의 경계선을 기준으로 서로 다른 영역 성질을 갖는다. 그는 평등 한 대우를 받기 위한 영역 성질로서 ‘도덕적 인격 을 제시한다. 도덕적 인격이란 도덕적 호소가 가능하고 그런 호소에 관심을 기울 이는 능력이 있다는 것인데, 이 능력을 최소치만 갖고 있다면 평등한 대우에 대한 권한을 갖게 된다. 도덕적 인격이라고 해서 도덕적으로 훌륭하다는 뜻이 아니라 도덕과 무관하다는 말과 대비되는 뜻으로 쓰고 있다. 그런데 어린 아이는 인격체로서의 최소한의 기준을 충족하고 있는지가 논란이 될 수 있다. 이에 대해 롤스는 도덕적 인격을 규정하는 최소한의 요구 조건은 잠재적 능력이지 그것의 실현 여부가 아니기에 어린 아이도 평등한 존재라고 말한다. 성어는 위와 같은 롤스의 시도를 비판한다. 도덕에 대한 민감성의 수준은 사람에 따라 다르다. 그래서 도덕적 인격의 능력이 그렇게 중요하다면 그것을 갖춘 정도에 따라 도덕적 위계를 다르게 하지 말아야 할 이유가 분명하지 않다고 말한다. 그리고 평등한 권리를 갖는 존재가 되기 위한 최소한의 경계선을 어디에 그어야 하는지도 문제로 남는다고 본다. 한편 롤스에서는 도덕적인 능력을 태어날 때부터 가지고 있지 않거나 영구적으로 상실한 사람은 도더저 지위를 가지고 있지 못하게 되는데, 이는 몽상적인 평등 개념과 어긋난다. 그래서 싱어는 평등의 근거로 ‘이익 평등 고려의 원칙 을 내세운다. 그에 따르면 어떤 존재가 이익, 즉 이해관계를 갖기 위해서는 기본적으로 고통과 쾌락을 느낄 수 있는 능력을 갖고 있어야 한다. 그리고 그 능력을 가진 존재는 이해관계를 가진 존재이기 때문에 평등한 도더저 고려의 대상이 된다. 이때 이해 관계가 강한 존재를 더 대우하는 것이 가능하다. 반면에 그 능력을 갖지 못한 존재는 아무런 선호냐 이익도 갖지 않기 때문에 평등한 도덕적 고려의 대상이 되지 않는다.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtYkXWkqN4rO",
        "colab_type": "text"
      },
      "source": [
        "## Keyword Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8C4E0m2K1ff",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "79b83d07-0782-41d3-aeaf-836db891d2a3"
      },
      "source": [
        "from krwordrank.word import KRWordRank\n",
        "docs = kss.split_sentences(text)\n",
        "a = []\n",
        "b = []\n",
        "a = ' '.join(docs[:13])\n",
        "b = ' '.join(docs[13:])\n",
        "documents = [a,b]\n",
        "print(documents)\n",
        "wordrank_extractor = KRWordRank(min_count=4, max_length=10)\n",
        "keywords, rank, graph = wordrank_extractor.extract(documents, num_keywords=50)\n",
        "pprint(keywords)\n",
        "p=[]\n",
        "kw=[]\n",
        "for k, v in keywords.items():\n",
        "  p.append(h.pos(k))\n",
        "for ls in p:\n",
        "  for tup in ls:\n",
        "      print(tup[0],tup[1])\n",
        "      if tup[1]==\"N\":\n",
        "        kw.append(tup[0])\n",
        "print(kw)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'권리': 0.6117891845742025,\n",
            " '규칙': 1.1600366400776854,\n",
            " '근거': 1.266302285991514,\n",
            " '능력': 1.4960162876255603,\n",
            " '다르': 0.7188428731796166,\n",
            " '대우': 1.292746548345348,\n",
            " '대한': 0.7064500526673395,\n",
            " '도덕적': 2.9297005257749773,\n",
            " '따라': 1.380328033941605,\n",
            " '롤스는': 1.2214599951784773,\n",
            " '모든': 1.2173210047065806,\n",
            " '무엇': 0.4437546555359268,\n",
            " '사람': 0.3616353555453698,\n",
            " '성질': 0.8357632792387676,\n",
            " '어떤': 1.3331597298937496,\n",
            " '영역': 1.3170633092729913,\n",
            " '원의': 1.2167080242592405,\n",
            " '이유가': 1.6894097070247822,\n",
            " '인간': 1.8062621061153457,\n",
            " '인격': 0.9951593423956626,\n",
            " '있는': 0.8440517960395072,\n",
            " '있다.': 1.3678924056950432,\n",
            " '자연': 0.36611270873996604,\n",
            " '정의': 0.5951644075456364,\n",
            " '제시': 0.6079039712483222,\n",
            " '존재': 1.0660547263630094,\n",
            " '최소': 0.3625632457759096,\n",
            " '평등': 2.6076410810870496}\n",
            "도덕적 N\n",
            "평등 N\n",
            "인간 N\n",
            "이유 N\n",
            "가 J\n",
            "능력 N\n",
            "따르 P\n",
            "아 E\n",
            "있 P\n",
            "다 E\n",
            ". S\n",
            "어떤 M\n",
            "영역 N\n",
            "대우 N\n",
            "근거 N\n",
            "롤스 N\n",
            "는 J\n",
            "모든 M\n",
            "원의 N\n",
            "규칙 N\n",
            "존재 N\n",
            "인격 N\n",
            "있 P\n",
            "는 E\n",
            "성질 N\n",
            "다르 N\n",
            "대하 P\n",
            "ㄴ E\n",
            "권리 N\n",
            "제시 N\n",
            "정의 N\n",
            "무엇 N\n",
            "자연 N\n",
            "최소 N\n",
            "사람 N\n",
            "['도덕적', '평등', '인간', '이유', '능력', '영역', '대우', '근거', '롤스', '원의', '규칙', '존재', '인격', '성질', '다르', '권리', '제시', '정의', '무엇', '자연', '최소', '사람']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JS0FMkeN8Sb",
        "colab_type": "text"
      },
      "source": [
        "## Relation Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Af1flDKI9M8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "3de3afba-e435-4954-f1b0-a3a30615ceb5"
      },
      "source": [
        "parser = frame_parser.FrameParser(model_path=path, language='ko')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "srl model: framenet\n",
            "language: ko\n",
            "version: 1.2\n",
            "using viterbi: False\n",
            "using masking: True\n",
            "pretrained BERT: bert-base-multilingual-cased\n",
            "using TGT special token: True\n",
            "used dictionary:\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lu2idx.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lufrmap.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n",
            "...loaded model path: /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "...model is loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuHEQofWAsnA",
        "colab_type": "text"
      },
      "source": [
        "### Linked List Implemented to Show the whole list of sentences and their parsed tags in a consecutive manner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wIph64I5OnLZ",
        "colab": {}
      },
      "source": [
        "def split_into_sentences(object):\n",
        "  ls = kss.split_sentences(object)\n",
        "  return ls\n",
        "\n",
        "def frameParse(text):\n",
        "  parser = frame_parser.FrameParser(model_path=path, language='ko')\n",
        "  parsed = parser.parser(text, sent_id='1', result_format='conll')\n",
        "  return parsed\n",
        "  \n",
        "\n",
        "def sortCandidate(parsed, num_candidates):\n",
        "  for i in range(len(parsed)):\n",
        "    count=0\n",
        "    for element in parsed[i][3]:\n",
        "      if element.startswith(\"O\"):\n",
        "        count+=1\n",
        "  q[i] = len(words) - count\n",
        "  hq = heapq.nlargest(num_candidates, q)\n",
        "  indices.append(q.index(hq[idx]))\n",
        "  # for idx in range(num_candidates):\n",
        "  #   temp = parsed[q.index(hq[idx])]\n",
        "  #   words = temp[0]\n",
        "  #   [[row[i] for row in temp] for i in range(len(words))]\n",
        "  return indices\n",
        "\n",
        "\n",
        "def findConsecutiveBIO(words, tag):\n",
        "  began = False\n",
        "  count = 0\n",
        "  q = deque(words)\n",
        "  for i in range(len(words)):\n",
        "    if tag[i]=='O':\n",
        "      q.popleft()\n",
        "    if tag[i].startswith('B'):\n",
        "      began=True\n",
        "    if began & tag[i].startswith('I'):\n",
        "      count+=1\n",
        "  return q\n",
        "\n",
        "# Linked List 구현해서 parsed 후보군에 사용\n",
        "class Node:\n",
        "    def __init__(self, data):\n",
        "        self.words = data[0]\n",
        "        self.tags = data[3]\n",
        "        self.next = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str((self.words, self.tags))\n",
        "\n",
        "class LinkedList:\n",
        "  def __init__(self):\n",
        "      self.head = None\n",
        "\n",
        "  def __repr__(self):\n",
        "      node = self.head\n",
        "      nodes = []\n",
        "      while node is not None:\n",
        "          nodes.append(str(node.tags))\n",
        "          node = node.next\n",
        "      nodes.append(\"None\")\n",
        "      return ' -> '.join(nodes)\n",
        "  \n",
        "class LinkedList:\n",
        "  def __init__(self):\n",
        "      self.head = None\n",
        "\n",
        "  def __repr__(self):\n",
        "      node = self.head\n",
        "      nodes = []\n",
        "      while node is not None:\n",
        "          nodes.append(str(node.tags))\n",
        "          node = node.next\n",
        "      nodes.append(\"None\")\n",
        "      return ' -> '.join(nodes)\n",
        "\n",
        "def extractFrame(text):\n",
        "  ls = split_into_sentences(text)\n",
        "  final = {}\n",
        "  \n",
        "  for idx in range(len(ls)):\n",
        "    parsed = frameParse(ls[idx]) # candidates 생성\n",
        "    final.setdefault(idx,str)\n",
        "    parsedList =  LinkedList()\n",
        "    for j in range(len(parsed)):\n",
        "      parsed_candidate = parsed[j]\n",
        "      new_node = Node(parsed_candidate)\n",
        "      if j == 0:\n",
        "        old_node = new_node\n",
        "        parsedList.head = old_node\n",
        "      elif j==len(parsed)-1:\n",
        "        old_node.next = new_node\n",
        "        new_node.next = None\n",
        "        print(idx,'  ',parsedList)\n",
        "        final[idx] = parsedList\n",
        "      else:\n",
        "        old_node.next = new_node\n",
        "        old_node = new_node\n",
        "  return final\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD2QZjRqZaM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(text):\n",
        "  all = extractFrame(text)\n",
        "  s = \"\"\n",
        "  res = {}\n",
        "\n",
        "  i = 1\n",
        "  res.setdefault(i,tuple)\n",
        "  for k, v in all.items():\n",
        "    a = v.head\n",
        "    while a is not None:\n",
        "      temp = \" \"\n",
        "      if len(a.words) == len(a.tags):\n",
        "        q = findConsecutiveBIO(a.words, a.tags)\n",
        "      else:\n",
        "        print(\"ERROR OCCURED\")\n",
        "        \n",
        "      count = len(a.words)\n",
        "      for tag in a.tags:\n",
        "       if tag.startswith(\"O\"):\n",
        "          count -= 1\n",
        "      if count > len(a.words)*0.5:\n",
        "        print(\"The threshold is reached !! 😛 😝 😜 🤪 🤨 🧐 🤓 😎 🤩 🥳 😏\")\n",
        "        temp = \" \".join(q) \n",
        "        print(temp)\n",
        "        words = a.words\n",
        "        tags = a.tags\n",
        "      res[i] = (a.words, a.tags , \" \".join(q))\n",
        "      a = a.next\n",
        "    i+=1\n",
        "  return res\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIIiD0vrpdmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94d6d5dd-1a4c-43b5-d646-960fb1ef00f3"
      },
      "source": [
        "extracted = main(text)\n",
        "pprint(extracted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "srl model: framenet\n",
            "language: ko\n",
            "version: 1.2\n",
            "using viterbi: False\n",
            "using masking: True\n",
            "pretrained BERT: bert-base-multilingual-cased\n",
            "using TGT special token: True\n",
            "used dictionary:\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lu2idx.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lufrmap.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n",
            "...loaded model path: /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "...model is loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/utils.py:279: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  pred_logits = sm(masked_logit).view(1,-1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0    ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'O', 'O', 'O', 'I-Class', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Time', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'I-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'O', 'O', 'I-Trajector_event', 'I-Trajector_event', 'O', 'I-Trajector_event'] -> ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'I-Time', 'I-Time', 'I-Time', 'I-Time', 'I-Time', 'I-Time', 'I-Time', 'I-Time', 'B-Old', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'I-Means', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Time', 'O', 'I-Goal', 'I-Goal', 'O', 'O', 'O'] -> None\n",
            "srl model: framenet\n",
            "language: ko\n",
            "version: 1.2\n",
            "using viterbi: False\n",
            "using masking: True\n",
            "pretrained BERT: bert-base-multilingual-cased\n",
            "using TGT special token: True\n",
            "used dictionary:\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lu2idx.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lufrmap.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n",
            "...loaded model path: /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "...model is loaded\n",
            "1    ['B-Time', 'B-Theme', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['B-Time', 'B-Goal', 'I-Goal', 'O', 'B-Items', 'O', 'O', 'O', 'O'] -> None\n",
            "srl model: framenet\n",
            "language: ko\n",
            "version: 1.2\n",
            "using viterbi: False\n",
            "using masking: True\n",
            "pretrained BERT: bert-base-multilingual-cased\n",
            "using TGT special token: True\n",
            "used dictionary:\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lu2idx.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lufrmap.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n",
            "...loaded model path: /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "...model is loaded\n",
            "2    ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['B-Agent', 'I-Instrument', 'I-Instrument', 'I-Instrument', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['B-Entity', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['B-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> None\n",
            "The threshold is reached !! 😛 😝 😜 🤪 🤨 🧐 🤓 😎 🤩 🥳 😏\n",
            "이상 제 기능을 하지못할 때에 이를 대체하기 위해 이식을 실시한다.\n",
            "The threshold is reached !! 😛 😝 😜 🤪 🤨 🧐 🤓 😎 🤩 🥳 😏\n",
            "제 기능을 하지못할 때에 이를 대체하기 위해 이식을 실시한다.\n",
            "ERROR OCCURED\n",
            "{1: (['신체의',\n",
            "      '세포,',\n",
            "      '조직,',\n",
            "      '장기가',\n",
            "      '손상되어',\n",
            "      '더',\n",
            "      '이상',\n",
            "      '제',\n",
            "      '기능을',\n",
            "      '하지못할',\n",
            "      '때에',\n",
            "      '이를',\n",
            "      '대체하기',\n",
            "      '위해',\n",
            "      '이식을',\n",
            "      '실시한다.'],\n",
            "     ['O',\n",
            "      'O',\n",
            "      'I-Means',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'I-Time',\n",
            "      'O',\n",
            "      'I-Goal',\n",
            "      'I-Goal',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O'],\n",
            "     '대체하기 위해 이식을 실시한다.'),\n",
            " 2: (['이때', '이식으로', '옮겨', '붙이는', '세포,', '조직,', '장기를', '이식편이라', '한다.'],\n",
            "     ['B-Time', 'B-Goal', 'I-Goal', 'O', 'B-Items', 'O', 'O', 'O', 'O'],\n",
            "     '조직, 장기를 이식편이라 한다.'),\n",
            " 3: (['자신이나',\n",
            "      '일란성',\n",
            "      '쌍둥이의',\n",
            "      '이식편을',\n",
            "      '이용할',\n",
            "      '',\n",
            "      '수',\n",
            "      '없다면',\n",
            "      '다른',\n",
            "      '사람의',\n",
            "      '이식편으로',\n",
            "      '‘동종',\n",
            "      '이식’을',\n",
            "      '실시한다.'],\n",
            "     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
            "     '')}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvm4aMtEz1A1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "6f28df78-5b42-4d60-a3e2-d78b92ad9c89"
      },
      "source": [
        "pprint(extracted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: (['신체의',\n",
            "      '세포,',\n",
            "      '조직,',\n",
            "      '장기가',\n",
            "      '손상되어',\n",
            "      '더',\n",
            "      '이상',\n",
            "      '제',\n",
            "      '기능을',\n",
            "      '하지못할',\n",
            "      '때에',\n",
            "      '이를',\n",
            "      '대체하기',\n",
            "      '위해',\n",
            "      '이식을',\n",
            "      '실시한다.'],\n",
            "     ['O',\n",
            "      'O',\n",
            "      'I-Means',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'I-Time',\n",
            "      'O',\n",
            "      'I-Goal',\n",
            "      'I-Goal',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O'],\n",
            "     ' '),\n",
            " 2: (['이때', '이식으로', '옮겨', '붙이는', '세포,', '조직,', '장기를', '이식편이라', '한다.'],\n",
            "     ['B-Time', 'B-Goal', 'I-Goal', 'O', 'B-Items', 'O', 'O', 'O', 'O'],\n",
            "     ' '),\n",
            " 3: (['자신이나',\n",
            "      '일란성',\n",
            "      '쌍둥이의',\n",
            "      '이식편을',\n",
            "      '이용할',\n",
            "      '',\n",
            "      '수',\n",
            "      '없다면',\n",
            "      '다른',\n",
            "      '사람의',\n",
            "      '이식편으로',\n",
            "      '‘동종',\n",
            "      '이식’을',\n",
            "      '실시한다.'],\n",
            "     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
            "     ' ')}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQNkF3aGty_0",
        "colab_type": "text"
      },
      "source": [
        "### TODO : 여기서부터 extracted를 가지고 purpose / goal / effect / entity 등을 찾아야함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf62cktfw2C1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k, tup in extracted.items():\n",
        "  a, b, c = tup\n",
        "  roles = [0]*len(b)\n",
        "  words = [0]*len(b)\n",
        "  for i in range(len(b)):\n",
        "    try: \n",
        "      roles[i] = b[i].split(\"-\")[1] \n",
        "      \n",
        "    except: \n",
        "      roles[i] = b[i].split(\"-\")[0]\n",
        "  print(roles)\n",
        "  for _ in roles:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S7zGTQO4BUO7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "326d4f22-e8b0-4f94-f2dd-23251248d598"
      },
      "source": [
        "# 1st sentence : GOALS & MEANS \n",
        "\n",
        "parsed = parser.parser(ls[0], sent_id='1', result_format='conll')\n",
        "words = parsed[0][0]\n",
        "print(\"😀 Words vector from sentence \",words)\n",
        "q = [-1]*len(parsed)\n",
        "for i in range(len(parsed)):\n",
        "  count=0\n",
        "  for element in parsed[i][3]:\n",
        "    if element.startswith(\"O\"):\n",
        "      count+=1\n",
        "  q[i] = len(words) - count\n",
        "hq = heapq.nlargest(2, q)\n",
        "\n",
        "\n",
        "print(\"Number of parsed candidates \",len(parsed))\n",
        "print(q, heapq.nlargest(2, q)) # 첫번째 경우 사용\n",
        "\n",
        "\n",
        "words = parsed[0][0]\n",
        "roles = parsed[0][2]\n",
        "tagged = parsed[0][3]\n",
        "for idx in range(len(tagged)):\n",
        "  try:\n",
        "    roles[i] = tagged[i].split(\"-\")[1]\n",
        "  except:\n",
        "    roles[i] = tagged[i].split(\"-\")[0]\n",
        "\n",
        "print(\"roles \",roles)\n",
        "\n",
        "goals = []\n",
        "means = []\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Goal\":\n",
        "    goals.append(words[_])\n",
        "  if roles[_]=='Means':\n",
        "    means.append(words[_])\n",
        "\n",
        "# MEANS\n",
        "MEANS = ' '.join(means)\n",
        "pos = h.pos(MEANS)\n",
        "imp = []\n",
        "for tup in h.pos(MEANS):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "MEANS_TRUNCATED = ' '.join(imp)\n",
        "print(MEANS_TRUNCATED)\n",
        "\n",
        "# GOALS\n",
        "GOAL = ' '.join(goals)\n",
        "pos = h.pos(GOAL)\n",
        "imp = []\n",
        "for tup in h.pos(GOAL):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "GOAL_TRUNCATED = ' '.join(imp)\n",
        "print(GOAL_TRUNCATED)\n",
        "\n",
        "final_string = MEANS_TRUNCATED + ' -- >' + GOAL_TRUNCATED\n",
        "print(\"😀 final string to be inserted\",final_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['TEXT2PPTX의', '구현하기', '위해', 'two', 'track', 'process를', '거쳤습니다.'], ['_', '_', '위하다.v', '_', '_', '_', '_'], ['_', '_', 'Purpose', '_', '_', '_', '_'], ['B-Goal', 'I-Goal', 'O', 'O', 'I-Means', 'I-Means', 'I-Means']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F9zsJryuBRri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "dc2971b6-b2a7-444a-bcbe-9f3f7bef79f0"
      },
      "source": [
        "# 2nd sentence : PURPOSE & INSTRUMENT\n",
        "parsed = parser.parser(ls[1], sent_id='1', result_format='conll')\n",
        "words = parsed[0][0]\n",
        "print(\"😀 Words vector from sentence \",words)\n",
        "q = [-1]*len(parsed)\n",
        "for i in range(len(parsed)):\n",
        "  count=0\n",
        "  for element in parsed[i][3]:\n",
        "    if element.startswith(\"O\"):\n",
        "      count+=1\n",
        "  q[i] = len(words) - count\n",
        "\n",
        "\n",
        "print(\"Number of parsed candidates \",len(parsed))\n",
        "print(q, heapq.nlargest(3, q))\n",
        "print(itemgetter(*[1,5])(parsed))\n",
        "candidates = itemgetter(*[1,8])(parsed)\n",
        "for _ in candidates:\n",
        "  words = _[0]\n",
        "  tagged = _[3]\n",
        "  print(\"tagged vector of candidates: \",tagged)\n",
        "  roles = _[2]\n",
        "  for i in range(len(parsed)):\n",
        "    try:\n",
        "      roles[i] = tagged[i].split(\"-\")[1]\n",
        "    except:\n",
        "      roles[i] = tagged[i].split(\"-\")[0]\n",
        "  print(roles,f'Tagged words for the roles : {tagged}')\n",
        "# roles are selected as the one with purpose and instrument\n",
        "\n",
        "purpose = []\n",
        "instrument = []\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Purpose\":\n",
        "    purpose.append(words[_])\n",
        "    \n",
        "  elif roles[_]==\"Instrument\":\n",
        "    instrument.append(words[_])\n",
        "    \n",
        "  elif roles[_]!=\"_\": # using 포함\n",
        "    instrument.append(words[_])\n",
        "print(\"Instrument: \",instrument)\n",
        "print(\"Purpose: \",purpose)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# PURPOSE\n",
        "PURPOSE = ' '.join(purpose)\n",
        "pos = h.pos(PURPOSE)\n",
        "imp = []\n",
        "for tup in pos:\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "PURPOSE_TRUNCATED = ' '.join(imp)\n",
        "print(PURPOSE_TRUNCATED)\n",
        "\n",
        "# INSTRUMENT\n",
        "INSTRUMENT = ' '.join(instrument)\n",
        "pos = h.pos(INSTRUMENT)\n",
        "imp = []\n",
        "for tup in pos:\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "INST_TRUNCATED = ' '.join(imp)\n",
        "print(INST_TRUNCATED)\n",
        "\n",
        "final_string = INST_TRUNCATED+ ' ➡️' + PURPOSE_TRUNCATED \n",
        "print(\"😀 final string to be inserted \", final_string)\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/utils.py:279: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  pred_logits = sm(masked_logit).view(1,-1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "😀 Words vector from sentence  ['대본을', '피피티로', '옮기기', '전에', '요약하고', '분석하기', '위해', '자연어처리의', '최신', '기술을', '다수', '사용하였습니다.']\n",
            "Number of parsed candidates  9\n",
            "[2, 11, 4, 0, 6, 1, 1, 1, 11] [11, 11, 6]\n",
            "([['대본을', '피피티로', '옮기기', '전에', '요약하고', '분석하기', '위해', '자연어처리의', '최신', '기술을', '다수', '사용하였습니다.'], ['_', '_', '_', '전.n', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', 'Time_vector', '_', '_', '_', '_', '_', '_', '_', '_'], ['B-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'O', 'B-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event']], [['대본을', '피피티로', '옮기기', '전에', '요약하고', '분석하기', '위해', '자연어처리의', '최신', '기술을', '다수', '사용하였습니다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '최신.n', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', 'Relative_time', '_', '_', '_'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Focal_participant', 'O', 'O']])\n",
            "tagged vector of candidates:  ['B-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'O', 'B-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event']\n",
            "['Landmark_event', 'Landmark_event', 'Landmark_event', 'O', 'Event', 'Event', 'Event', 'Event', 'Event', '_', '_', '_'] Tagged words for the roles : ['B-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'O', 'B-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event']\n",
            "tagged vector of candidates:  ['B-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'B-Instrument', 'I-Instrument', 'I-Instrument', 'B-Manner', 'O']\n",
            "['Purpose', 'Purpose', 'Purpose', 'Purpose', 'Purpose', 'Purpose', 'Purpose', 'Instrument', 'Instrument', '_', '_', 'Using'] Tagged words for the roles : ['B-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'B-Instrument', 'I-Instrument', 'I-Instrument', 'B-Manner', 'O']\n",
            "Instrument:  ['자연어처리의', '최신', '사용하였습니다.']\n",
            "Purpose:  ['대본을', '피피티로', '옮기기', '전에', '요약하고', '분석하기', '위해']\n",
            "대본 피피티 전 요약 분석\n",
            "자연어처리 최신 사용\n",
            "😀 final string to be inserted  자연어처리 최신 사용 ➡️대본 피피티 전 요약 분석\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HxZxxnMiVuKw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "945f2131-26cd-44d3-cd1c-c00ef0bb5f39"
      },
      "source": [
        "# 3rd sentence : Goal - Means\n",
        "parsed = parser.parser(ls[2], sent_id='1', result_format='conll')\n",
        "words = parsed[0][0]\n",
        "print(\"Words vector from sentence \",words)\n",
        "q = [-1]*len(parsed)\n",
        "for i in range(len(parsed)):\n",
        "  count=0\n",
        "  for element in parsed[i][3]:\n",
        "    if element.startswith(\"O\"):\n",
        "      count+=1\n",
        "  q[i] = len(words) - count\n",
        "\n",
        "print(q, heapq.nlargest(3, q))\n",
        "\n",
        "a,b,c=parsed\n",
        "tagged = b[3]\n",
        "for i in range(len(tagged)):\n",
        "  try:\n",
        "    roles[i] = tagged[i].split(\"-\")[1]\n",
        "  except:\n",
        "    roles[i] = tagged[i].split(\"-\")[0]\n",
        "\n",
        "\n",
        "\n",
        "goals = []\n",
        "means = []\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Goal\":\n",
        "    goals.append(words[_])\n",
        "  if roles[_]=='Means':\n",
        "    means.append(words[_])\n",
        "\n",
        "# MEANS\n",
        "MEANS = ' '.join(means)\n",
        "pos = h.pos(MEANS)\n",
        "imp = []\n",
        "for tup in h.pos(MEANS):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "MEANS_TRUNCATED = ' '.join(imp)\n",
        "print(MEANS_TRUNCATED)\n",
        "\n",
        "# GOALS\n",
        "GOAL = ' '.join(goals)\n",
        "pos = h.pos(GOAL)\n",
        "imp = []\n",
        "for tup in h.pos(GOAL):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "GOAL_TRUNCATED = ' '.join(imp)\n",
        "print(GOAL_TRUNCATED)\n",
        "\n",
        "final_string = MEANS_TRUNCATED + ' -- >' + GOAL_TRUNCATED\n",
        "print(\"😀 final string to be inserted\",final_string)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/utils.py:279: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  pred_logits = sm(masked_logit).view(1,-1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Words vector from sentence  ['또한', '파이썬에서', '파워포인트의', '소스에', '접근하기', '위해', 'xml', '코드를', '심층적으로', '분석했습니다.']\n",
            "[2, 8, 3] [8, 3, 2]\n",
            "askdf\n",
            "askdf\n",
            "askdf\n",
            "askdf\n",
            "   \n",
            "코드 심층적 분석\n",
            "파이썬 파워포인트 소스 접근하기\n",
            "😀 final string to be inserted 코드 심층적 분석-->파이썬 파워포인트 소스 접근하기\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_vBkRqWXnfr",
        "colab_type": "text"
      },
      "source": [
        "## PPTX에 활용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ_738Krj4Wm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prs = Presentation(\"final_template.pptx\")  # 원하는 template 종류 불러오기\n",
        "slide_0 = prs.slides[0] # 제복 slide (제목 슬라이드)\n",
        "slide_2 = prs.slides[2] # 두 번째 slide"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ_iciyKj4Wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slide_2.placeholders.element[3][2][2][1][1].text = \"Two Track Process\" # 여덟 번째 슬라이드 제목\n",
        "slide_2.placeholders.element[5][2][2][1][1].text = \"\"\n",
        "slide_2.placeholders.element[6][2][2][1][1].text = \"\"\n",
        "slide_2.placeholders.element[17][2][2][1][1].text = \"\"\n",
        "slide_2.placeholders.element[18][2][2][1][1].text = \"\"\n",
        "slide_2.placeholders.element[19][2][2][1][1].text = \"TEXT2PPTX\"\n",
        "slide_2.placeholders.element[11][2][2][1][1].text = \"자연어처리의 최신 기술을 다수 사용\"\n",
        "slide_2.placeholders.element[10][2][2][1][1].text = \"대본을 피피티로 옮기기 전에 요약하고 분석\"\n",
        "slide_2.placeholders.element[14][2][2][1][1].text = \"파이썬에서 파워포인트의 소스에 접근하기 위해 xml 코드를 심층적으로 분석\"\n",
        "slide_2.placeholders.element[13][2][2][1][1].text = \"사용자가 자연어로 쓰인 대본을 텍2피에 제공하면 높은 수준의 피피티로 제공\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEgNzSF4Xq8a",
        "colab_type": "text"
      },
      "source": [
        "## 교육용 교재에 활용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRL1bZb4XusT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}