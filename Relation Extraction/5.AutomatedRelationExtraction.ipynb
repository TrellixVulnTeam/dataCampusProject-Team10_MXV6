{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutomatedRelationExtraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoonkim313/dataCampusProject-Team10/blob/master/Relation%20Extraction/AutomatedRelationExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "obGBWFLnIypX",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "08999495-273c-429d-ae47-4660184937b4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install transformers\n",
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk \n",
        "!pip3 install konlpy\n",
        "!pip install kss\n",
        "%cd /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
        "!pip install python-pptx\n",
        "from frameBERT import frame_parser\n",
        "path=\"/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 3.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 23.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 29.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=00e03b42e3ddae0e8fcd988ac4b4bac61009becae1e0ef3c448d01a01aeb1283\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,037 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [882 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Ign:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [255 kB]\n",
            "Get:17 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,855 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [116 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,334 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,413 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [27.1 kB]\n",
            "Get:22 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [895 kB]\n",
            "Fetched 8,088 kB in 3s (2,322 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "g++ is already the newest version (4:7.4.0-1ubuntu2.3).\n",
            "g++ set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxxf86dga1 openjdk-8-jdk-headless openjdk-8-jre\n",
            "  openjdk-8-jre-headless x11-utils\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source visualvm icedtea-8-plugin libnss-mdns\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxxf86dga1 openjdk-8-jdk openjdk-8-jdk-headless\n",
            "  openjdk-8-jre openjdk-8-jre-headless x11-utils\n",
            "0 upgraded, 10 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 40.7 MB of archives.\n",
            "After this operation, 153 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-core all 2.37-1 [1,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-extra all 2.37-1 [1,953 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java all 0.33.3-20ubuntu0.1 [34.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java-jni amd64 0.33.3-20ubuntu0.1 [28.3 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre-headless amd64 8u265-b01-0ubuntu2~18.04 [27.5 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre amd64 8u265-b01-0ubuntu2~18.04 [69.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk-headless amd64 8u265-b01-0ubuntu2~18.04 [8,262 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk amd64 8u265-b01-0ubuntu2~18.04 [1,610 kB]\n",
            "Fetched 40.7 MB in 2s (18.0 MB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../1-fonts-dejavu-core_2.37-1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../2-fonts-dejavu-extra_2.37-1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../3-x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../4-libatk-wrapper-java_0.33.3-20ubuntu0.1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../5-libatk-wrapper-java-jni_0.33.3-20ubuntu0.1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../6-openjdk-8-jre-headless_8u265-b01-0ubuntu2~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jre:amd64.\n",
            "Preparing to unpack .../7-openjdk-8-jre_8u265-b01-0ubuntu2~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../8-openjdk-8-jdk-headless_8u265-b01-0ubuntu2~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk:amd64.\n",
            "Preparing to unpack .../9-openjdk-8-jdk_8u265-b01-0ubuntu2~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "Setting up fonts-dejavu-core (2.37-1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Setting up libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\n",
            "Setting up openjdk-8-jre:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/policytool to provide /usr/bin/policytool (policytool) in auto mode\n",
            "Setting up openjdk-8-jdk:amd64 (8u265-b01-0ubuntu2~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/appletviewer to provide /usr/bin/appletviewer (appletviewer) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.0MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/49/725710351d78d26c65337b1e3b322d7b27b34b704535ab56afc0d9ab0ffd/JPype1-1.0.1-cp36-cp36m-manylinux2010_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 34.3MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.2)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: beautifulsoup4, JPype1, tweepy, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.0.1 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.9.0\n",
            "Collecting kss\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/bb/4772901b3b934ac204f32a0bd6fc0567871d8378f9bbc7dd5fd5e16c6ee7/kss-1.3.1.tar.gz\n",
            "Building wheels for collected packages: kss\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-1.3.1-cp36-cp36m-linux_x86_64.whl size=251614 sha256=4e0ff3acdab8d3ae74c38bb40b17d8328c966dad0bc031d38fc8596f4d4f0239\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/98/d1/53f75f89925cd95779824778725ee3fa36e7aa55ed26ad54a8\n",
            "Successfully built kss\n",
            "Installing collected packages: kss\n",
            "Successfully installed kss-1.3.1\n",
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "Collecting python-pptx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/86/eb979f7b0333ec769041aae36df8b9f1bd8bea5bbad44620663890dce561/python-pptx-0.6.18.tar.gz (8.9MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9MB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from python-pptx) (4.2.6)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from python-pptx) (7.0.0)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/b9/0f095f47d1d8fdf830cab77907467beb784bc13c583e27844a22aa923934/XlsxWriter-1.3.2-py2.py3-none-any.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 36.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: python-pptx\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.18-cp36-none-any.whl size=275706 sha256=5f8c77c7e3256f0d663e317df025a58a729230e8fa2d61cd7f8529e36869bc98\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/1f/2c/29acca422b420a0b5210bd2cd7e9669804520d602d2462f20b\n",
            "Successfully built python-pptx\n",
            "Installing collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-1.3.2 python-pptx-0.6.18\n",
            "\n",
            "###DEVICE: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ydc905pQlFy1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6d2ccbcf-a63c-4fe9-c749-d3ca1d07c164"
      },
      "source": [
        "!pip install python-pptx"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-pptx in /usr/local/lib/python3.6/dist-packages (0.6.18)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from python-pptx) (7.0.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from python-pptx) (4.2.6)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.6/dist-packages (from python-pptx) (1.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MbmSXGrwJ585",
        "colab": {}
      },
      "source": [
        "import kss\n",
        "from konlpy.tag import Hannanum\n",
        "h = Hannanum()\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import heapq\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "from operator import itemgetter \n",
        "from pptx import Presentation\n",
        "from pptx.util import Inches\n",
        "from pptx.enum.shapes import MSO_SHAPE\n",
        "from collections import deque\n",
        "from ast import literal_eval\n",
        "from collections import defaultdict"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_v5o0el7I9ur",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0098c710-c8fe-42fa-c79c-01c8600795a3"
      },
      "source": [
        "text ='''\n",
        "신체의 세포, 조직, 장기가 손상되어 더 이상 제 기능을 하지못할 때에 이를 대체하기 위해 이식을 실시한다. \n",
        "이때 이식으로 옮겨 붙이는 세포, 조직, 장기를 이식편이라 한다. 자신이나 일란성 쌍둥이의 이식편을 이용할 \n",
        "수 없다면 다른 사람의 이식편으로 ‘동종 이식’을 실시한다. \n",
        "'''\n",
        "text = re.sub(\"\\n\",\" \",text)\n",
        "pprint(text)\n",
        "t = '''\n",
        "그런데 우리의 몸은 자신의 것이 아닌 물질이 \n",
        "체내로 유입될 경우 면역 반응을 일으키므로, 유전적\n",
        "으로 동일하지 않은 이식편에 대해 항상 거부 반응을 일으킨다.\n",
        "면역적 거부 반응은 면역 세포가 표면에 발현하는 주조직적합\n",
        "복합체(MHC) 분자의 차이에 의해 유발된다. 개체마다 MHC에\n",
        "차이가 있는데 서로 간의 유전적 거리가 멀수록 MHC에 차이가\n",
        "커져 거부 반응이 강해진다. 이를 막기 위해 면역 억제제를 사용\n",
        "하는데, 이는 면역 반응을 억제하여 질병 감염의 위험성을 높인다.\n",
        "이식에는 많은 비용이 소요될 뿐만 아니라 이식이 가능한 동종\n",
        "이식편의 수가 매우 부족하기 때문에 이를 대체하는 방법이 개발\n",
        "되고 있다. 우선 인공 심장과 같은 ‘전자 기기 인공 장기’를 이용\n",
        "하는 방법이 있다. 하지만 이는 장기의 기능을 일시적으로 대체\n",
        "하는 데 사용되며, 추가 전력 공급 및 정기적 부품 교체 등이\n",
        "요구되는 단점이 있고, 아직 인간의 장기를 완전히 대체할 만큼\n",
        "정교한 단계에 이르지는 못했다.\n",
        "'''"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(' 신체의 세포, 조직, 장기가 손상되어 더 이상 제 기능을 하지못할 때에 이를 대체하기 위해 이식을 실시한다.  이때 이식으로 옮겨 '\n",
            " '붙이는 세포, 조직, 장기를 이식편이라 한다. 자신이나 일란성 쌍둥이의 이식편을 이용할  수 없다면 다른 사람의 이식편으로 ‘동종 '\n",
            " '이식’을 실시한다.  ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Af1flDKI9M8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "3de3afba-e435-4954-f1b0-a3a30615ceb5"
      },
      "source": [
        "parser = frame_parser.FrameParser(model_path=path, language='ko')"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "srl model: framenet\n",
            "language: ko\n",
            "version: 1.2\n",
            "using viterbi: False\n",
            "using masking: True\n",
            "pretrained BERT: bert-base-multilingual-cased\n",
            "using TGT special token: True\n",
            "used dictionary:\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lu2idx.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lufrmap.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n",
            "...loaded model path: /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "...model is loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuHEQofWAsnA",
        "colab_type": "text"
      },
      "source": [
        "### Linked List Implemented to Show the whole list of sentences and their parsed tags in a consecutive manner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wIph64I5OnLZ",
        "colab": {}
      },
      "source": [
        "def split_into_sentences(object):\n",
        "  ls = kss.split_sentences(object)\n",
        "  return ls\n",
        "\n",
        "def frameParse(text):\n",
        "  parser = frame_parser.FrameParser(model_path=path, language='ko')\n",
        "  parsed = parser.parser(text, sent_id='1', result_format='conll')\n",
        "  return parsed\n",
        "  \n",
        "\n",
        "def sortCandidate(parsed, num_candidates):\n",
        "  for i in range(len(parsed)):\n",
        "    count=0\n",
        "    for element in parsed[i][3]:\n",
        "      if element.startswith(\"O\"):\n",
        "        count+=1\n",
        "  q[i] = len(words) - count\n",
        "  hq = heapq.nlargest(num_candidates, q)\n",
        "  indices.append(q.index(hq[idx]))\n",
        "  # for idx in range(num_candidates):\n",
        "  #   temp = parsed[q.index(hq[idx])]\n",
        "  #   words = temp[0]\n",
        "  #   [[row[i] for row in temp] for i in range(len(words))]\n",
        "  return indices\n",
        "\n",
        "\n",
        "def findConsecutiveBIO(words, tag):\n",
        "  began = False\n",
        "  count = 0\n",
        "  q = deque(words)\n",
        "  for i in range(len(words)):\n",
        "    if tag[i]=='O':\n",
        "      q.popleft()\n",
        "    if tag[i].startswith('B'):\n",
        "      began=True\n",
        "    if began & tag[i].startswith('I'):\n",
        "      count+=1\n",
        "  return q\n",
        "\n",
        "# Linked List 구현해서 parsed 후보군에 사용\n",
        "class Node:\n",
        "    def __init__(self, data):\n",
        "        self.words = data[0]\n",
        "        self.tags = data[3]\n",
        "        self.next = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str((self.words, self.tags))\n",
        "\n",
        "class LinkedList:\n",
        "  def __init__(self):\n",
        "      self.head = None\n",
        "\n",
        "  def __repr__(self):\n",
        "      node = self.head\n",
        "      nodes = []\n",
        "      while node is not None:\n",
        "          nodes.append(str(node.tags))\n",
        "          node = node.next\n",
        "      nodes.append(\"None\")\n",
        "      return ' -> '.join(nodes)\n",
        "  \n",
        "class LinkedList:\n",
        "  def __init__(self):\n",
        "      self.head = None\n",
        "\n",
        "  def __repr__(self):\n",
        "      node = self.head\n",
        "      nodes = []\n",
        "      while node is not None:\n",
        "          nodes.append(str(node.tags))\n",
        "          node = node.next\n",
        "      nodes.append(\"None\")\n",
        "      return ' -> '.join(nodes)\n",
        "\n",
        "def extractFrame(text):\n",
        "  ls = split_into_sentences(text)\n",
        "  final = {}\n",
        "  \n",
        "  for idx in range(len(ls)):\n",
        "    parsed = frameParse(ls[idx]) # candidates 생성\n",
        "    final.setdefault(idx,str)\n",
        "    parsedList =  LinkedList()\n",
        "    for j in range(len(parsed)):\n",
        "      parsed_candidate = parsed[j]\n",
        "      new_node = Node(parsed_candidate)\n",
        "      if j == 0:\n",
        "        old_node = new_node\n",
        "        parsedList.head = old_node\n",
        "      elif j==len(parsed)-1:\n",
        "        old_node.next = new_node\n",
        "        new_node.next = None\n",
        "        print(idx,'  ',parsedList)\n",
        "        final[idx] = parsedList\n",
        "      else:\n",
        "        old_node.next = new_node\n",
        "        old_node = new_node\n",
        "  return final\n",
        "\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD2QZjRqZaM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(text):\n",
        "  all = extractFrame(text)\n",
        "  s = \"\"\n",
        "  res = {}\n",
        "\n",
        "  i = 1\n",
        "  res.setdefault(i,tuple)\n",
        "  for k, v in all.items():\n",
        "    a = v.head\n",
        "    while a is not None:\n",
        "      temp = \" \"\n",
        "      if len(a.words) == len(a.tags):\n",
        "        q = findConsecutiveBIO(a.words, a.tags)\n",
        "      else:\n",
        "        print(\"ERROR OCCURED\")\n",
        "        \n",
        "      count = len(a.words)\n",
        "      for tag in a.tags:\n",
        "       if tag.startswith(\"O\"):\n",
        "          count -= 1\n",
        "      if count > len(a.words)*0.5:\n",
        "        print(\"The threshold is reached !! 😛 😝 😜 🤪 🤨 🧐 🤓 😎 🤩 🥳 😏\")\n",
        "        temp = \" \".join(q) \n",
        "        print(temp)\n",
        "        words = a.words\n",
        "        tags = a.tags\n",
        "      res[i] = (a.words, a.tags , \" \".join(q))\n",
        "      a = a.next\n",
        "    i+=1\n",
        "  return res\n"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIIiD0vrpdmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94d6d5dd-1a4c-43b5-d646-960fb1ef00f3"
      },
      "source": [
        "extracted = main(text)\n",
        "pprint(extracted)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "srl model: framenet\n",
            "language: ko\n",
            "version: 1.2\n",
            "using viterbi: False\n",
            "using masking: True\n",
            "pretrained BERT: bert-base-multilingual-cased\n",
            "using TGT special token: True\n",
            "used dictionary:\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lu2idx.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lufrmap.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n",
            "...loaded model path: /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "...model is loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/utils.py:279: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  pred_logits = sm(masked_logit).view(1,-1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0    ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'O', 'O', 'O', 'I-Class', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Time', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'I-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'O', 'O', 'I-Trajector_event', 'I-Trajector_event', 'O', 'I-Trajector_event'] -> ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'I-Time', 'I-Time', 'I-Time', 'I-Time', 'I-Time', 'I-Time', 'I-Time', 'I-Time', 'B-Old', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'I-Means', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Time', 'O', 'I-Goal', 'I-Goal', 'O', 'O', 'O'] -> None\n",
            "srl model: framenet\n",
            "language: ko\n",
            "version: 1.2\n",
            "using viterbi: False\n",
            "using masking: True\n",
            "pretrained BERT: bert-base-multilingual-cased\n",
            "using TGT special token: True\n",
            "used dictionary:\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lu2idx.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lufrmap.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n",
            "...loaded model path: /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "...model is loaded\n",
            "1    ['B-Time', 'B-Theme', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['B-Time', 'B-Goal', 'I-Goal', 'O', 'B-Items', 'O', 'O', 'O', 'O'] -> None\n",
            "srl model: framenet\n",
            "language: ko\n",
            "version: 1.2\n",
            "using viterbi: False\n",
            "using masking: True\n",
            "pretrained BERT: bert-base-multilingual-cased\n",
            "using TGT special token: True\n",
            "used dictionary:\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lu2idx.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/kfn1.2_lufrmap.json\n",
            "\t /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n",
            "...loaded model path: /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
            "...model is loaded\n",
            "2    ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['B-Agent', 'I-Instrument', 'I-Instrument', 'I-Instrument', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['B-Entity', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['B-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] -> None\n",
            "The threshold is reached !! 😛 😝 😜 🤪 🤨 🧐 🤓 😎 🤩 🥳 😏\n",
            "이상 제 기능을 하지못할 때에 이를 대체하기 위해 이식을 실시한다.\n",
            "The threshold is reached !! 😛 😝 😜 🤪 🤨 🧐 🤓 😎 🤩 🥳 😏\n",
            "제 기능을 하지못할 때에 이를 대체하기 위해 이식을 실시한다.\n",
            "ERROR OCCURED\n",
            "{1: (['신체의',\n",
            "      '세포,',\n",
            "      '조직,',\n",
            "      '장기가',\n",
            "      '손상되어',\n",
            "      '더',\n",
            "      '이상',\n",
            "      '제',\n",
            "      '기능을',\n",
            "      '하지못할',\n",
            "      '때에',\n",
            "      '이를',\n",
            "      '대체하기',\n",
            "      '위해',\n",
            "      '이식을',\n",
            "      '실시한다.'],\n",
            "     ['O',\n",
            "      'O',\n",
            "      'I-Means',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'I-Time',\n",
            "      'O',\n",
            "      'I-Goal',\n",
            "      'I-Goal',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O'],\n",
            "     '대체하기 위해 이식을 실시한다.'),\n",
            " 2: (['이때', '이식으로', '옮겨', '붙이는', '세포,', '조직,', '장기를', '이식편이라', '한다.'],\n",
            "     ['B-Time', 'B-Goal', 'I-Goal', 'O', 'B-Items', 'O', 'O', 'O', 'O'],\n",
            "     '조직, 장기를 이식편이라 한다.'),\n",
            " 3: (['자신이나',\n",
            "      '일란성',\n",
            "      '쌍둥이의',\n",
            "      '이식편을',\n",
            "      '이용할',\n",
            "      '',\n",
            "      '수',\n",
            "      '없다면',\n",
            "      '다른',\n",
            "      '사람의',\n",
            "      '이식편으로',\n",
            "      '‘동종',\n",
            "      '이식’을',\n",
            "      '실시한다.'],\n",
            "     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
            "     '')}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvm4aMtEz1A1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "6f28df78-5b42-4d60-a3e2-d78b92ad9c89"
      },
      "source": [
        "pprint(extracted)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: (['신체의',\n",
            "      '세포,',\n",
            "      '조직,',\n",
            "      '장기가',\n",
            "      '손상되어',\n",
            "      '더',\n",
            "      '이상',\n",
            "      '제',\n",
            "      '기능을',\n",
            "      '하지못할',\n",
            "      '때에',\n",
            "      '이를',\n",
            "      '대체하기',\n",
            "      '위해',\n",
            "      '이식을',\n",
            "      '실시한다.'],\n",
            "     ['O',\n",
            "      'O',\n",
            "      'I-Means',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O',\n",
            "      'I-Time',\n",
            "      'O',\n",
            "      'I-Goal',\n",
            "      'I-Goal',\n",
            "      'O',\n",
            "      'O',\n",
            "      'O'],\n",
            "     ' '),\n",
            " 2: (['이때', '이식으로', '옮겨', '붙이는', '세포,', '조직,', '장기를', '이식편이라', '한다.'],\n",
            "     ['B-Time', 'B-Goal', 'I-Goal', 'O', 'B-Items', 'O', 'O', 'O', 'O'],\n",
            "     ' '),\n",
            " 3: (['자신이나',\n",
            "      '일란성',\n",
            "      '쌍둥이의',\n",
            "      '이식편을',\n",
            "      '이용할',\n",
            "      '',\n",
            "      '수',\n",
            "      '없다면',\n",
            "      '다른',\n",
            "      '사람의',\n",
            "      '이식편으로',\n",
            "      '‘동종',\n",
            "      '이식’을',\n",
            "      '실시한다.'],\n",
            "     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
            "     ' ')}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQNkF3aGty_0",
        "colab_type": "text"
      },
      "source": [
        "### TODO : 여기서부터 extracted를 가지고 purpose / goal / effect / entity 등을 찾아야함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf62cktfw2C1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "f1fddff6-8b96-4d85-83fe-5dbbf2a9ebca"
      },
      "source": [
        "for k, tup in extracted.items():\n",
        "  a, b, c = tup\n",
        "  roles = [0]*len(b)\n",
        "  words = [0]*len(b)\n",
        "  for i in range(len(b)):\n",
        "    try: \n",
        "      roles[i] = b[i].split(\"-\")[1] \n",
        "      \n",
        "    except: \n",
        "      roles[i] = b[i].split(\"-\")[0]\n",
        "  print(roles)\n",
        "  for _ in roles:\n",
        "    pass"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Goal', 'Goal', 'O', 'O', 'Means', 'Means', 'Means']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-bf643a255534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mroles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'None'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ast.py\u001b[0m in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mleft\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'malformed node or string: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ast.py\u001b[0m in \u001b[0;36m_convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mleft\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'malformed node or string: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: malformed node or string: <_ast.Name object at 0x7f5ab7354da0>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S7zGTQO4BUO7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "326d4f22-e8b0-4f94-f2dd-23251248d598"
      },
      "source": [
        "# 1st sentence : GOALS & MEANS \n",
        "\n",
        "parsed = parser.parser(ls[0], sent_id='1', result_format='conll')\n",
        "words = parsed[0][0]\n",
        "print(\"😀 Words vector from sentence \",words)\n",
        "q = [-1]*len(parsed)\n",
        "for i in range(len(parsed)):\n",
        "  count=0\n",
        "  for element in parsed[i][3]:\n",
        "    if element.startswith(\"O\"):\n",
        "      count+=1\n",
        "  q[i] = len(words) - count\n",
        "hq = heapq.nlargest(2, q)\n",
        "\n",
        "\n",
        "print(\"Number of parsed candidates \",len(parsed))\n",
        "print(q, heapq.nlargest(2, q)) # 첫번째 경우 사용\n",
        "\n",
        "\n",
        "words = parsed[0][0]\n",
        "roles = parsed[0][2]\n",
        "tagged = parsed[0][3]\n",
        "for idx in range(len(tagged)):\n",
        "  try:\n",
        "    roles[i] = tagged[i].split(\"-\")[1]\n",
        "  except:\n",
        "    roles[i] = tagged[i].split(\"-\")[0]\n",
        "\n",
        "print(\"roles \",roles)\n",
        "\n",
        "goals = []\n",
        "means = []\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Goal\":\n",
        "    goals.append(words[_])\n",
        "  if roles[_]=='Means':\n",
        "    means.append(words[_])\n",
        "\n",
        "# MEANS\n",
        "MEANS = ' '.join(means)\n",
        "pos = h.pos(MEANS)\n",
        "imp = []\n",
        "for tup in h.pos(MEANS):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "MEANS_TRUNCATED = ' '.join(imp)\n",
        "print(MEANS_TRUNCATED)\n",
        "\n",
        "# GOALS\n",
        "GOAL = ' '.join(goals)\n",
        "pos = h.pos(GOAL)\n",
        "imp = []\n",
        "for tup in h.pos(GOAL):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "GOAL_TRUNCATED = ' '.join(imp)\n",
        "print(GOAL_TRUNCATED)\n",
        "\n",
        "final_string = MEANS_TRUNCATED + ' -- >' + GOAL_TRUNCATED\n",
        "print(\"😀 final string to be inserted\",final_string)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['TEXT2PPTX의', '구현하기', '위해', 'two', 'track', 'process를', '거쳤습니다.'], ['_', '_', '위하다.v', '_', '_', '_', '_'], ['_', '_', 'Purpose', '_', '_', '_', '_'], ['B-Goal', 'I-Goal', 'O', 'O', 'I-Means', 'I-Means', 'I-Means']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lRYgqOwnM5X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12fd4e08-6821-4b06-9e21-c00404bdbf07"
      },
      "source": [
        ""
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['O', 'O', 'O', 'B-Event', 'I-Event', 'I-Event', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F9zsJryuBRri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "dc2971b6-b2a7-444a-bcbe-9f3f7bef79f0"
      },
      "source": [
        "# 2nd sentence : PURPOSE & INSTRUMENT\n",
        "parsed = parser.parser(ls[1], sent_id='1', result_format='conll')\n",
        "words = parsed[0][0]\n",
        "print(\"😀 Words vector from sentence \",words)\n",
        "q = [-1]*len(parsed)\n",
        "for i in range(len(parsed)):\n",
        "  count=0\n",
        "  for element in parsed[i][3]:\n",
        "    if element.startswith(\"O\"):\n",
        "      count+=1\n",
        "  q[i] = len(words) - count\n",
        "\n",
        "\n",
        "print(\"Number of parsed candidates \",len(parsed))\n",
        "print(q, heapq.nlargest(3, q))\n",
        "print(itemgetter(*[1,5])(parsed))\n",
        "candidates = itemgetter(*[1,8])(parsed)\n",
        "for _ in candidates:\n",
        "  words = _[0]\n",
        "  tagged = _[3]\n",
        "  print(\"tagged vector of candidates: \",tagged)\n",
        "  roles = _[2]\n",
        "  for i in range(len(parsed)):\n",
        "    try:\n",
        "      roles[i] = tagged[i].split(\"-\")[1]\n",
        "    except:\n",
        "      roles[i] = tagged[i].split(\"-\")[0]\n",
        "  print(roles,f'Tagged words for the roles : {tagged}')\n",
        "# roles are selected as the one with purpose and instrument\n",
        "\n",
        "purpose = []\n",
        "instrument = []\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Purpose\":\n",
        "    purpose.append(words[_])\n",
        "    \n",
        "  elif roles[_]==\"Instrument\":\n",
        "    instrument.append(words[_])\n",
        "    \n",
        "  elif roles[_]!=\"_\": # using 포함\n",
        "    instrument.append(words[_])\n",
        "print(\"Instrument: \",instrument)\n",
        "print(\"Purpose: \",purpose)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# PURPOSE\n",
        "PURPOSE = ' '.join(purpose)\n",
        "pos = h.pos(PURPOSE)\n",
        "imp = []\n",
        "for tup in pos:\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "PURPOSE_TRUNCATED = ' '.join(imp)\n",
        "print(PURPOSE_TRUNCATED)\n",
        "\n",
        "# INSTRUMENT\n",
        "INSTRUMENT = ' '.join(instrument)\n",
        "pos = h.pos(INSTRUMENT)\n",
        "imp = []\n",
        "for tup in pos:\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "INST_TRUNCATED = ' '.join(imp)\n",
        "print(INST_TRUNCATED)\n",
        "\n",
        "final_string = INST_TRUNCATED+ ' ➡️' + PURPOSE_TRUNCATED \n",
        "print(\"😀 final string to be inserted \", final_string)\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/utils.py:279: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  pred_logits = sm(masked_logit).view(1,-1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "😀 Words vector from sentence  ['대본을', '피피티로', '옮기기', '전에', '요약하고', '분석하기', '위해', '자연어처리의', '최신', '기술을', '다수', '사용하였습니다.']\n",
            "Number of parsed candidates  9\n",
            "[2, 11, 4, 0, 6, 1, 1, 1, 11] [11, 11, 6]\n",
            "([['대본을', '피피티로', '옮기기', '전에', '요약하고', '분석하기', '위해', '자연어처리의', '최신', '기술을', '다수', '사용하였습니다.'], ['_', '_', '_', '전.n', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', 'Time_vector', '_', '_', '_', '_', '_', '_', '_', '_'], ['B-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'O', 'B-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event']], [['대본을', '피피티로', '옮기기', '전에', '요약하고', '분석하기', '위해', '자연어처리의', '최신', '기술을', '다수', '사용하였습니다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '최신.n', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', 'Relative_time', '_', '_', '_'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Focal_participant', 'O', 'O']])\n",
            "tagged vector of candidates:  ['B-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'O', 'B-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event']\n",
            "['Landmark_event', 'Landmark_event', 'Landmark_event', 'O', 'Event', 'Event', 'Event', 'Event', 'Event', '_', '_', '_'] Tagged words for the roles : ['B-Landmark_event', 'I-Landmark_event', 'I-Landmark_event', 'O', 'B-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event', 'I-Event']\n",
            "tagged vector of candidates:  ['B-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'B-Instrument', 'I-Instrument', 'I-Instrument', 'B-Manner', 'O']\n",
            "['Purpose', 'Purpose', 'Purpose', 'Purpose', 'Purpose', 'Purpose', 'Purpose', 'Instrument', 'Instrument', '_', '_', 'Using'] Tagged words for the roles : ['B-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'I-Purpose', 'B-Instrument', 'I-Instrument', 'I-Instrument', 'B-Manner', 'O']\n",
            "Instrument:  ['자연어처리의', '최신', '사용하였습니다.']\n",
            "Purpose:  ['대본을', '피피티로', '옮기기', '전에', '요약하고', '분석하기', '위해']\n",
            "대본 피피티 전 요약 분석\n",
            "자연어처리 최신 사용\n",
            "😀 final string to be inserted  자연어처리 최신 사용 ➡️대본 피피티 전 요약 분석\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HxZxxnMiVuKw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "945f2131-26cd-44d3-cd1c-c00ef0bb5f39"
      },
      "source": [
        "# 3rd sentence : Goal - Means\n",
        "parsed = parser.parser(ls[2], sent_id='1', result_format='conll')\n",
        "words = parsed[0][0]\n",
        "print(\"Words vector from sentence \",words)\n",
        "q = [-1]*len(parsed)\n",
        "for i in range(len(parsed)):\n",
        "  count=0\n",
        "  for element in parsed[i][3]:\n",
        "    if element.startswith(\"O\"):\n",
        "      count+=1\n",
        "  q[i] = len(words) - count\n",
        "\n",
        "print(q, heapq.nlargest(3, q))\n",
        "\n",
        "a,b,c=parsed\n",
        "tagged = b[3]\n",
        "for i in range(len(tagged)):\n",
        "  try:\n",
        "    roles[i] = tagged[i].split(\"-\")[1]\n",
        "  except:\n",
        "    roles[i] = tagged[i].split(\"-\")[0]\n",
        "\n",
        "\n",
        "\n",
        "goals = []\n",
        "means = []\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Goal\":\n",
        "    goals.append(words[_])\n",
        "  if roles[_]=='Means':\n",
        "    means.append(words[_])\n",
        "\n",
        "# MEANS\n",
        "MEANS = ' '.join(means)\n",
        "pos = h.pos(MEANS)\n",
        "imp = []\n",
        "for tup in h.pos(MEANS):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "MEANS_TRUNCATED = ' '.join(imp)\n",
        "print(MEANS_TRUNCATED)\n",
        "\n",
        "# GOALS\n",
        "GOAL = ' '.join(goals)\n",
        "pos = h.pos(GOAL)\n",
        "imp = []\n",
        "for tup in h.pos(GOAL):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "GOAL_TRUNCATED = ' '.join(imp)\n",
        "print(GOAL_TRUNCATED)\n",
        "\n",
        "final_string = MEANS_TRUNCATED + ' -- >' + GOAL_TRUNCATED\n",
        "print(\"😀 final string to be inserted\",final_string)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction/frameBERT/src/utils.py:279: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  pred_logits = sm(masked_logit).view(1,-1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Words vector from sentence  ['또한', '파이썬에서', '파워포인트의', '소스에', '접근하기', '위해', 'xml', '코드를', '심층적으로', '분석했습니다.']\n",
            "[2, 8, 3] [8, 3, 2]\n",
            "askdf\n",
            "askdf\n",
            "askdf\n",
            "askdf\n",
            "   \n",
            "코드 심층적 분석\n",
            "파이썬 파워포인트 소스 접근하기\n",
            "😀 final string to be inserted 코드 심층적 분석-->파이썬 파워포인트 소스 접근하기\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VQxErkIzOj0W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "8cbbe6fe-11dd-4e8b-a633-1aff0b26911a"
      },
      "source": [
        "# # Fourth Sentence\n",
        "\n",
        "# parsed = parser.parser(ls[3], sent_id='1', result_format='conll')\n",
        "# words = parsed[0][0]\n",
        "# print(\"😀 Words vector from sentence \",words)\n",
        "# q = [-1]*len(parsed)\n",
        "# for i in range(len(parsed)):\n",
        "#   count=0\n",
        "#   for element in parsed[i][3]:\n",
        "#     if element.startswith(\"O\"):\n",
        "#       count+=1\n",
        "#   q[i] = len(words) - count\n",
        "\n",
        "\n",
        "# print(\"Number of parsed candidates \",len(parsed))\n",
        "# print(q, heapq.nlargest(3, q))\n",
        "\n",
        "\n",
        "candidates = itemgetter(*[0,7])(parsed)\n",
        "for _ in candidates:\n",
        "  words = _[0]\n",
        "  tagged = _[3]\n",
        "  print(\"tagged vector of candidates: \",tagged)\n",
        "  roles = _[2]\n",
        "  for i in range(len(tagged)):\n",
        "    try:\n",
        "      roles[i] = tagged[i].split(\"-\")[1]\n",
        "    except:\n",
        "      roles[i] = tagged[i].split(\"-\")[0]\n",
        "\n",
        "  print(words,f'\\nTagged words for the roles : ',roles)\n",
        "  print()\n",
        "# roles are selected as the one with effects only\n",
        "\n",
        "tagged = parsed[0][3]\n",
        "roles = parsed[0][2]\n",
        "for i in range(len(tagged)):\n",
        "    try:\n",
        "      roles[i] = tagged[i].split(\"-\")[1]\n",
        "    except:\n",
        "      roles[i] = tagged[i].split(\"-\")[0]\n",
        "\n",
        "print(words,'\\nTagged words for the roles : ',roles)  \n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "purpose = []\n",
        "instrument = []\n",
        "effects = []\n",
        "entity = []\n",
        "\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Purpose\":\n",
        "    purpose.append(words[_])\n",
        "    \n",
        "  if roles[_]==\"Instrument\":\n",
        "    instrument.append(words[_])\n",
        "  \n",
        "  if roles[_]==\"Effect\":\n",
        "    effects.append(words[_])\n",
        "    \n",
        "  \n",
        "  if roles[_]==\"Entity\":\n",
        "    entity.append(words[_])\n",
        "    \n",
        "  elif roles[_]!=\"_\": # using 포함\n",
        "    instrument.append(words[_])\n",
        "print(\"Effects \",effects)\n",
        "\n",
        "\n",
        "# Effects\n",
        "EFFECT = ' '.join(effects)\n",
        "pos = h.pos(EFFECT)\n",
        "imp = []\n",
        "for tup in pos:\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "EFFECT_TRUNCATED = ' '.join(imp)\n",
        "print(EFFECT_TRUNCATED)\n",
        "\n",
        "final_string = EFFECT_TRUNCATED\n",
        "print(\"😀 final string to be inserted \", final_string)\n",
        "\n",
        "    \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tagged vector of candidates:  ['O', 'O', 'B-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect', 'I-Effect']\n",
            "['그', '결과', '사용자가', '자연어로', '쓰인', '대본을', '텍2피에', '제공하면', '높은', '수준의', '피피티로', '제공할', '수', '있게', '되었습니다.'] \n",
            "Tagged words for the roles :  ['O', 'O', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect']\n",
            "\n",
            "tagged vector of candidates:  ['O', 'O', 'B-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Event', 'I-Entity', 'I-Entity', 'I-Entity', 'I-Entity', 'O', 'O']\n",
            "['그', '결과', '사용자가', '자연어로', '쓰인', '대본을', '텍2피에', '제공하면', '높은', '수준의', '피피티로', '제공할', '수', '있게', '되었습니다.'] \n",
            "Tagged words for the roles :  ['O', 'O', 'Entity', 'Entity', 'Entity', 'Entity', 'Entity', 'Entity', 'Event', 'Entity', 'Entity', 'Entity', 'Entity', 'O', 'O']\n",
            "\n",
            "['그', '결과', '사용자가', '자연어로', '쓰인', '대본을', '텍2피에', '제공하면', '높은', '수준의', '피피티로', '제공할', '수', '있게', '되었습니다.'] \n",
            "Tagged words for the roles :  ['O', 'O', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect', 'Effect']\n",
            "\n",
            "Effects  ['사용자가', '자연어로', '쓰인', '대본을', '텍2피에', '제공하면', '높은', '수준의', '피피티로', '제공할', '수', '있게', '되었습니다.']\n",
            "사용자 자연어 대본 텍2피 제공 수준 피피티 제공 수\n",
            "😀 final string to be inserted  사용자 자연어 대본 텍2피 제공 수준 피피티 제공 수\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89ANwCXZXnXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_vBkRqWXnfr",
        "colab_type": "text"
      },
      "source": [
        "## PPTX에 활용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ_738Krj4Wm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prs = Presentation(\"final_template.pptx\")  # 원하는 template 종류 불러오기\n",
        "slide_0 = prs.slides[0] # 제복 slide (제목 슬라이드)\n",
        "slide_2 = prs.slides[2] # 두 번째 slide"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ_iciyKj4Wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slide_2.placeholders.element[3][2][2][1][1].text = \"Two Track Process\" # 여덟 번째 슬라이드 제목\n",
        "slide_2.placeholders.element[5][2][2][1][1].text = \"\"\n",
        "slide_2.placeholders.element[6][2][2][1][1].text = \"\"\n",
        "slide_2.placeholders.element[17][2][2][1][1].text = \"\"\n",
        "slide_2.placeholders.element[18][2][2][1][1].text = \"\"\n",
        "slide_2.placeholders.element[19][2][2][1][1].text = \"TEXT2PPTX\"\n",
        "slide_2.placeholders.element[11][2][2][1][1].text = \"자연어처리의 최신 기술을 다수 사용\"\n",
        "slide_2.placeholders.element[10][2][2][1][1].text = \"대본을 피피티로 옮기기 전에 요약하고 분석\"\n",
        "slide_2.placeholders.element[14][2][2][1][1].text = \"파이썬에서 파워포인트의 소스에 접근하기 위해 xml 코드를 심층적으로 분석\"\n",
        "slide_2.placeholders.element[13][2][2][1][1].text = \"사용자가 자연어로 쓰인 대본을 텍2피에 제공하면 높은 수준의 피피티로 제공\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEgNzSF4Xq8a",
        "colab_type": "text"
      },
      "source": [
        "## 교육용 교재에 활용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRL1bZb4XusT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
