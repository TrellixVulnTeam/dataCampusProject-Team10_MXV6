{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutomatedRelationExtraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoonkim313/dataCampusProject-Team10/blob/master/Relation%20Extraction/5.AutomatedRelationExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "collapsed": true,
        "id": "obGBWFLnIypX",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "52b87c91-0deb-44ef-a5c3-4b3a163c24a7"
      },
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/mnt')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/mnt/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0, nb_path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/mnt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhtOguP693NI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d98050d0-abde-4826-a5e1-2ee191c4f8fc"
      },
      "source": [
        "!pip install --target=$nb_path transformers\n",
        "!apt-get update\n",
        "!apt-get g++ openjdk-8-jdk \n",
        "!pip3 install --target=$nb_path konlpy\n",
        "!pip install --target=$nb_path soykeyword\n",
        "!pip install --target=$nb_path krwordrank"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 5.1MB/s \n",
            "\u001b[?25hCollecting numpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/9a/7d474ba0860a41f771c9523d8c4ea56b084840b5ca4092d96bdee8a3b684/numpy-1.19.1-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5MB 304kB/s \n",
            "\u001b[?25hCollecting regex!=2019.12.17\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/f2/b3af9ce9df4b7e121dfeece41fc95e37b14f0153821f35d08edb0b0813ff/regex-2020.7.14-cp36-cp36m-manylinux2010_x86_64.whl (660kB)\n",
            "\u001b[K     |████████████████████████████████| 665kB 49.8MB/s \n",
            "\u001b[?25hCollecting requests\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.3MB/s \n",
            "\u001b[?25hCollecting packaging\n",
            "  Downloading https://files.pythonhosted.org/packages/46/19/c5ab91b1b05cfe63cccd5cfc971db9214c6dd6ced54e33c30d5af1d2bc43/packaging-20.4-py2.py3-none-any.whl\n",
            "Collecting filelock\n",
            "  Downloading https://files.pythonhosted.org/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\n",
            "Collecting dataclasses; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/d2/6f02df2616fd4016075f60157c7a0452b38d8f7938ae94343911e0fb0b09/dataclasses-0.7-py3-none-any.whl\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 46.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 48.8MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.27\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/7e/281edb5bc3274dfb894d90f4dbacfceaca381c2435ec6187a2c6f329aed7/tqdm-4.48.2-py2.py3-none-any.whl (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.7MB/s \n",
            "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/f0/a391d1463ebb1b233795cabfc0ef38d3db4442339de68f847026199e69d7/urllib3-1.25.10-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 57.4MB/s \n",
            "\u001b[?25hCollecting idna<3,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.0MB/s \n",
            "\u001b[?25hCollecting chardet<4,>=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 56.0MB/s \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl (156kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 55.7MB/s \n",
            "\u001b[?25hCollecting six\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
            "Collecting click\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.8MB/s \n",
            "\u001b[?25hCollecting joblib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/dd/0e015051b4a27ec5a58b02ab774059f3289a94b0906f880a3f9507e74f38/joblib-0.16.0-py3-none-any.whl (300kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 51.6MB/s \n",
            "\u001b[?25hCollecting pyparsing>=2.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=d5411d76d1d99c858d445448293ea841effa1c8d2be61ba9ba0396ee6795cd0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.25.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.24.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, regex, urllib3, idna, chardet, certifi, requests, six, click, joblib, tqdm, sacremoses, pyparsing, packaging, filelock, dataclasses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed certifi-2020.6.20 chardet-3.0.4 click-7.1.2 dataclasses-0.7 filelock-3.0.12 idna-2.10 joblib-0.16.0 numpy-1.19.1 packaging-20.4 pyparsing-2.4.7 regex-2020.7.14 requests-2.24.0 sacremoses-0.0.43 sentencepiece-0.1.91 six-1.15.0 tokenizers-0.8.1rc1 tqdm-4.48.2 transformers-3.0.2 urllib3-1.25.10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyparsing",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MbmSXGrwJ585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "6b41994d-caff-41e2-f103-a73f338ca336"
      },
      "source": [
        "from konlpy.tag import Hannanum, Okt\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import heapq\n",
        "import pandas as pd\n",
        "from operator import itemgetter\n",
        "from collections import deque\n",
        "from ast import literal_eval\n",
        "from collections import defaultdict\n",
        "from soykeyword.lasso import LassoKeywordExtractor\n",
        "from pprint import pprint\n",
        "from krwordrank.word import KRWordRank\n",
        "from copy import deepcopy\n",
        "import kss\n",
        "%cd /content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\n",
        "from frameBERT import frame_parser\n",
        "path=\"/content/drive/Shared drives/BigDATA TEAM 10/OpenInformationExtraction\"\n",
        "h = Hannanum()\n",
        "okt = Okt()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a49c715830fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHannanum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOkt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_v5o0el7I9ur",
        "colab": {}
      },
      "source": [
        "text = '''\n",
        "평등은 자유와 더불어 근대 사회의 핵심 이념으로 자리 잡고있다. 인간은 가령 인종이나 성별과 상관없이 누구나 평등하다고 생각한다. 모든 인간은 평등하다고 말하는데, 이 말은 무슨 뜻일까? 그리고 그 근거는 무엇인가? \n",
        "일단 이 말을 모든 인간을 모든 측면에서 똑같이 대우하는 절대칙 평등으로 생각하는 이는 없다. 인간은 저마다 다르게 가지고 대어난 능력과 소질을 똑같게 만들 수 없기 때문이다. 절대적 평등은 개인의 개성이냐 자율성 등의 가치와 충돌하기도 한다. 평등에 대한 요구는 모든 불평등을 악으로 보는 것이 아니라 충분한 이유가 제시되지 않은 불평등을 제거하는 데 목표를 두고 있다. ‘이유 없는 차별 금지’라는 조건적 평등 원칙은 차별 대우를 할 때는 이유를 제시할 것을 요구하고 있다. 이것은 어떤 이유가 제시된다면 특정한 부류에 속하는 사람들에게는 평등한 대우를, 그 부류에 속하지 않는 사람들에게는 차별저 대우를 하는 것을 허용한다. 그렇다면 사람들을 특정한 부류로 구분하는 기준은 무엇인가? 이것은 바로 평등의 근거에 대한 물음이다.\n",
        "근대의 여러 인권 선언에 나타난 평등 개념은 개인들 사이의 평등성을 타고난 자연적 권리로 간주하였다. 하지만 이러한 자연권 이론은 무엇이 자연적 권리이고 권리의 존재가 자명한 이유가 무엇인지 등의 문제에 부딪히게 된다. 그래서 롤스는 기존의 자연권 사상에 의존하지 않는 방시으로 인간 평등의 근거를 마런하려고 한다. 그는 어떤 규칙이 공평하고 일관되게 운영되며, 그 규칙에 따라 유사한 경우는 유사하게 취급된다면 형식적 정의는 실현된다고 본다. 하지만 롤스는 형식저 정의에 따라 규칙을 준수하는 것만으로는 정의를 담보할 수 없다고 생각한다. 그 규칙이 더 높은 도덕적 권위를 지닌 다른 이넘과 충돌할 수 었기에, 실질적 정의가 보장되기 위해서는 규칙의 내용이 중요한 것이다.\n",
        "롤스는 인간 평등의 근거를 설명하면서 영역 성질 (range property) 개념을 도입한다. 예를 들어 어떤 원의 내부에 있는 점들은 그 위치가 서로 다르지만 원의 내부에 있다는 점에서 동일한 영역 성질을 갖는다. 반면에 원의 내부에 있는 집과 원의 외부에 였는 점은원의 경계선을 기준으로 서로 다른 영역 성질을 갖는다. 그는 평등한 대우를 받기 위한 영역 성질로서 ‘도덕적 인격'을 제시한다. 도덕적 인격이란 도덕적 호소가 가능하고 그런 호소에 관심을 기이는 능력이 있다는 것인데, 이 능력을 최소치만 갖고 있다면 평등한 대우에 대한 권한을 갖게 된다. 도덕적 인격이라고 해서 도덕적으로 훌륭하다는 뜻이 아니라 도덕과 무관하다는 말과 대비되는 뜻으로 쓰고 있다. 그런데 어린 아이는 인격체로서의 최소한의 기준을 충족하고 있는지가 논란이 될 수 있다. 이에 대해 롤스는 도덕적 인격을 규정하는 최소한의 요구 조건은 잠재적 능력이지 그것의 실현 여부가 아니기에 어린 아이도 평등한 존재라고 말한다. 싱어는 위와 같은 롤스의 시도를 비판한다. 도덕에 대한 민감성의 수준은 사람에 따라 다르다. 그래서 도덕적 인격의 능력이 그렇게 중요하다면 그것을 갖춘 정도에 따라 도덕적 위계를 다르게 하지 말아야 할 이유가 분명하지 않다고 말한다. \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gYfwrUetU3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paragraphs = text.split('\\n')\n",
        "paragraphs = [i for i in paragraphs if i]\n",
        "wordrank_extractor = KRWordRank(min_count=4, max_length=10)\n",
        "keywords, rank, graph = wordrank_extractor.extract(paragraphs, num_keywords=30)\n",
        "print(keywords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGCJPBUtvXj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Text:\n",
        "  def __init__(self, text):\n",
        "    self.text = text\n",
        "    text = re.sub('\\n',' ',text)\n",
        "    text = re.sub(\"'\",' ',text)\n",
        "    paragraphs = text.split('\\n')\n",
        "    self.paragraphs = [i for i in paragraphs if i]\n",
        "    self.docs = [kss.split_sentences(paragraph) for paragraph in paragraphs if kss.split_sentences(paragraph)]\n",
        "\n",
        "  def keywords(self, num = 40):\n",
        "    wordrank_extractor = KRWordRank(min_count=4, max_length=10)\n",
        "    keywords, rank, graph = wordrank_extractor.extract(self.paragraphs, num_keywords=num)\n",
        "    p=[]\n",
        "    kw=[]\n",
        "    for k, v in keywords.items():\n",
        "      p.append(okt.pos(k))\n",
        "    for ls in p:\n",
        "      for tup in ls:\n",
        "          if tup[1].startswith(\"N\"):\n",
        "            kw.append(tup[0])\n",
        "    return kw\n",
        "    \n",
        "class Highlight(Text):\n",
        "  def __init__(self, text, candidates=None):\n",
        "    self.text = text\n",
        "    self.newtext = deepcopy(self.text)\n",
        "    if candidates==None:\n",
        "        conj = '그리고, 그런데, 그러나, 그래도, 그래서, 또는, 및, 즉, 게다가, 따라서, 때문에, 아니면, 왜냐하면, 단, 오히려, 비록'\n",
        "        conj = conj.replace(\"'\",\"\")\n",
        "        conj = conj.replace(\", \",\" \")\n",
        "        self.candidates = conj.split(\" \")\n",
        "        self.idx = [(self.text.find(\" \"+cand), len(cand)) for cand in self.candidates if self.text.find(cand)!=-1]\n",
        "    else:\n",
        "        self.candidates = candidates\n",
        "        self.idx = [(self.text.find(\" \"+cand), len(cand)) for cand in self.candidates if self.text.find(cand)!=-1]\n",
        "  def add_tags(self, tag):\n",
        "    for i in range(len(self.idx)):\n",
        "      idx = [(self.newtext.find(\" \"+cand), len(cand)) for cand in self.candidates if self.newtext.find(cand)!=-1]\n",
        "      indices = (idx[i][0],idx[i][0]+idx[i][1]+1)\n",
        "      word = self.newtext[indices[0]+1:indices[1]]\n",
        "      if candidates = None:\n",
        "        tagged = \"<mark style='background-color: #0F4C81'>%s</mark>\" % (tag, word, tag)\n",
        "      else:\n",
        "        tagged = \"<mark style='background-color: #FF6F61'>%s</mark>\" % (tag, word, tag)\n",
        "      self.newtext = tagged.join([self.newtext[:indices[0]], self.newtext[indices[1]:]])\n",
        "    return self.newtext\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moeLewu3QE2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c67281c-c34d-4cc2-827a-29ab6a89739a"
      },
      "source": [
        "def findall(p, s):\n",
        "    i = s.find(p)\n",
        "    while i != -1:\n",
        "        yield i\n",
        "        i = s.find(p, i+1)\n",
        "\n",
        "[(i, i+len(\"하지만\")) for i in findall(\"하지만\", text)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(590, 593), (782, 785)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYz2ZMyKyASO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "  t = Text(text)\n",
        "  \n",
        "  kw=t.keywords()\n",
        "  print(kw)\n",
        "  h = Highlight(text, kw)\n",
        "  tag = \"keyword\"\n",
        "  css_format = h.add_tags(tag)\n",
        "  print(css_format)\n",
        "  h = Highlight(text)\n",
        "  tag = \"conj\"  # html 만들 떄 mark 부분을 conj로 변경하기\n",
        "  css_format=h.add_tags(tag)\n",
        "  print(css_format)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZucoOKFqvFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft2LfkPlZhK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pprint(okt.tagset)\n",
        "print()\n",
        "pprint(h.tagset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA8bpjMZbtnM",
        "colab_type": "text"
      },
      "source": [
        "#### 하지만, 그런데 등의 연결사에 해당하는 부분 추출\n",
        "\n",
        "css 문법\n",
        "<html>\n",
        "<head>\n",
        "<style>\n",
        "mark { \n",
        "  background-color: blue;\n",
        "  color: black;\n",
        "}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "<p>A mark element is displayed like this:</p>\n",
        "\n",
        "평등은 자유와 더불어 근대 사회의 핵심 이념으로 자리 잡고있다. <mark>인간</mark>은 가령 인종이나 성별과 상관없이 누구나 <mark>평등</mark>하다고 생각한다.<keyword>모든</keyword> 인간은 평등하다고 말하는데, 이 말은 무슨 뜻일까? 그리고 그<keyword>근거</keyword>는<keyword>무엇</keyword>인가? \n",
        "일단 이 말을 모든 인간을 모든 측면에서 똑같이<keyword>대우</keyword>하는 절대칙 평등으로 생각하는 이는 없다. 인간은 저마다 다르게 가지고 대어난<keyword>능력</keyword>과 소질을 똑같게 만들 수 없기 때문이다. 절대적 평등은 개인의 개성이냐 자율성 등의 가치와 충돌하기도 한다. 평등에<keyword>대한</keyword> 요구는 모든 불평등을 악으로 보는 것이 아니라 충분한<keyword>이유</keyword>가<keyword>제시</keyword>되지 않은 불평등을 제거하는 데 목표를 두고 있다. ‘이유 없는 차별 금지’라는 조건적 평등<keyword>원</keyword>칙은 차별 대우를 할 때는 이유를 제시할 것을 요구하고 있다. 이것은 어떤 이유가 제시된다면 특정한 부류에 속하는<keyword>사람</keyword>들에게는 평등한 대우를, 그 부류에 속하지 않는 사람들에게는 차별저 대우를 하는 것을 허용한다. 그렇다면 사람들을 특정한 부류로 구분하는 기준은 무엇인가? 이것은 바로 평등의 근거에 대한 물음이다.\n",
        "근대의 여러 인권 선언에 나타난 평등 개념은 개인들 사이의 평등성을 타고난<keyword>자연</keyword>적 권리로 간주하였다. 하지만 이러한 자연권 이론은 무엇이 자연적 권리이고 권리의 존재가 자명한 이유가 무엇인지 등의 문제에 부딪히게 된다. 그래서<keyword>롤스</keyword>는 기존의 자연권 사상에 의존하지 않는 방시으로 인간 평등의 근거를 마런하려고 한다. 그는 어떤<keyword>규칙</keyword>이 공평하고 일관되게 운영되며, 그 규칙에 따라 유사한 경우는 유사하게 취급된다면 형식적<keyword>정의</keyword>는 실현된다고 본다. 하지만 롤스는 형식저 정의에 따라 규칙을 준수하는 것만으로는 정의를 담보할 수 없다고 생각한다. 그 규칙이 더 높은<keyword>도덕</keyword>적 권위를 지닌 다른 이넘과 충돌할 수 었기에, 실질적 정의가 보장되기 위해서는 규칙의 내용이 중요한 것이다.\n",
        "롤스는 인간 평등의 근거를 설명하면서<keyword>영역</keyword><keyword>성질</keyword> (range property) 개념을 도입한다. 예를 들어 어떤 원의 내부에 있는 점들은 그 위치가 서로 다르지만 원의 내부에 있다는 점에서 동일한 영역 성질을 갖는다. 반면에 원의 내부에 있는 집과 원의 외부에 였는 점은원의 경계선을 기준으로 서로 다른 영역 성질을 갖는다. 그는 평등한 대우를 받기 위한 영역 성질로서 ‘도덕적<keyword>인격</keyword>'을 제시한다. 도덕적 인격이란 도덕적 호소가 가능하고 그런 호소에 관심을 기이는 능력이 있다는 것인데, 이 능력을 최소치만 갖고 있다면 평등한 대우에 대한 권한을 갖게 된다. 도덕적 인격이라고 해서 도덕적으로 훌륭하다는 뜻이 아니라 도덕과 무관하다는 말과 대비되는 뜻으로 쓰고 있다. 그런데 어린 아이는 인격체로서의 최소한의 기준을 충족하고 있는지가 논란이 될 수 있다. 이에 대해 롤스는 도덕적 인격을 규정하는 최소한의 요구 조건은 잠재적 능력이지 그것의 실현 여부가 아니기에 어린 아이도 평등한 존재라고 말한다. 싱어는 위와 같은 롤스의 시도를 비판한다. 도덕에 대한 민감성의 수준은 사람에 따라 다르다. 그래서 도덕적 인격의 능력이 그렇게 중요하다면 그것을 갖춘 정도에 따라 도덕적 위계를 다르게 하지 말아야 할 이유가 분명하지 않다고 말한다. \n",
        "\n",
        "\n",
        "<p>Change the default CSS settings to see the effect.</p>\n",
        "\n",
        "</body>\n",
        "</html>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JS0FMkeN8Sb",
        "colab_type": "text"
      },
      "source": [
        "## Relation Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Af1flDKI9M8",
        "colab": {}
      },
      "source": [
        "parser = frame_parser.FrameParser(model_path=path, language='ko')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuHEQofWAsnA",
        "colab_type": "text"
      },
      "source": [
        "### Linked List Implemented to Show the whole list of sentences and their parsed tags in a consecutive manner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wIph64I5OnLZ",
        "colab": {}
      },
      "source": [
        "def split_into_sentences(object):\n",
        "  ls = kss.split_sentences(object)\n",
        "  return ls\n",
        "\n",
        "def frameParse(text):\n",
        "  parser = frame_parser.FrameParser(model_path=path, language='ko')\n",
        "  parsed = parser.parser(text, sent_id='1', result_format='conll')\n",
        "  return parsed\n",
        "  \n",
        "\n",
        "def sortCandidate(parsed, num_candidates):\n",
        "  for i in range(len(parsed)):\n",
        "    count=0\n",
        "    for element in parsed[i][3]:\n",
        "      if element.startswith(\"O\"):\n",
        "        count+=1\n",
        "  q[i] = len(words) - count\n",
        "  hq = heapq.nlargest(num_candidates, q)\n",
        "  indices.append(q.index(hq[idx]))\n",
        "  # for idx in range(num_candidates):\n",
        "  #   temp = parsed[q.index(hq[idx])]\n",
        "  #   words = temp[0]\n",
        "  #   [[row[i] for row in temp] for i in range(len(words))]\n",
        "  return indices\n",
        "\n",
        "\n",
        "def findConsecutiveBIO(words, tag):\n",
        "  began = False\n",
        "  count = 0\n",
        "  q = deque(words)\n",
        "  for i in range(len(words)):\n",
        "    if tag[i]=='O':\n",
        "      q.popleft()\n",
        "    if tag[i].startswith('B'):\n",
        "      began=True\n",
        "    if began & tag[i].startswith('I'):\n",
        "      count+=1\n",
        "  return q\n",
        "\n",
        "# Linked List 구현해서 parsed 후보군에 사용\n",
        "class Node:\n",
        "    def __init__(self, data):\n",
        "        self.words = data[0]\n",
        "        self.tags = data[3]\n",
        "        self.next = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str((self.words, self.tags))\n",
        "\n",
        "class LinkedList:\n",
        "  def __init__(self):\n",
        "      self.head = None\n",
        "\n",
        "  def __repr__(self):\n",
        "      node = self.head\n",
        "      nodes = []\n",
        "      while node is not None:\n",
        "          nodes.append(str(node.tags))\n",
        "          node = node.next\n",
        "      nodes.append(\"None\")\n",
        "      return ' -> '.join(nodes)\n",
        "  \n",
        "class LinkedList:\n",
        "  def __init__(self):\n",
        "      self.head = None\n",
        "\n",
        "  def __repr__(self):\n",
        "      node = self.head\n",
        "      nodes = []\n",
        "      while node is not None:\n",
        "          nodes.append(str(node.tags))\n",
        "          node = node.next\n",
        "      nodes.append(\"None\")\n",
        "      return ' -> '.join(nodes)\n",
        "\n",
        "def extractFrame(text):\n",
        "  ls = split_into_sentences(text)\n",
        "  final = {}\n",
        "  \n",
        "  for idx in range(len(ls)):\n",
        "    parsed = frameParse(ls[idx]) # candidates 생성\n",
        "    final.setdefault(idx,str)\n",
        "    parsedList =  LinkedList()\n",
        "    for j in range(len(parsed)):\n",
        "      parsed_candidate = parsed[j]\n",
        "      new_node = Node(parsed_candidate)\n",
        "      if j == 0:\n",
        "        old_node = new_node\n",
        "        parsedList.head = old_node\n",
        "      elif j==len(parsed)-1:\n",
        "        old_node.next = new_node\n",
        "        new_node.next = None\n",
        "        print(idx,'  ',parsedList)\n",
        "        final[idx] = parsedList\n",
        "      else:\n",
        "        old_node.next = new_node\n",
        "        old_node = new_node\n",
        "  return final\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD2QZjRqZaM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(text):\n",
        "  all = extractFrame(text)\n",
        "  s = \"\"\n",
        "  res = {}\n",
        "\n",
        "  i = 1\n",
        "  res.setdefault(i,tuple)\n",
        "  for k, v in all.items():\n",
        "    a = v.head\n",
        "    while a is not None:\n",
        "      temp = \" \"\n",
        "      if len(a.words) == len(a.tags):\n",
        "        q = findConsecutiveBIO(a.words, a.tags)\n",
        "      else:\n",
        "        print(\"ERROR OCCURED\")\n",
        "        \n",
        "      count = len(a.words)\n",
        "      for tag in a.tags:\n",
        "       if tag.startswith(\"O\"):\n",
        "          count -= 1\n",
        "      if count > len(a.words)*0.5:\n",
        "        print(\"The threshold is reached !! 😛 😝 😜 🤪 🤨 🧐 🤓 😎 🤩 🥳 😏\")\n",
        "        temp = \" \".join(q) \n",
        "        print(temp)\n",
        "        words = a.words\n",
        "        tags = a.tags\n",
        "      res[i] = (a.words, a.tags , \" \".join(q))\n",
        "      a = a.next\n",
        "    i+=1\n",
        "  return res\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIIiD0vrpdmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extracted = main(text)\n",
        "pprint(extracted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvm4aMtEz1A1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pprint(extracted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQNkF3aGty_0",
        "colab_type": "text"
      },
      "source": [
        "### TODO : 여기서부터 extracted를 가지고 purpose / goal / effect / entity 등을 찾아야함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf62cktfw2C1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k, tup in extracted.items():\n",
        "  a, b, c = tup\n",
        "  roles = [0]*len(b)\n",
        "  words = [0]*len(b)\n",
        "  for i in range(len(b)):\n",
        "    try: \n",
        "      roles[i] = b[i].split(\"-\")[1] \n",
        "      \n",
        "    except: \n",
        "      roles[i] = b[i].split(\"-\")[0]\n",
        "  print(roles)\n",
        "  for _ in roles:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S7zGTQO4BUO7",
        "colab": {}
      },
      "source": [
        "# 1st sentence : GOALS & MEANS \n",
        "\n",
        "parsed = parser.parser(ls[0], sent_id='1', result_format='conll')\n",
        "words = parsed[0][0]\n",
        "print(\"😀 Words vector from sentence \",words)\n",
        "q = [-1]*len(parsed)\n",
        "for i in range(len(parsed)):\n",
        "  count=0\n",
        "  for element in parsed[i][3]:\n",
        "    if element.startswith(\"O\"):\n",
        "      count+=1\n",
        "  q[i] = len(words) - count\n",
        "hq = heapq.nlargest(2, q)\n",
        "\n",
        "\n",
        "print(\"Number of parsed candidates \",len(parsed))\n",
        "print(q, heapq.nlargest(2, q)) # 첫번째 경우 사용\n",
        "\n",
        "\n",
        "words = parsed[0][0]\n",
        "roles = parsed[0][2]\n",
        "tagged = parsed[0][3]\n",
        "for idx in range(len(tagged)):\n",
        "  try:\n",
        "    roles[i] = tagged[i].split(\"-\")[1]\n",
        "  except:\n",
        "    roles[i] = tagged[i].split(\"-\")[0]\n",
        "\n",
        "print(\"roles \",roles)\n",
        "\n",
        "goals = []\n",
        "means = []\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Goal\":\n",
        "    goals.append(words[_])\n",
        "  if roles[_]=='Means':\n",
        "    means.append(words[_])\n",
        "\n",
        "# MEANS\n",
        "MEANS = ' '.join(means)\n",
        "pos = h.pos(MEANS)\n",
        "imp = []\n",
        "for tup in h.pos(MEANS):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "MEANS_TRUNCATED = ' '.join(imp)\n",
        "print(MEANS_TRUNCATED)\n",
        "\n",
        "# GOALS\n",
        "GOAL = ' '.join(goals)\n",
        "pos = h.pos(GOAL)\n",
        "imp = []\n",
        "for tup in h.pos(GOAL):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "GOAL_TRUNCATED = ' '.join(imp)\n",
        "print(GOAL_TRUNCATED)\n",
        "\n",
        "final_string = MEANS_TRUNCATED + ' -- >' + GOAL_TRUNCATED\n",
        "print(\"😀 final string to be inserted\",final_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F9zsJryuBRri",
        "colab": {}
      },
      "source": [
        "# 2nd sentence : PURPOSE & INSTRUMENT\n",
        "parsed = parser.parser(ls[1], sent_id='1', result_format='conll')\n",
        "words = parsed[0][0]\n",
        "print(\"😀 Words vector from sentence \",words)\n",
        "q = [-1]*len(parsed)\n",
        "for i in range(len(parsed)):\n",
        "  count=0\n",
        "  for element in parsed[i][3]:\n",
        "    if element.startswith(\"O\"):\n",
        "      count+=1\n",
        "  q[i] = len(words) - count\n",
        "\n",
        "\n",
        "print(\"Number of parsed candidates \",len(parsed))\n",
        "print(q, heapq.nlargest(3, q))\n",
        "print(itemgetter(*[1,5])(parsed))\n",
        "candidates = itemgetter(*[1,8])(parsed)\n",
        "for _ in candidates:\n",
        "  words = _[0]\n",
        "  tagged = _[3]\n",
        "  print(\"tagged vector of candidates: \",tagged)\n",
        "  roles = _[2]\n",
        "  for i in range(len(parsed)):\n",
        "    try:\n",
        "      roles[i] = tagged[i].split(\"-\")[1]\n",
        "    except:\n",
        "      roles[i] = tagged[i].split(\"-\")[0]\n",
        "  print(roles,f'Tagged words for the roles : {tagged}')\n",
        "# roles are selected as the one with purpose and instrument\n",
        "\n",
        "purpose = []\n",
        "instrument = []\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Purpose\":\n",
        "    purpose.append(words[_])\n",
        "    \n",
        "  elif roles[_]==\"Instrument\":\n",
        "    instrument.append(words[_])\n",
        "    \n",
        "  elif roles[_]!=\"_\": # using 포함\n",
        "    instrument.append(words[_])\n",
        "print(\"Instrument: \",instrument)\n",
        "print(\"Purpose: \",purpose)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# PURPOSE\n",
        "PURPOSE = ' '.join(purpose)\n",
        "pos = h.pos(PURPOSE)\n",
        "imp = []\n",
        "for tup in pos:\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "PURPOSE_TRUNCATED = ' '.join(imp)\n",
        "print(PURPOSE_TRUNCATED)\n",
        "\n",
        "# INSTRUMENT\n",
        "INSTRUMENT = ' '.join(instrument)\n",
        "pos = h.pos(INSTRUMENT)\n",
        "imp = []\n",
        "for tup in pos:\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "INST_TRUNCATED = ' '.join(imp)\n",
        "print(INST_TRUNCATED)\n",
        "\n",
        "final_string = INST_TRUNCATED+ ' ➡️' + PURPOSE_TRUNCATED \n",
        "print(\"😀 final string to be inserted \", final_string)\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HxZxxnMiVuKw",
        "colab": {}
      },
      "source": [
        "# 3rd sentence : Goal - Means\n",
        "parsed = parser.parser(ls[2], sent_id='1', result_format='conll')\n",
        "words = parsed[0][0]\n",
        "print(\"Words vector from sentence \",words)\n",
        "q = [-1]*len(parsed)\n",
        "for i in range(len(parsed)):\n",
        "  count=0\n",
        "  for element in parsed[i][3]:\n",
        "    if element.startswith(\"O\"):\n",
        "      count+=1\n",
        "  q[i] = len(words) - count\n",
        "\n",
        "print(q, heapq.nlargest(3, q))\n",
        "\n",
        "a,b,c=parsed\n",
        "tagged = b[3]\n",
        "for i in range(len(tagged)):\n",
        "  try:\n",
        "    roles[i] = tagged[i].split(\"-\")[1]\n",
        "  except:\n",
        "    roles[i] = tagged[i].split(\"-\")[0]\n",
        "\n",
        "\n",
        "\n",
        "goals = []\n",
        "means = []\n",
        "for _ in range(len(words)):\n",
        "  if roles[_]==\"Goal\":\n",
        "    goals.append(words[_])\n",
        "  if roles[_]=='Means':\n",
        "    means.append(words[_])\n",
        "\n",
        "# MEANS\n",
        "MEANS = ' '.join(means)\n",
        "pos = h.pos(MEANS)\n",
        "imp = []\n",
        "for tup in h.pos(MEANS):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "MEANS_TRUNCATED = ' '.join(imp)\n",
        "print(MEANS_TRUNCATED)\n",
        "\n",
        "# GOALS\n",
        "GOAL = ' '.join(goals)\n",
        "pos = h.pos(GOAL)\n",
        "imp = []\n",
        "for tup in h.pos(GOAL):\n",
        "  if tup[1]==\"N\":\n",
        "    imp.append(tup[0])\n",
        "GOAL_TRUNCATED = ' '.join(imp)\n",
        "print(GOAL_TRUNCATED)\n",
        "\n",
        "final_string = MEANS_TRUNCATED + ' -- >' + GOAL_TRUNCATED\n",
        "print(\"😀 final string to be inserted\",final_string)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_vBkRqWXnfr",
        "colab_type": "text"
      },
      "source": [
        "## PPTX에 활용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ_738Krj4Wm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prs = Presentation(\"final_template.pptx\")  # 원하는 template 종류 불러오기\n",
        "slide_0 = prs.slides[0] # 제복 slide (제목 슬라이드)\n",
        "slide_2 = prs.slides[2] # 두 번째 slide"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ_iciyKj4Wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slide_2.placeholders.element[3][2][2][1][1].text = \"Two Track Process\" # 여덟 번째 슬라이드 제목\n",
        "slide_2.placeholders.element[5][2][2][1][1].text = \"\"\n",
        "slide_2.placeholders.element[6][2][2][1][1].text = \"\"\n",
        "slide_2.placeholders.element[17][2][2][1][1].text = \"\"\n",
        "slide_2.placeholders.element[18][2][2][1][1].text = \"\"\n",
        "slide_2.placeholders.element[19][2][2][1][1].text = \"TEXT2PPTX\"\n",
        "slide_2.placeholders.element[11][2][2][1][1].text = \"자연어처리의 최신 기술을 다수 사용\"\n",
        "slide_2.placeholders.element[10][2][2][1][1].text = \"대본을 피피티로 옮기기 전에 요약하고 분석\"\n",
        "slide_2.placeholders.element[14][2][2][1][1].text = \"파이썬에서 파워포인트의 소스에 접근하기 위해 xml 코드를 심층적으로 분석\"\n",
        "slide_2.placeholders.element[13][2][2][1][1].text = \"사용자가 자연어로 쓰인 대본을 텍2피에 제공하면 높은 수준의 피피티로 제공\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEgNzSF4Xq8a",
        "colab_type": "text"
      },
      "source": [
        "## 교육용 교재에 활용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRL1bZb4XusT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}