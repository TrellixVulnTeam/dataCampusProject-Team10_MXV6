{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/printed_data_info.json',encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '한글 손글씨 데이터',\n",
       " 'description': 'Korean OCR Data Set (Printed Text, Normal)',\n",
       " 'data_created': '2019-10-11'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pd.DataFrame(data['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_captured</th>\n",
       "      <th>file_name</th>\n",
       "      <th>height</th>\n",
       "      <th>id</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-03 18:18:14</td>\n",
       "      <td>00000000.png</td>\n",
       "      <td>124</td>\n",
       "      <td>00000000</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-03 18:11:32</td>\n",
       "      <td>00000001.png</td>\n",
       "      <td>128</td>\n",
       "      <td>00000001</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-03 18:18:43</td>\n",
       "      <td>00000002.png</td>\n",
       "      <td>103</td>\n",
       "      <td>00000002</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-03 18:11:51</td>\n",
       "      <td>00000003.png</td>\n",
       "      <td>105</td>\n",
       "      <td>00000003</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-03 18:13:46</td>\n",
       "      <td>00000004.png</td>\n",
       "      <td>135</td>\n",
       "      <td>00000004</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date_captured     file_name  height        id  width\n",
       "0  2019-10-03 18:18:14  00000000.png     124  00000000     98\n",
       "1  2019-10-03 18:11:32  00000001.png     128  00000001     91\n",
       "2  2019-10-03 18:18:43  00000002.png     103  00000002    108\n",
       "3  2019-10-03 18:11:51  00000003.png     105  00000003    110\n",
       "4  2019-10-03 18:13:46  00000004.png     135  00000004    102"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.set_index('id').drop(columns='date_captured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.DataFrame(data['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attributes</th>\n",
       "      <th>id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'font': '만화진흥원', 'type': '글자(음절)', 'is_aug': ...</td>\n",
       "      <td>00000000</td>\n",
       "      <td>00000000</td>\n",
       "      <td>궶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'font': '고양', 'type': '글자(음절)', 'is_aug': False}</td>\n",
       "      <td>00000001</td>\n",
       "      <td>00000001</td>\n",
       "      <td>뵞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'font': '부산', 'type': '글자(음절)', 'is_aug': False}</td>\n",
       "      <td>00000002</td>\n",
       "      <td>00000002</td>\n",
       "      <td>푚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'font': '한겨레', 'type': '글자(음절)', 'is_aug': Fa...</td>\n",
       "      <td>00000003</td>\n",
       "      <td>00000003</td>\n",
       "      <td>섆</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'font': '노토산스', 'type': '글자(음절)', 'is_aug': F...</td>\n",
       "      <td>00000004</td>\n",
       "      <td>00000004</td>\n",
       "      <td>읂</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          attributes        id  image_id text\n",
       "0  {'font': '만화진흥원', 'type': '글자(음절)', 'is_aug': ...  00000000  00000000    궶\n",
       "1  {'font': '고양', 'type': '글자(음절)', 'is_aug': False}  00000001  00000001    뵞\n",
       "2  {'font': '부산', 'type': '글자(음절)', 'is_aug': False}  00000002  00000002    푚\n",
       "3  {'font': '한겨레', 'type': '글자(음절)', 'is_aug': Fa...  00000003  00000003    섆\n",
       "4  {'font': '노토산스', 'type': '글자(음절)', 'is_aug': F...  00000004  00000004    읂"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = annotations.set_index('id').drop(columns='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(images, annotations ,left_index=True, right_index=True,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>02477800</th>\n",
       "      <td>02477800.png</td>\n",
       "      <td>106</td>\n",
       "      <td>280</td>\n",
       "      <td>여행하다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02477801</th>\n",
       "      <td>02477801.png</td>\n",
       "      <td>117</td>\n",
       "      <td>278</td>\n",
       "      <td>하룻밤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02477802</th>\n",
       "      <td>02477802.png</td>\n",
       "      <td>106</td>\n",
       "      <td>303</td>\n",
       "      <td>장례식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02477803</th>\n",
       "      <td>02477803.png</td>\n",
       "      <td>106</td>\n",
       "      <td>212</td>\n",
       "      <td>파티</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02477804</th>\n",
       "      <td>02477804.png</td>\n",
       "      <td>128</td>\n",
       "      <td>282</td>\n",
       "      <td>마음씨</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file_name  height  width  text\n",
       "id                                         \n",
       "02477800  02477800.png     106    280  여행하다\n",
       "02477801  02477801.png     117    278   하룻밤\n",
       "02477802  02477802.png     106    303   장례식\n",
       "02477803  02477803.png     106    212    파티\n",
       "02477804  02477804.png     128    282   마음씨"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns = 'attributes')[802000:8023000].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "path = './data/sentence/'\n",
    "for filename in os.listdir('./data/sentence'):\n",
    "    new_path  = './data/all/'\n",
    "    shutil.move(path + filename, new_path + filename)\n",
    "    a +=1\n",
    "    if a % 10000 ==0:\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "path = './data/word/'\n",
    "for filename in os.listdir('./data/word'):\n",
    "    new_path  = './data/all/'\n",
    "    shutil.move(path + filename, new_path + filename)\n",
    "    a +=1\n",
    "    if a % 10000 ==0:\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "path = './data/syllable/'\n",
    "for filename in os.listdir('./data/syllable'):\n",
    "    new_path  = './data/all/'\n",
    "    shutil.move(path + filename, new_path + filename)\n",
    "    a +=1\n",
    "    if a % 20000 ==0:\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/all/\"\n",
    "train_dir = \"./data/training/\"\n",
    "validation_dir = './data/validation/'\n",
    "evaluation_dir = './data/evaluation/'\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    rand = np.random.rand(1)\n",
    "    if rand >= 0.4:\n",
    "        shutil.move(path + filename, train_dir + filename)\n",
    "    elif rand < 0.2:\n",
    "        shutil.move(path + filename, evaluation_dir + filename)\n",
    "    else:\n",
    "        shutil.move(path + filename, validation_dir + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series = pd.Series(os.listdir('./data/training/'))\n",
    "validation_series = pd.Series(os.listdir('./data/validation/'))\n",
    "evaulation_series = pd.Series(os.listdir('./data/evaluation/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_series,columns=['file_name'])\n",
    "validation_df = pd.DataFrame(validation_series,columns=['file_name'])\n",
    "evaluation_df = pd.DataFrame(evaulation_series,columns=['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(data, train_df, how='right')\n",
    "validation_df = pd.merge(data, validation_df, how='right')\n",
    "evaluation_df = pd.merge(data, evaluation_df, how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(501693, 167111, 166912)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(validation_df), len(evaluation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('train_gt.txt', mode='wt', encoding='utf-8')\n",
    "path = 'training/'\n",
    "for i in range(len(train_df)):\n",
    "    str1 = path + train_df['file_name'][i] + '\\t' + train_df['text'][i] + '\\n'\n",
    "    f.write(str1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('validation_gt.txt', mode='wt', encoding='utf-8')\n",
    "path = 'validation/'\n",
    "for i in range(len(validation_df)):\n",
    "    str1 = path + validation_df['file_name'][i] + '\\t' + validation_df['text'][i] + '\\n'\n",
    "    f.write(str1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('evaluation_gt.txt', mode='wt', encoding='utf-8')\n",
    "path = 'evaluation/'\n",
    "for i in range(len(evaluation_df)):\n",
    "    str1 = path + evaluation_df['file_name'][i] + '\\t' + evaluation_df['text'][i] + '\\n'\n",
    "    f.write(str1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('train_gt.txt', mode='r', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt update\n",
    "# !sudo apt-get install -y libgl1-mesa-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written 1000 / 501693\n",
      "Written 2000 / 501693\n",
      "Written 3000 / 501693\n",
      "Written 4000 / 501693\n",
      "Written 5000 / 501693\n",
      "Written 6000 / 501693\n",
      "Written 7000 / 501693\n",
      "Written 8000 / 501693\n",
      "Written 9000 / 501693\n",
      "Written 10000 / 501693\n",
      "Written 11000 / 501693\n",
      "Written 12000 / 501693\n",
      "Written 13000 / 501693\n",
      "Written 14000 / 501693\n",
      "Written 15000 / 501693\n",
      "Written 16000 / 501693\n",
      "Written 17000 / 501693\n",
      "Written 18000 / 501693\n",
      "Written 19000 / 501693\n",
      "Written 20000 / 501693\n",
      "Written 21000 / 501693\n",
      "Written 22000 / 501693\n",
      "Written 23000 / 501693\n",
      "Written 24000 / 501693\n",
      "Written 25000 / 501693\n",
      "Written 26000 / 501693\n",
      "Written 27000 / 501693\n",
      "Written 28000 / 501693\n",
      "Written 29000 / 501693\n",
      "Written 30000 / 501693\n",
      "Written 31000 / 501693\n",
      "Written 32000 / 501693\n",
      "Written 33000 / 501693\n",
      "Written 34000 / 501693\n",
      "Written 35000 / 501693\n",
      "Written 36000 / 501693\n",
      "Written 37000 / 501693\n",
      "Written 38000 / 501693\n",
      "Written 39000 / 501693\n",
      "Written 40000 / 501693\n",
      "Written 41000 / 501693\n",
      "Written 42000 / 501693\n",
      "Written 43000 / 501693\n",
      "Written 44000 / 501693\n",
      "Written 45000 / 501693\n",
      "Written 46000 / 501693\n",
      "Written 47000 / 501693\n",
      "Written 48000 / 501693\n",
      "Written 49000 / 501693\n",
      "Written 50000 / 501693\n",
      "Written 51000 / 501693\n",
      "Written 52000 / 501693\n",
      "Written 53000 / 501693\n",
      "Written 54000 / 501693\n",
      "Written 55000 / 501693\n",
      "Written 56000 / 501693\n",
      "Written 57000 / 501693\n",
      "Written 58000 / 501693\n",
      "Written 59000 / 501693\n",
      "Written 60000 / 501693\n",
      "Written 61000 / 501693\n",
      "Written 62000 / 501693\n",
      "Written 63000 / 501693\n",
      "Written 64000 / 501693\n",
      "Written 65000 / 501693\n",
      "Written 66000 / 501693\n",
      "Written 67000 / 501693\n",
      "Written 68000 / 501693\n",
      "Written 69000 / 501693\n",
      "Written 70000 / 501693\n",
      "Written 71000 / 501693\n",
      "Written 72000 / 501693\n",
      "Written 73000 / 501693\n",
      "Written 74000 / 501693\n",
      "Written 75000 / 501693\n",
      "Written 76000 / 501693\n",
      "Written 77000 / 501693\n",
      "Written 78000 / 501693\n",
      "Written 79000 / 501693\n",
      "Written 80000 / 501693\n",
      "Written 81000 / 501693\n",
      "Written 82000 / 501693\n",
      "Written 83000 / 501693\n",
      "Written 84000 / 501693\n",
      "Written 85000 / 501693\n",
      "Written 86000 / 501693\n",
      "Written 87000 / 501693\n",
      "Written 88000 / 501693\n",
      "Written 89000 / 501693\n",
      "Written 90000 / 501693\n",
      "Written 91000 / 501693\n",
      "Written 92000 / 501693\n",
      "Written 93000 / 501693\n",
      "Written 94000 / 501693\n",
      "Written 95000 / 501693\n",
      "Written 96000 / 501693\n",
      "Written 97000 / 501693\n",
      "Written 98000 / 501693\n",
      "Written 99000 / 501693\n",
      "Written 100000 / 501693\n",
      "Written 101000 / 501693\n",
      "Written 102000 / 501693\n",
      "Written 103000 / 501693\n",
      "Written 104000 / 501693\n",
      "Written 105000 / 501693\n",
      "Written 106000 / 501693\n",
      "Written 107000 / 501693\n",
      "Written 108000 / 501693\n",
      "Written 109000 / 501693\n",
      "Written 110000 / 501693\n",
      "Written 111000 / 501693\n",
      "Written 112000 / 501693\n",
      "Written 113000 / 501693\n",
      "Written 114000 / 501693\n",
      "Written 115000 / 501693\n",
      "Written 116000 / 501693\n",
      "Written 117000 / 501693\n",
      "Written 118000 / 501693\n",
      "Written 119000 / 501693\n",
      "Written 120000 / 501693\n",
      "Written 121000 / 501693\n",
      "Written 122000 / 501693\n",
      "Written 123000 / 501693\n",
      "Written 124000 / 501693\n",
      "Written 125000 / 501693\n",
      "Written 126000 / 501693\n",
      "Written 127000 / 501693\n",
      "Written 128000 / 501693\n",
      "Written 129000 / 501693\n",
      "Written 130000 / 501693\n",
      "Written 131000 / 501693\n",
      "Written 132000 / 501693\n",
      "Written 133000 / 501693\n",
      "Written 134000 / 501693\n",
      "Written 135000 / 501693\n",
      "Written 136000 / 501693\n",
      "Written 137000 / 501693\n",
      "Written 138000 / 501693\n",
      "Written 139000 / 501693\n",
      "Written 140000 / 501693\n",
      "Written 141000 / 501693\n",
      "Written 142000 / 501693\n",
      "Written 143000 / 501693\n",
      "Written 144000 / 501693\n",
      "Written 145000 / 501693\n",
      "Written 146000 / 501693\n",
      "Written 147000 / 501693\n",
      "Written 148000 / 501693\n",
      "Written 149000 / 501693\n",
      "Written 150000 / 501693\n",
      "Written 151000 / 501693\n",
      "Written 152000 / 501693\n",
      "Written 153000 / 501693\n",
      "Written 154000 / 501693\n",
      "Written 155000 / 501693\n",
      "Written 156000 / 501693\n",
      "Written 157000 / 501693\n",
      "Written 158000 / 501693\n",
      "Written 159000 / 501693\n",
      "Written 160000 / 501693\n",
      "Written 161000 / 501693\n",
      "Written 162000 / 501693\n",
      "Written 163000 / 501693\n",
      "Written 164000 / 501693\n",
      "Written 165000 / 501693\n",
      "Written 166000 / 501693\n",
      "Written 167000 / 501693\n",
      "Written 168000 / 501693\n",
      "Written 169000 / 501693\n",
      "Written 170000 / 501693\n",
      "Written 171000 / 501693\n",
      "Written 172000 / 501693\n",
      "Written 173000 / 501693\n",
      "Written 174000 / 501693\n",
      "Written 175000 / 501693\n",
      "Written 176000 / 501693\n",
      "Written 177000 / 501693\n",
      "Written 178000 / 501693\n",
      "Written 179000 / 501693\n",
      "Written 180000 / 501693\n",
      "Written 181000 / 501693\n",
      "Written 182000 / 501693\n",
      "Written 183000 / 501693\n",
      "Written 184000 / 501693\n",
      "Written 185000 / 501693\n",
      "Written 186000 / 501693\n",
      "Written 187000 / 501693\n",
      "Written 188000 / 501693\n",
      "Written 189000 / 501693\n",
      "Written 190000 / 501693\n",
      "Written 191000 / 501693\n",
      "Written 192000 / 501693\n",
      "Written 193000 / 501693\n",
      "Written 194000 / 501693\n",
      "Written 195000 / 501693\n",
      "Written 196000 / 501693\n",
      "Written 197000 / 501693\n",
      "Written 198000 / 501693\n",
      "Written 199000 / 501693\n",
      "Written 200000 / 501693\n",
      "Written 201000 / 501693\n",
      "Written 202000 / 501693\n",
      "Written 203000 / 501693\n",
      "Written 204000 / 501693\n",
      "Written 205000 / 501693\n",
      "Written 206000 / 501693\n",
      "Written 207000 / 501693\n",
      "Written 208000 / 501693\n",
      "Written 209000 / 501693\n",
      "Written 210000 / 501693\n",
      "Written 211000 / 501693\n",
      "Written 212000 / 501693\n",
      "Written 213000 / 501693\n",
      "Written 214000 / 501693\n",
      "Written 215000 / 501693\n",
      "Written 216000 / 501693\n",
      "Written 217000 / 501693\n",
      "Written 218000 / 501693\n",
      "Written 219000 / 501693\n",
      "Written 220000 / 501693\n",
      "Written 221000 / 501693\n",
      "Written 222000 / 501693\n",
      "Written 223000 / 501693\n",
      "Written 224000 / 501693\n",
      "Written 225000 / 501693\n",
      "Written 226000 / 501693\n",
      "Written 227000 / 501693\n",
      "Written 228000 / 501693\n",
      "Written 229000 / 501693\n",
      "Written 230000 / 501693\n",
      "Written 231000 / 501693\n",
      "Written 232000 / 501693\n",
      "Written 233000 / 501693\n",
      "Written 234000 / 501693\n",
      "Written 235000 / 501693\n",
      "Written 236000 / 501693\n",
      "Written 237000 / 501693\n",
      "Written 238000 / 501693\n",
      "Written 239000 / 501693\n",
      "Written 240000 / 501693\n",
      "Written 241000 / 501693\n",
      "Written 242000 / 501693\n",
      "Written 243000 / 501693\n",
      "Written 244000 / 501693\n",
      "Written 245000 / 501693\n",
      "Written 246000 / 501693\n",
      "Written 247000 / 501693\n",
      "Written 248000 / 501693\n",
      "Written 249000 / 501693\n",
      "Written 250000 / 501693\n",
      "Written 251000 / 501693\n",
      "Written 252000 / 501693\n",
      "Written 253000 / 501693\n",
      "Written 254000 / 501693\n",
      "Written 255000 / 501693\n",
      "Written 256000 / 501693\n",
      "Written 257000 / 501693\n",
      "Written 258000 / 501693\n",
      "Written 259000 / 501693\n",
      "Written 260000 / 501693\n",
      "Written 261000 / 501693\n",
      "Written 262000 / 501693\n",
      "Written 263000 / 501693\n",
      "Written 264000 / 501693\n",
      "Written 265000 / 501693\n",
      "Written 266000 / 501693\n",
      "Written 267000 / 501693\n",
      "Written 268000 / 501693\n",
      "Written 269000 / 501693\n",
      "Written 270000 / 501693\n",
      "Written 271000 / 501693\n",
      "Written 272000 / 501693\n",
      "Written 273000 / 501693\n",
      "Written 274000 / 501693\n",
      "Written 275000 / 501693\n",
      "Written 276000 / 501693\n",
      "Written 277000 / 501693\n",
      "Written 278000 / 501693\n",
      "Written 279000 / 501693\n",
      "Written 280000 / 501693\n",
      "Written 281000 / 501693\n",
      "Written 282000 / 501693\n",
      "Written 283000 / 501693\n",
      "Written 284000 / 501693\n",
      "Written 285000 / 501693\n",
      "Written 286000 / 501693\n",
      "Written 287000 / 501693\n",
      "Written 288000 / 501693\n",
      "Written 289000 / 501693\n",
      "Written 290000 / 501693\n",
      "Written 291000 / 501693\n",
      "Written 292000 / 501693\n",
      "Written 293000 / 501693\n",
      "Written 294000 / 501693\n",
      "Written 295000 / 501693\n",
      "Written 296000 / 501693\n",
      "Written 297000 / 501693\n",
      "Written 298000 / 501693\n",
      "Written 299000 / 501693\n",
      "Written 300000 / 501693\n",
      "Written 301000 / 501693\n",
      "Written 302000 / 501693\n",
      "Written 303000 / 501693\n",
      "Written 304000 / 501693\n",
      "Written 305000 / 501693\n",
      "Written 306000 / 501693\n",
      "Written 307000 / 501693\n",
      "Written 308000 / 501693\n",
      "Written 309000 / 501693\n",
      "Written 310000 / 501693\n",
      "Written 311000 / 501693\n",
      "Written 312000 / 501693\n",
      "Written 313000 / 501693\n",
      "Written 314000 / 501693\n",
      "Written 315000 / 501693\n",
      "Written 316000 / 501693\n",
      "Written 317000 / 501693\n",
      "Written 318000 / 501693\n",
      "Written 319000 / 501693\n",
      "Written 320000 / 501693\n",
      "Written 321000 / 501693\n",
      "Written 322000 / 501693\n",
      "Written 323000 / 501693\n",
      "Written 324000 / 501693\n",
      "Written 325000 / 501693\n",
      "Written 326000 / 501693\n",
      "Written 327000 / 501693\n",
      "Written 328000 / 501693\n",
      "Written 329000 / 501693\n",
      "Written 330000 / 501693\n",
      "Written 331000 / 501693\n",
      "Written 332000 / 501693\n",
      "Written 333000 / 501693\n",
      "Written 334000 / 501693\n",
      "Written 335000 / 501693\n",
      "Written 336000 / 501693\n",
      "Written 337000 / 501693\n",
      "Written 338000 / 501693\n",
      "Written 339000 / 501693\n",
      "Written 340000 / 501693\n",
      "Written 341000 / 501693\n",
      "Written 342000 / 501693\n",
      "Written 343000 / 501693\n",
      "Written 344000 / 501693\n",
      "Written 345000 / 501693\n",
      "Written 346000 / 501693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written 347000 / 501693\n",
      "Written 348000 / 501693\n",
      "Written 349000 / 501693\n",
      "Written 350000 / 501693\n",
      "Written 351000 / 501693\n",
      "Written 352000 / 501693\n",
      "Written 353000 / 501693\n",
      "Written 354000 / 501693\n",
      "Written 355000 / 501693\n",
      "Written 356000 / 501693\n",
      "Written 357000 / 501693\n",
      "Written 358000 / 501693\n",
      "Written 359000 / 501693\n",
      "Written 360000 / 501693\n",
      "Written 361000 / 501693\n",
      "Written 362000 / 501693\n",
      "Written 363000 / 501693\n",
      "Written 364000 / 501693\n",
      "Written 365000 / 501693\n",
      "Written 366000 / 501693\n",
      "Written 367000 / 501693\n",
      "Written 368000 / 501693\n",
      "Written 369000 / 501693\n",
      "Written 370000 / 501693\n",
      "Written 371000 / 501693\n",
      "Written 372000 / 501693\n",
      "Written 373000 / 501693\n",
      "Written 374000 / 501693\n",
      "Written 375000 / 501693\n",
      "Written 376000 / 501693\n",
      "Written 377000 / 501693\n",
      "Written 378000 / 501693\n",
      "Written 379000 / 501693\n",
      "Written 380000 / 501693\n",
      "Written 381000 / 501693\n",
      "Written 382000 / 501693\n",
      "Written 383000 / 501693\n",
      "Written 384000 / 501693\n",
      "Written 385000 / 501693\n",
      "Written 386000 / 501693\n",
      "Written 387000 / 501693\n",
      "Written 388000 / 501693\n",
      "Written 389000 / 501693\n",
      "Written 390000 / 501693\n",
      "Written 391000 / 501693\n",
      "Written 392000 / 501693\n",
      "Written 393000 / 501693\n",
      "Written 394000 / 501693\n",
      "Written 395000 / 501693\n",
      "Written 396000 / 501693\n",
      "Written 397000 / 501693\n",
      "Written 398000 / 501693\n",
      "Written 399000 / 501693\n",
      "Written 400000 / 501693\n",
      "Written 401000 / 501693\n",
      "Written 402000 / 501693\n",
      "Written 403000 / 501693\n",
      "Written 404000 / 501693\n",
      "Written 405000 / 501693\n",
      "Written 406000 / 501693\n",
      "Written 407000 / 501693\n",
      "Written 408000 / 501693\n",
      "Written 409000 / 501693\n",
      "Written 410000 / 501693\n",
      "Written 411000 / 501693\n",
      "Written 412000 / 501693\n",
      "Written 413000 / 501693\n",
      "Written 414000 / 501693\n",
      "Written 415000 / 501693\n",
      "Written 416000 / 501693\n",
      "Written 417000 / 501693\n",
      "Written 418000 / 501693\n",
      "Written 419000 / 501693\n",
      "Written 420000 / 501693\n",
      "Written 421000 / 501693\n",
      "Written 422000 / 501693\n",
      "Written 423000 / 501693\n",
      "Written 424000 / 501693\n",
      "Written 425000 / 501693\n",
      "Written 426000 / 501693\n",
      "Written 427000 / 501693\n",
      "Written 428000 / 501693\n",
      "Written 429000 / 501693\n",
      "Written 430000 / 501693\n",
      "Written 431000 / 501693\n",
      "Written 432000 / 501693\n",
      "Written 433000 / 501693\n",
      "Written 434000 / 501693\n",
      "Written 435000 / 501693\n",
      "Written 436000 / 501693\n",
      "Written 437000 / 501693\n",
      "Written 438000 / 501693\n",
      "Written 439000 / 501693\n",
      "Written 440000 / 501693\n",
      "Written 441000 / 501693\n",
      "Written 442000 / 501693\n",
      "Written 443000 / 501693\n",
      "Written 444000 / 501693\n",
      "Written 445000 / 501693\n",
      "Written 446000 / 501693\n",
      "Written 447000 / 501693\n",
      "Written 448000 / 501693\n",
      "Written 449000 / 501693\n",
      "Written 450000 / 501693\n",
      "Written 451000 / 501693\n",
      "Written 452000 / 501693\n",
      "Written 453000 / 501693\n",
      "Written 454000 / 501693\n",
      "Written 455000 / 501693\n",
      "Written 456000 / 501693\n",
      "Written 457000 / 501693\n",
      "Written 458000 / 501693\n",
      "Written 459000 / 501693\n",
      "Written 460000 / 501693\n",
      "Written 461000 / 501693\n",
      "Written 462000 / 501693\n",
      "Written 463000 / 501693\n",
      "Written 464000 / 501693\n",
      "Written 465000 / 501693\n",
      "Written 466000 / 501693\n",
      "Written 467000 / 501693\n",
      "Written 468000 / 501693\n",
      "Written 469000 / 501693\n",
      "Written 470000 / 501693\n",
      "Written 471000 / 501693\n",
      "Written 472000 / 501693\n",
      "Written 473000 / 501693\n",
      "Written 474000 / 501693\n",
      "Written 475000 / 501693\n",
      "Written 476000 / 501693\n",
      "Written 477000 / 501693\n",
      "Written 478000 / 501693\n",
      "Written 479000 / 501693\n",
      "Written 480000 / 501693\n",
      "Written 481000 / 501693\n",
      "Written 482000 / 501693\n",
      "Written 483000 / 501693\n",
      "Written 484000 / 501693\n",
      "Written 485000 / 501693\n",
      "Written 486000 / 501693\n",
      "Written 487000 / 501693\n",
      "Written 488000 / 501693\n",
      "Written 489000 / 501693\n",
      "Written 490000 / 501693\n",
      "Written 491000 / 501693\n",
      "Written 492000 / 501693\n",
      "Written 493000 / 501693\n",
      "Written 494000 / 501693\n",
      "Written 495000 / 501693\n",
      "Written 496000 / 501693\n",
      "Written 497000 / 501693\n",
      "Written 498000 / 501693\n",
      "Written 499000 / 501693\n",
      "Written 500000 / 501693\n",
      "Written 501000 / 501693\n",
      "Created dataset with 501693 samples\n",
      "Written 1000 / 167111\n",
      "Written 2000 / 167111\n",
      "Written 3000 / 167111\n",
      "Written 4000 / 167111\n",
      "Written 5000 / 167111\n",
      "Written 6000 / 167111\n",
      "Written 7000 / 167111\n",
      "Written 8000 / 167111\n",
      "Written 9000 / 167111\n",
      "Written 10000 / 167111\n",
      "Written 11000 / 167111\n",
      "Written 12000 / 167111\n",
      "Written 13000 / 167111\n",
      "Written 14000 / 167111\n",
      "Written 15000 / 167111\n",
      "Written 16000 / 167111\n",
      "Written 17000 / 167111\n",
      "Written 18000 / 167111\n",
      "Written 19000 / 167111\n",
      "Written 20000 / 167111\n",
      "Written 21000 / 167111\n",
      "Written 22000 / 167111\n",
      "Written 23000 / 167111\n",
      "Written 24000 / 167111\n",
      "Written 25000 / 167111\n",
      "Written 26000 / 167111\n",
      "Written 27000 / 167111\n",
      "Written 28000 / 167111\n",
      "Written 29000 / 167111\n",
      "Written 30000 / 167111\n",
      "Written 31000 / 167111\n",
      "Written 32000 / 167111\n",
      "Written 33000 / 167111\n",
      "Written 34000 / 167111\n",
      "Written 35000 / 167111\n",
      "Written 36000 / 167111\n",
      "Written 37000 / 167111\n",
      "Written 38000 / 167111\n",
      "Written 39000 / 167111\n",
      "Written 40000 / 167111\n",
      "Written 41000 / 167111\n",
      "Written 42000 / 167111\n",
      "Written 43000 / 167111\n",
      "Written 44000 / 167111\n",
      "Written 45000 / 167111\n",
      "Written 46000 / 167111\n",
      "Written 47000 / 167111\n",
      "Written 48000 / 167111\n",
      "Written 49000 / 167111\n",
      "Written 50000 / 167111\n",
      "Written 51000 / 167111\n",
      "Written 52000 / 167111\n",
      "Written 53000 / 167111\n",
      "Written 54000 / 167111\n",
      "Written 55000 / 167111\n",
      "Written 56000 / 167111\n",
      "Written 57000 / 167111\n",
      "Written 58000 / 167111\n",
      "Written 59000 / 167111\n",
      "Written 60000 / 167111\n",
      "Written 61000 / 167111\n",
      "Written 62000 / 167111\n",
      "Written 63000 / 167111\n",
      "Written 64000 / 167111\n",
      "Written 65000 / 167111\n",
      "Written 66000 / 167111\n",
      "Written 67000 / 167111\n",
      "Written 68000 / 167111\n",
      "Written 69000 / 167111\n",
      "Written 70000 / 167111\n",
      "Written 71000 / 167111\n",
      "Written 72000 / 167111\n",
      "Written 73000 / 167111\n",
      "Written 74000 / 167111\n",
      "Written 75000 / 167111\n",
      "Written 76000 / 167111\n",
      "Written 77000 / 167111\n",
      "Written 78000 / 167111\n",
      "Written 79000 / 167111\n",
      "Written 80000 / 167111\n",
      "Written 81000 / 167111\n",
      "Written 82000 / 167111\n",
      "Written 83000 / 167111\n",
      "Written 84000 / 167111\n",
      "Written 85000 / 167111\n",
      "Written 86000 / 167111\n",
      "Written 87000 / 167111\n",
      "Written 88000 / 167111\n",
      "Written 89000 / 167111\n",
      "Written 90000 / 167111\n",
      "Written 91000 / 167111\n",
      "Written 92000 / 167111\n",
      "Written 93000 / 167111\n",
      "Written 94000 / 167111\n",
      "Written 95000 / 167111\n",
      "Written 96000 / 167111\n",
      "Written 97000 / 167111\n",
      "Written 98000 / 167111\n",
      "Written 99000 / 167111\n",
      "Written 100000 / 167111\n",
      "Written 101000 / 167111\n",
      "Written 102000 / 167111\n",
      "Written 103000 / 167111\n",
      "Written 104000 / 167111\n",
      "Written 105000 / 167111\n",
      "Written 106000 / 167111\n",
      "Written 107000 / 167111\n",
      "Written 108000 / 167111\n",
      "Written 109000 / 167111\n",
      "Written 110000 / 167111\n",
      "Written 111000 / 167111\n",
      "Written 112000 / 167111\n",
      "Written 113000 / 167111\n",
      "Written 114000 / 167111\n",
      "Written 115000 / 167111\n",
      "Written 116000 / 167111\n",
      "Written 117000 / 167111\n",
      "Written 118000 / 167111\n",
      "Written 119000 / 167111\n",
      "Written 120000 / 167111\n",
      "Written 121000 / 167111\n",
      "Written 122000 / 167111\n",
      "Written 123000 / 167111\n",
      "Written 124000 / 167111\n",
      "Written 125000 / 167111\n",
      "Written 126000 / 167111\n",
      "Written 127000 / 167111\n",
      "Written 128000 / 167111\n",
      "Written 129000 / 167111\n",
      "Written 130000 / 167111\n",
      "Written 131000 / 167111\n",
      "Written 132000 / 167111\n",
      "Written 133000 / 167111\n",
      "Written 134000 / 167111\n",
      "Written 135000 / 167111\n",
      "Written 136000 / 167111\n",
      "Written 137000 / 167111\n",
      "Written 138000 / 167111\n",
      "Written 139000 / 167111\n",
      "Written 140000 / 167111\n",
      "Written 141000 / 167111\n",
      "Written 142000 / 167111\n",
      "Written 143000 / 167111\n",
      "Written 144000 / 167111\n",
      "Written 145000 / 167111\n",
      "Written 146000 / 167111\n",
      "Written 147000 / 167111\n",
      "Written 148000 / 167111\n",
      "Written 149000 / 167111\n",
      "Written 150000 / 167111\n",
      "Written 151000 / 167111\n",
      "Written 152000 / 167111\n",
      "Written 153000 / 167111\n",
      "Written 154000 / 167111\n",
      "Written 155000 / 167111\n",
      "Written 156000 / 167111\n",
      "Written 157000 / 167111\n",
      "Written 158000 / 167111\n",
      "Written 159000 / 167111\n",
      "Written 160000 / 167111\n",
      "Written 161000 / 167111\n",
      "Written 162000 / 167111\n",
      "Written 163000 / 167111\n",
      "Written 164000 / 167111\n",
      "Written 165000 / 167111\n",
      "Written 166000 / 167111\n",
      "Written 167000 / 167111\n",
      "Created dataset with 167111 samples\n"
     ]
    }
   ],
   "source": [
    "# create train_lmdb file validation_lmdb\n",
    "!python3 ./deep-text-recognition-benchmark/create_lmdb_dataset.py \\\n",
    "--inputPath ./data \\\n",
    "--gtFile ./data/train_gt.txt \\\n",
    "--outputPath ./data/data_lmdb/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation_lmdb file\n",
    "!python3 ./deep-text-recognition-benchmark/create_lmdb_dataset.py \\\n",
    "--inputPath ./data \\\n",
    "--gtFile ./data/validation_gt.txt \\\n",
    "--outputPath ./data/data_lmdb/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written 1000 / 166912\n",
      "Written 2000 / 166912\n",
      "Written 3000 / 166912\n",
      "Written 4000 / 166912\n",
      "Written 5000 / 166912\n",
      "Written 6000 / 166912\n",
      "Written 7000 / 166912\n",
      "Written 8000 / 166912\n",
      "Written 9000 / 166912\n",
      "Written 10000 / 166912\n",
      "Written 11000 / 166912\n",
      "Written 12000 / 166912\n",
      "Written 13000 / 166912\n",
      "Written 14000 / 166912\n",
      "Written 15000 / 166912\n",
      "Written 16000 / 166912\n",
      "Written 17000 / 166912\n",
      "Written 18000 / 166912\n",
      "Written 19000 / 166912\n",
      "Written 20000 / 166912\n",
      "Written 21000 / 166912\n",
      "Written 22000 / 166912\n",
      "Written 23000 / 166912\n",
      "Written 24000 / 166912\n",
      "Written 25000 / 166912\n",
      "Written 26000 / 166912\n",
      "Written 27000 / 166912\n",
      "Written 28000 / 166912\n",
      "Written 29000 / 166912\n",
      "Written 30000 / 166912\n",
      "Written 31000 / 166912\n",
      "Written 32000 / 166912\n",
      "Written 33000 / 166912\n",
      "Written 34000 / 166912\n",
      "Written 35000 / 166912\n",
      "Written 36000 / 166912\n",
      "Written 37000 / 166912\n",
      "Written 38000 / 166912\n",
      "Written 39000 / 166912\n",
      "Written 40000 / 166912\n",
      "Written 41000 / 166912\n",
      "Written 42000 / 166912\n",
      "Written 43000 / 166912\n",
      "Written 44000 / 166912\n",
      "Written 45000 / 166912\n",
      "Written 46000 / 166912\n",
      "Written 47000 / 166912\n",
      "Written 48000 / 166912\n",
      "Written 49000 / 166912\n",
      "Written 50000 / 166912\n",
      "Written 51000 / 166912\n",
      "Written 52000 / 166912\n",
      "Written 53000 / 166912\n",
      "Written 54000 / 166912\n",
      "Written 55000 / 166912\n",
      "Written 56000 / 166912\n",
      "Written 57000 / 166912\n",
      "Written 58000 / 166912\n",
      "Written 59000 / 166912\n",
      "Written 60000 / 166912\n",
      "Written 61000 / 166912\n",
      "Written 62000 / 166912\n",
      "Written 63000 / 166912\n",
      "Written 64000 / 166912\n",
      "Written 65000 / 166912\n",
      "Written 66000 / 166912\n",
      "Written 67000 / 166912\n",
      "Written 68000 / 166912\n",
      "Written 69000 / 166912\n",
      "Written 70000 / 166912\n",
      "Written 71000 / 166912\n",
      "Written 72000 / 166912\n",
      "Written 73000 / 166912\n",
      "Written 74000 / 166912\n",
      "Written 75000 / 166912\n",
      "Written 76000 / 166912\n",
      "Written 77000 / 166912\n",
      "Written 78000 / 166912\n",
      "Written 79000 / 166912\n",
      "Written 80000 / 166912\n",
      "Written 81000 / 166912\n",
      "Written 82000 / 166912\n",
      "Written 83000 / 166912\n",
      "Written 84000 / 166912\n",
      "Written 85000 / 166912\n",
      "Written 86000 / 166912\n",
      "Written 87000 / 166912\n",
      "Written 88000 / 166912\n",
      "Written 89000 / 166912\n",
      "Written 90000 / 166912\n",
      "Written 91000 / 166912\n",
      "Written 92000 / 166912\n",
      "Written 93000 / 166912\n",
      "Written 94000 / 166912\n",
      "Written 95000 / 166912\n",
      "Written 96000 / 166912\n",
      "Written 97000 / 166912\n",
      "Written 98000 / 166912\n",
      "Written 99000 / 166912\n",
      "Written 100000 / 166912\n",
      "Written 101000 / 166912\n",
      "Written 102000 / 166912\n",
      "Written 103000 / 166912\n",
      "Written 104000 / 166912\n",
      "Written 105000 / 166912\n",
      "Written 106000 / 166912\n",
      "Written 107000 / 166912\n",
      "Written 108000 / 166912\n",
      "Written 109000 / 166912\n",
      "Written 110000 / 166912\n",
      "Written 111000 / 166912\n",
      "Written 112000 / 166912\n",
      "Written 113000 / 166912\n",
      "Written 114000 / 166912\n",
      "Written 115000 / 166912\n",
      "Written 116000 / 166912\n",
      "Written 117000 / 166912\n",
      "Written 118000 / 166912\n",
      "Written 119000 / 166912\n",
      "Written 120000 / 166912\n",
      "Written 121000 / 166912\n",
      "Written 122000 / 166912\n",
      "Written 123000 / 166912\n",
      "Written 124000 / 166912\n",
      "Written 125000 / 166912\n",
      "Written 126000 / 166912\n",
      "Written 127000 / 166912\n",
      "Written 128000 / 166912\n",
      "Written 129000 / 166912\n",
      "Written 130000 / 166912\n",
      "Written 131000 / 166912\n",
      "Written 132000 / 166912\n",
      "Written 133000 / 166912\n",
      "Written 134000 / 166912\n",
      "Written 135000 / 166912\n",
      "Written 136000 / 166912\n",
      "Written 137000 / 166912\n",
      "Written 138000 / 166912\n",
      "Written 139000 / 166912\n",
      "Written 140000 / 166912\n",
      "Written 141000 / 166912\n",
      "Written 142000 / 166912\n",
      "Written 143000 / 166912\n",
      "Written 144000 / 166912\n",
      "Written 145000 / 166912\n",
      "Written 146000 / 166912\n",
      "Written 147000 / 166912\n",
      "Written 148000 / 166912\n",
      "Written 149000 / 166912\n",
      "Written 150000 / 166912\n",
      "Written 151000 / 166912\n",
      "Written 152000 / 166912\n",
      "Written 153000 / 166912\n",
      "Written 154000 / 166912\n",
      "Written 155000 / 166912\n",
      "Written 156000 / 166912\n",
      "Written 157000 / 166912\n",
      "Written 158000 / 166912\n",
      "Written 159000 / 166912\n",
      "Written 160000 / 166912\n",
      "Written 161000 / 166912\n",
      "Written 162000 / 166912\n",
      "Written 163000 / 166912\n",
      "Written 164000 / 166912\n",
      "Written 165000 / 166912\n",
      "Written 166000 / 166912\n",
      "Created dataset with 166912 samples\n"
     ]
    }
   ],
   "source": [
    "# create evaluation_lmdb file\n",
    "!python3 ./deep-text-recognition-benchmark/create_lmdb_dataset.py \\\n",
    "--inputPath ./data \\\n",
    "--gtFile ./data/evaluation_gt.txt \\\n",
    "--outputPath ./data/data_lmdb/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the images containing characters which are not in opt.character\n",
      "Filtering the images whose label is longer than opt.batch_max_length\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root: ./data/data_lmdb/training\n",
      "opt.select_data: ['/']\n",
      "opt.batch_ratio: ['1']\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    ./data/data_lmdb/training\t dataset: /\n",
      "sub-directory:\t/.\t num samples: 185728\n",
      "num total samples of /: 185728 x 1.0 (total_data_usage_ratio) = 185728\n",
      "num samples of / per batch: 32 x 1.0 (batch_ratio) = 32\n",
      "--------------------------------------------------------------------------------\n",
      "Total_batch_size: 32 = 32\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    ./data/data_lmdb/validation\t dataset: /\n",
      "sub-directory:\t/.\t num samples: 61754\n",
      "--------------------------------------------------------------------------------\n",
      "model input parameters 32 100 20 1 512 256 974 25 TPS ResNet BiLSTM Attn\n",
      "Skip Transformation.LocalizationNetwork.localization_fc2.weight as it is already initialized\n",
      "Skip Transformation.LocalizationNetwork.localization_fc2.bias as it is already initialized\n",
      "Model:\n",
      "DataParallel(\n",
      "  (module): Model(\n",
      "    (Transformation): TPS_SpatialTransformerNetwork(\n",
      "      (LocalizationNetwork): LocalizationNetwork(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace)\n",
      "          (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (6): ReLU(inplace)\n",
      "          (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (10): ReLU(inplace)\n",
      "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (14): ReLU(inplace)\n",
      "          (15): AdaptiveAvgPool2d(output_size=1)\n",
      "        )\n",
      "        (localization_fc1): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (1): ReLU(inplace)\n",
      "        )\n",
      "        (localization_fc2): Linear(in_features=256, out_features=40, bias=True)\n",
      "      )\n",
      "      (GridGenerator): GridGenerator()\n",
      "    )\n",
      "    (FeatureExtraction): ResNet_FeatureExtractor(\n",
      "      (ConvNet): ResNet(\n",
      "        (conv0_1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn0_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv0_2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn0_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (layer1): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (layer2): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace)\n",
      "          )\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (maxpool3): MaxPool2d(kernel_size=2, stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
      "        (layer3): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace)\n",
      "          )\n",
      "          (3): BasicBlock(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace)\n",
      "          )\n",
      "          (4): BasicBlock(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace)\n",
      "          )\n",
      "        )\n",
      "        (conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (layer4): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace)\n",
      "          )\n",
      "        )\n",
      "        (conv4_1): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), bias=False)\n",
      "        (bn4_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv4_2): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "        (bn4_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))\n",
      "    (SequenceModeling): Sequential(\n",
      "      (0): BidirectionalLSTM(\n",
      "        (rnn): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
      "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (1): BidirectionalLSTM(\n",
      "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (Prediction): Attention(\n",
      "      (attention_cell): AttentionCell(\n",
      "        (i2h): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (h2h): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (score): Linear(in_features=256, out_features=1, bias=False)\n",
      "        (rnn): LSTMCell(1230, 256)\n",
      "      )\n",
      "      (generator): Linear(in_features=256, out_features=974, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Trainable params num :  50754198\n",
      "Optimizer:\n",
      "Adadelta (\n",
      "Parameter Group 0\n",
      "    eps: 1e-08\n",
      "    lr: 1\n",
      "    rho: 0.95\n",
      "    weight_decay: 0\n",
      ")\n",
      "------------ Options -------------\n",
      "exp_name: TPS-ResNet-BiLSTM-Attn-Seed1111\n",
      "train_data: ./data/data_lmdb/training\n",
      "valid_data: ./data/data_lmdb/validation\n",
      "manualSeed: 1111\n",
      "workers: 4\n",
      "batch_size: 32\n",
      "num_iter: 300000\n",
      "valInterval: 2000\n",
      "saved_model: \n",
      "FT: False\n",
      "adam: False\n",
      "lr: 1\n",
      "beta1: 0.9\n",
      "rho: 0.95\n",
      "eps: 1e-08\n",
      "grad_clip: 5\n",
      "baiduCTC: False\n",
      "select_data: ['/']\n",
      "batch_ratio: ['1']\n",
      "total_data_usage_ratio: 1.0\n",
      "batch_max_length: 25\n",
      "imgH: 32\n",
      "imgW: 100\n",
      "rgb: False\n",
      "character: 가각간갇갈감갑값갓강갖같갚갛개객걀걔거걱건걷걸검겁것겉게겨격겪견결겹경곁계고곡곤곧골곰곱곳공과관광괜괴굉교구국군굳굴굵굶굽궁권귀귓규균귤그극근글긁금급긋긍기긴길김깅깊까깍깎깐깔깜깝깡깥깨꺼꺾껌껍껏껑께껴꼬꼭꼴꼼꼽꽂꽃꽉꽤꾸꾼꿀꿈뀌끄끈끊끌끓끔끗끝끼낌나낙낚난날낡남납낫낭낮낯낱낳내냄냇냉냐냥너넉넌널넓넘넣네넥넷녀녁년념녕노녹논놀놈농높놓놔뇌뇨누눈눕뉘뉴늄느늑는늘늙능늦늬니닐님다닥닦단닫달닭닮담답닷당닿대댁댐댓더덕던덜덟덤덥덧덩덮데델도독돈돌돕돗동돼되된두둑둘둠둡둥뒤뒷드득든듣들듬듭듯등디딩딪따딱딴딸땀땅때땜떠떡떤떨떻떼또똑뚜뚫뚱뛰뜨뜩뜯뜰뜻띄라락란람랍랑랗래랜램랫략량러럭런럴럼럽럿렁렇레렉렌려력련렬렵령례로록론롬롭롯료루룩룹룻뤄류륙률륭르른름릇릎리릭린림립릿링마막만많말맑맘맙맛망맞맡맣매맥맨맵맺머먹먼멀멈멋멍멎메멘멩며면멸명몇모목몬몰몸몹못몽묘무묵묶문묻물뭄뭇뭐뭘뭣므미민믿밀밉밌및밑바박밖반받발밝밟밤밥방밭배백뱀뱃뱉버번벌범법벗베벤벨벼벽변별볍병볕보복볶본볼봄봇봉뵈뵙부북분불붉붐붓붕붙뷰브븐블비빌빔빗빚빛빠빡빨빵빼뺏뺨뻐뻔뻗뼈뼉뽑뿌뿐쁘쁨사삭산살삶삼삿상새색샌생샤서석섞선설섬섭섯성세섹센셈셋셔션소속손솔솜솟송솥쇄쇠쇼수숙순숟술숨숫숭숲쉬쉰쉽슈스슨슬슴습슷승시식신싣실싫심십싯싱싶싸싹싼쌀쌍쌓써썩썰썹쎄쏘쏟쑤쓰쓴쓸씀씌씨씩씬씹씻아악안앉않알앓암압앗앙앞애액앨야약얀얄얇양얕얗얘어억언얹얻얼엄업없엇엉엊엌엎에엔엘여역연열엷염엽엿영옆예옛오옥온올옮옳옷옹와완왕왜왠외왼요욕용우욱운울움웃웅워원월웨웬위윗유육율으윽은을음응의이익인일읽잃임입잇있잊잎자작잔잖잘잠잡잣장잦재쟁쟤저적전절젊점접젓정젖제젠젯져조족존졸좀좁종좋좌죄주죽준줄줌줍중쥐즈즉즌즐즘증지직진질짐집짓징짙짚짜짝짧째쨌쩌쩍쩐쩔쩜쪽쫓쭈쭉찌찍찢차착찬찮찰참찻창찾채책챔챙처척천철첩첫청체쳐초촉촌촛총촬최추축춘출춤춥춧충취츠측츰층치칙친칠침칫칭카칸칼캄캐캠커컨컬컴컵컷케켓켜코콘콜콤콩쾌쿄쿠퀴크큰클큼키킬타탁탄탈탑탓탕태택탤터턱턴털텅테텍텔템토톤톨톱통퇴투툴툼퉁튀튜트특튼튿틀틈티틱팀팅파팎판팔팝패팩팬퍼퍽페펜펴편펼평폐포폭폰표푸푹풀품풍퓨프플픔피픽필핏핑하학한할함합항해핵핸햄햇행향허헌험헤헬혀현혈협형혜호혹혼홀홈홉홍화확환활황회획횟횡효후훈훌훔훨휘휴흉흐흑흔흘흙흡흥흩희흰히힘\n",
      "sensitive: False\n",
      "PAD: False\n",
      "data_filtering_off: False\n",
      "Transformation: TPS\n",
      "FeatureExtraction: ResNet\n",
      "SequenceModeling: BiLSTM\n",
      "Prediction: Attn\n",
      "num_fiducial: 20\n",
      "input_channel: 1\n",
      "output_channel: 512\n",
      "hidden_size: 256\n",
      "num_gpu: 0\n",
      "num_class: 974\n",
      "---------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "Traceback (most recent call last):\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\r\n",
      "    send_bytes(obj)\r\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\r\n",
      "    send_bytes(obj)\r\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\r\n",
      "    self._send_bytes(m[offset:offset + size])\r\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\r\n",
      "    self._send_bytes(m[offset:offset + size])\r\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\r\n",
      "    send_bytes(obj)\r\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\r\n",
      "    self._send(header + buf)\r\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\r\n",
      "    self._send(header + buf)\r\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\r\n",
      "    n = write(self._handle, buf)\r\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\r\n",
      "    self._send_bytes(m[offset:offset + size])\r\n",
      "BrokenPipeError: [Errno 32] Broken pipe\r\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\r\n",
      "    self._send(header + buf)\r\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\r\n",
      "    n = write(self._handle, buf)\r\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\r\n",
      "    n = write(self._handle, buf)\r\n",
      "BrokenPipeError: [Errno 32] Broken pipe\r\n",
      "BrokenPipeError: [Errno 32] Broken pipe\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\r\n",
      "    send_bytes(obj)\r\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\r\n",
      "    self._send_bytes(m[offset:offset + size])\r\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\r\n",
      "    self._send(header + buf)\r\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\r\n",
      "    n = write(self._handle, buf)\r\n",
      "BrokenPipeError: [Errno 32] Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python3 ./deep-text-recognition-benchmark/train.py \\\n",
    "--train_data ./data/data_lmdb/training \\\n",
    "--valid_data ./data/data_lmdb/validation \\\n",
    "--Transformation TPS \\\n",
    "--FeatureExtraction ResNet \\\n",
    "--SequenceModeling BiLSTM \\\n",
    "--Prediction Attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model input parameters 32 100 20 1 512 256 974 50 TPS ResNet BiLSTM Attn\n",
      "loading pretrained model from ./deep-text-recognition-benchmark/saved_models/TPS-ResNet-BiLSTM-Attn/best_accuracy.pth\n",
      "Traceback (most recent call last):\n",
      "  File \"./deep-text-recognition-benchmark/test.py\", line 236, in <module>\n",
      "    test(opt)\n",
      "  File \"./deep-text-recognition-benchmark/test.py\", line 162, in test\n",
      "    model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 769, in load_state_dict\n",
      "    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
      "RuntimeError: Error(s) in loading state_dict for DataParallel:\n",
      "\tMissing key(s) in state_dict: \"module.FeatureExtraction.ConvNet.conv0_1.weight\", \"module.FeatureExtraction.ConvNet.bn0_1.weight\", \"module.FeatureExtraction.ConvNet.bn0_1.bias\", \"module.FeatureExtraction.ConvNet.bn0_1.running_mean\", \"module.FeatureExtraction.ConvNet.bn0_1.running_var\", \"module.FeatureExtraction.ConvNet.conv0_2.weight\", \"module.FeatureExtraction.ConvNet.bn0_2.weight\", \"module.FeatureExtraction.ConvNet.bn0_2.bias\", \"module.FeatureExtraction.ConvNet.bn0_2.running_mean\", \"module.FeatureExtraction.ConvNet.bn0_2.running_var\", \"module.FeatureExtraction.ConvNet.layer1.0.conv1.weight\", \"module.FeatureExtraction.ConvNet.layer1.0.bn1.weight\", \"module.FeatureExtraction.ConvNet.layer1.0.bn1.bias\", \"module.FeatureExtraction.ConvNet.layer1.0.bn1.running_mean\", \"module.FeatureExtraction.ConvNet.layer1.0.bn1.running_var\", \"module.FeatureExtraction.ConvNet.layer1.0.conv2.weight\", \"module.FeatureExtraction.ConvNet.layer1.0.bn2.weight\", \"module.FeatureExtraction.ConvNet.layer1.0.bn2.bias\", \"module.FeatureExtraction.ConvNet.layer1.0.bn2.running_mean\", \"module.FeatureExtraction.ConvNet.layer1.0.bn2.running_var\", \"module.FeatureExtraction.ConvNet.layer1.0.downsample.0.weight\", \"module.FeatureExtraction.ConvNet.layer1.0.downsample.1.weight\", \"module.FeatureExtraction.ConvNet.layer1.0.downsample.1.bias\", \"module.FeatureExtraction.ConvNet.layer1.0.downsample.1.running_mean\", \"module.FeatureExtraction.ConvNet.layer1.0.downsample.1.running_var\", \"module.FeatureExtraction.ConvNet.conv1.weight\", \"module.FeatureExtraction.ConvNet.bn1.weight\", \"module.FeatureExtraction.ConvNet.bn1.bias\", \"module.FeatureExtraction.ConvNet.bn1.running_mean\", \"module.FeatureExtraction.ConvNet.bn1.running_var\", \"module.FeatureExtraction.ConvNet.layer2.0.conv1.weight\", \"module.FeatureExtraction.ConvNet.layer2.0.bn1.weight\", \"module.FeatureExtraction.ConvNet.layer2.0.bn1.bias\", \"module.FeatureExtraction.ConvNet.layer2.0.bn1.running_mean\", \"module.FeatureExtraction.ConvNet.layer2.0.bn1.running_var\", \"module.FeatureExtraction.ConvNet.layer2.0.conv2.weight\", \"module.FeatureExtraction.ConvNet.layer2.0.bn2.weight\", \"module.FeatureExtraction.ConvNet.layer2.0.bn2.bias\", \"module.FeatureExtraction.ConvNet.layer2.0.bn2.running_mean\", \"module.FeatureExtraction.ConvNet.layer2.0.bn2.running_var\", \"module.FeatureExtraction.ConvNet.layer2.0.downsample.0.weight\", \"module.FeatureExtraction.ConvNet.layer2.0.downsample.1.weight\", \"module.FeatureExtraction.ConvNet.layer2.0.downsample.1.bias\", \"module.FeatureExtraction.ConvNet.layer2.0.downsample.1.running_mean\", \"module.FeatureExtraction.ConvNet.layer2.0.downsample.1.running_var\", \"module.FeatureExtraction.ConvNet.layer2.1.conv1.weight\", \"module.FeatureExtraction.ConvNet.layer2.1.bn1.weight\", \"module.FeatureExtraction.ConvNet.layer2.1.bn1.bias\", \"module.FeatureExtraction.ConvNet.layer2.1.bn1.running_mean\", \"module.FeatureExtraction.ConvNet.layer2.1.bn1.running_var\", \"module.FeatureExtraction.ConvNet.layer2.1.conv2.weight\", \"module.FeatureExtraction.ConvNet.layer2.1.bn2.weight\", \"module.FeatureExtraction.ConvNet.layer2.1.bn2.bias\", \"module.FeatureExtraction.ConvNet.layer2.1.bn2.running_mean\", \"module.FeatureExtraction.ConvNet.layer2.1.bn2.running_var\", \"module.FeatureExtraction.ConvNet.conv2.weight\", \"module.FeatureExtraction.ConvNet.bn2.weight\", \"module.FeatureExtraction.ConvNet.bn2.bias\", \"module.FeatureExtraction.ConvNet.bn2.running_mean\", \"module.FeatureExtraction.ConvNet.bn2.running_var\", \"module.FeatureExtraction.ConvNet.layer3.0.conv1.weight\", \"module.FeatureExtraction.ConvNet.layer3.0.bn1.weight\", \"module.FeatureExtraction.ConvNet.layer3.0.bn1.bias\", \"module.FeatureExtraction.ConvNet.layer3.0.bn1.running_mean\", \"module.FeatureExtraction.ConvNet.layer3.0.bn1.running_var\", \"module.FeatureExtraction.ConvNet.layer3.0.conv2.weight\", \"module.FeatureExtraction.ConvNet.layer3.0.bn2.weight\", \"module.FeatureExtraction.ConvNet.layer3.0.bn2.bias\", \"module.FeatureExtraction.ConvNet.layer3.0.bn2.running_mean\", \"module.FeatureExtraction.ConvNet.layer3.0.bn2.running_var\", \"module.FeatureExtraction.ConvNet.layer3.0.downsample.0.weight\", \"module.FeatureExtraction.ConvNet.layer3.0.downsample.1.weight\", \"module.FeatureExtraction.ConvNet.layer3.0.downsample.1.bias\", \"module.FeatureExtraction.ConvNet.layer3.0.downsample.1.running_mean\", \"module.FeatureExtraction.ConvNet.layer3.0.downsample.1.running_var\", \"module.FeatureExtraction.ConvNet.layer3.1.conv1.weight\", \"module.FeatureExtraction.ConvNet.layer3.1.bn1.weight\", \"module.FeatureExtraction.ConvNet.layer3.1.bn1.bias\", \"module.FeatureExtraction.ConvNet.layer3.1.bn1.running_mean\", \"module.FeatureExtraction.ConvNet.layer3.1.bn1.running_var\", \"module.FeatureExtraction.ConvNet.layer3.1.conv2.weight\", \"module.FeatureExtraction.ConvNet.layer3.1.bn2.weight\", \"module.FeatureExtraction.ConvNet.layer3.1.bn2.bias\", \"module.FeatureExtraction.ConvNet.layer3.1.bn2.running_mean\", \"module.FeatureExtraction.ConvNet.layer3.1.bn2.running_var\", \"module.FeatureExtraction.ConvNet.layer3.2.conv1.weight\", \"module.FeatureExtraction.ConvNet.layer3.2.bn1.weight\", \"module.FeatureExtraction.ConvNet.layer3.2.bn1.bias\", \"module.FeatureExtraction.ConvNet.layer3.2.bn1.running_mean\", \"module.FeatureExtraction.ConvNet.layer3.2.bn1.running_var\", \"module.FeatureExtraction.ConvNet.layer3.2.conv2.weight\", \"module.FeatureExtraction.ConvNet.layer3.2.bn2.weight\", \"module.FeatureExtraction.ConvNet.layer3.2.bn2.bias\", \"module.FeatureExtraction.ConvNet.layer3.2.bn2.running_mean\", \"module.FeatureExtraction.ConvNet.layer3.2.bn2.running_var\", \"module.FeatureExtraction.ConvNet.layer3.3.conv1.weight\", \"module.FeatureExtraction.ConvNet.layer3.3.bn1.weight\", \"module.FeatureExtraction.ConvNet.layer3.3.bn1.bias\", \"module.FeatureExtraction.ConvNet.layer3.3.bn1.running_mean\", \"module.FeatureExtraction.ConvNet.layer3.3.bn1.running_var\", \"module.FeatureExtraction.ConvNet.layer3.3.conv2.weight\", \"module.FeatureExtraction.ConvNet.layer3.3.bn2.weight\", \"module.FeatureExtraction.ConvNet.layer3.3.bn2.bias\", \"module.FeatureExtraction.ConvNet.layer3.3.bn2.running_mean\", \"module.FeatureExtraction.ConvNet.layer3.3.bn2.running_var\", \"module.FeatureExtraction.ConvNet.layer3.4.conv1.weight\", \"module.FeatureExtraction.ConvNet.layer3.4.bn1.weight\", \"module.FeatureExtraction.ConvNet.layer3.4.bn1.bias\", \"module.FeatureExtraction.ConvNet.layer3.4.bn1.running_mean\", \"module.FeatureExtraction.ConvNet.layer3.4.bn1.running_var\", \"module.FeatureExtraction.ConvNet.layer3.4.conv2.weight\", \"module.FeatureExtraction.ConvNet.layer3.4.bn2.weight\", \"module.FeatureExtraction.ConvNet.layer3.4.bn2.bias\", \"module.FeatureExtraction.ConvNet.layer3.4.bn2.running_mean\", \"module.FeatureExtraction.ConvNet.layer3.4.bn2.running_var\", \"module.FeatureExtraction.ConvNet.conv3.weight\", \"module.FeatureExtraction.ConvNet.bn3.weight\", \"module.FeatureExtraction.ConvNet.bn3.bias\", \"module.FeatureExtraction.ConvNet.bn3.running_mean\", \"module.FeatureExtraction.ConvNet.bn3.running_var\", \"module.FeatureExtraction.ConvNet.layer4.0.conv1.weight\", \"module.FeatureExtraction.ConvNet.layer4.0.bn1.weight\", \"module.FeatureExtraction.ConvNet.layer4.0.bn1.bias\", \"module.FeatureExtraction.ConvNet.layer4.0.bn1.running_mean\", \"module.FeatureExtraction.ConvNet.layer4.0.bn1.running_var\", \"module.FeatureExtraction.ConvNet.layer4.0.conv2.weight\", \"module.FeatureExtraction.ConvNet.layer4.0.bn2.weight\", \"module.FeatureExtraction.ConvNet.layer4.0.bn2.bias\", \"module.FeatureExtraction.ConvNet.layer4.0.bn2.running_mean\", \"module.FeatureExtraction.ConvNet.layer4.0.bn2.running_var\", \"module.FeatureExtraction.ConvNet.layer4.1.conv1.weight\", \"module.FeatureExtraction.ConvNet.layer4.1.bn1.weight\", \"module.FeatureExtraction.ConvNet.layer4.1.bn1.bias\", \"module.FeatureExtraction.ConvNet.layer4.1.bn1.running_mean\", \"module.FeatureExtraction.ConvNet.layer4.1.bn1.running_var\", \"module.FeatureExtraction.ConvNet.layer4.1.conv2.weight\", \"module.FeatureExtraction.ConvNet.layer4.1.bn2.weight\", \"module.FeatureExtraction.ConvNet.layer4.1.bn2.bias\", \"module.FeatureExtraction.ConvNet.layer4.1.bn2.running_mean\", \"module.FeatureExtraction.ConvNet.layer4.1.bn2.running_var\", \"module.FeatureExtraction.ConvNet.layer4.2.conv1.weight\", \"module.FeatureExtraction.ConvNet.layer4.2.bn1.weight\", \"module.FeatureExtraction.ConvNet.layer4.2.bn1.bias\", \"module.FeatureExtraction.ConvNet.layer4.2.bn1.running_mean\", \"module.FeatureExtraction.ConvNet.layer4.2.bn1.running_var\", \"module.FeatureExtraction.ConvNet.layer4.2.conv2.weight\", \"module.FeatureExtraction.ConvNet.layer4.2.bn2.weight\", \"module.FeatureExtraction.ConvNet.layer4.2.bn2.bias\", \"module.FeatureExtraction.ConvNet.layer4.2.bn2.running_mean\", \"module.FeatureExtraction.ConvNet.layer4.2.bn2.running_var\", \"module.FeatureExtraction.ConvNet.conv4_1.weight\", \"module.FeatureExtraction.ConvNet.bn4_1.weight\", \"module.FeatureExtraction.ConvNet.bn4_1.bias\", \"module.FeatureExtraction.ConvNet.bn4_1.running_mean\", \"module.FeatureExtraction.ConvNet.bn4_1.running_var\", \"module.FeatureExtraction.ConvNet.conv4_2.weight\", \"module.FeatureExtraction.ConvNet.bn4_2.weight\", \"module.FeatureExtraction.ConvNet.bn4_2.bias\", \"module.FeatureExtraction.ConvNet.bn4_2.running_mean\", \"module.FeatureExtraction.ConvNet.bn4_2.running_var\". \n",
      "\tUnexpected key(s) in state_dict: \"module.FeatureExtraction.ConvNet.0.weight\", \"module.FeatureExtraction.ConvNet.0.bias\", \"module.FeatureExtraction.ConvNet.3.weight\", \"module.FeatureExtraction.ConvNet.3.bias\", \"module.FeatureExtraction.ConvNet.6.weight\", \"module.FeatureExtraction.ConvNet.6.bias\", \"module.FeatureExtraction.ConvNet.8.weight\", \"module.FeatureExtraction.ConvNet.8.bias\", \"module.FeatureExtraction.ConvNet.11.weight\", \"module.FeatureExtraction.ConvNet.12.weight\", \"module.FeatureExtraction.ConvNet.12.bias\", \"module.FeatureExtraction.ConvNet.12.running_mean\", \"module.FeatureExtraction.ConvNet.12.running_var\", \"module.FeatureExtraction.ConvNet.12.num_batches_tracked\", \"module.FeatureExtraction.ConvNet.14.weight\", \"module.FeatureExtraction.ConvNet.15.weight\", \"module.FeatureExtraction.ConvNet.15.bias\", \"module.FeatureExtraction.ConvNet.15.running_mean\", \"module.FeatureExtraction.ConvNet.15.running_var\", \"module.FeatureExtraction.ConvNet.15.num_batches_tracked\", \"module.FeatureExtraction.ConvNet.18.weight\", \"module.FeatureExtraction.ConvNet.18.bias\". \n",
      "\tsize mismatch for module.Transformation.GridGenerator.P_hat: copying a param with shape torch.Size([12800, 23]) from checkpoint, the shape in current model is torch.Size([3200, 23]).\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python3 ./deep-text-recognition-benchmark/test.py \\\n",
    "--eval_data ./data/data_lmdb/ \\\n",
    "--benchmark_all_eval \\\n",
    "--Transformation TPS \\\n",
    "--FeatureExtraction ResNet \\\n",
    "--SequenceModeling BiLSTM \\\n",
    "--Prediction Attn \\\n",
    "--saved_model ./deep-text-recognition-benchmark/saved_models/TPS-ResNet-BiLSTM-Attn/best_accuracy.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model input parameters 32 100 20 1 512 256 974 25 TPS ResNet BiLSTM Attn\n",
      "loading pretrained model from ./deep-text-recognition-benchmark/saved_models/best_accuracy.pth\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/tarfile.py\", line 189, in nti\n",
      "    n = int(s.strip() or \"0\", 8)\n",
      "ValueError: invalid literal for int() with base 8: 'v.0.weig'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/tarfile.py\", line 2297, in next\n",
      "    tarinfo = self.tarinfo.fromtarfile(self)\n",
      "  File \"/opt/conda/lib/python3.6/tarfile.py\", line 1093, in fromtarfile\n",
      "    obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)\n",
      "  File \"/opt/conda/lib/python3.6/tarfile.py\", line 1035, in frombuf\n",
      "    chksum = nti(buf[148:156])\n",
      "  File \"/opt/conda/lib/python3.6/tarfile.py\", line 191, in nti\n",
      "    raise InvalidHeaderError(\"invalid header\")\n",
      "tarfile.InvalidHeaderError: invalid header\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/serialization.py\", line 524, in _load\n",
      "    return legacy_load(f)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/serialization.py\", line 448, in legacy_load\n",
      "    with closing(tarfile.open(fileobj=f, mode='r:', format=tarfile.PAX_FORMAT)) as tar, \\\n",
      "  File \"/opt/conda/lib/python3.6/tarfile.py\", line 1589, in open\n",
      "    return func(name, filemode, fileobj, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/tarfile.py\", line 1619, in taropen\n",
      "    return cls(name, mode, fileobj, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/tarfile.py\", line 1482, in __init__\n",
      "    self.firstmember = self.next()\n",
      "  File \"/opt/conda/lib/python3.6/tarfile.py\", line 2309, in next\n",
      "    raise ReadError(str(e))\n",
      "tarfile.ReadError: invalid header\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"./deep-text-recognition-benchmark/test1.py\", line 281, in <module>\n",
      "    test(opt)\n",
      "  File \"./deep-text-recognition-benchmark/test1.py\", line 207, in test\n",
      "    model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/serialization.py\", line 368, in load\n",
      "    return _load(f, map_location, pickle_module)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/serialization.py\", line 528, in _load\n",
      "    raise RuntimeError(\"{} is a zip archive (did you mean to use torch.jit.load()?)\".format(f.name))\n",
      "RuntimeError: ./deep-text-recognition-benchmark/saved_models/best_accuracy.pth is a zip archive (did you mean to use torch.jit.load()?)\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python3 ./deep-text-recognition-benchmark/test1.py \\\n",
    "--eval_data data/data_lmdb_release/evaluation \\\n",
    "--benchmark_all_eval \\\n",
    "--Transformation TPS \\\n",
    "--FeatureExtraction ResNet \\\n",
    "--SequenceModeling BiLSTM \\\n",
    "--Prediction Attn \\\n",
    "--saved_model ./deep-text-recognition-benchmark/saved_models/best_accuracy.pth \\\n",
    "--data_filtering_off \\\n",
    "--workers 4;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
